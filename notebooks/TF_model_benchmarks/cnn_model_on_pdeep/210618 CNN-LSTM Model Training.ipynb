{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Training a CNN-LSTM Model on Tensorflow Datasets\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from official.nlp.modeling.layers.position_embedding import RelativePositionEmbedding\n",
    "from mmproteo.utils import log, paths, utils, visualization\n",
    "from mmproteo.utils.formats.tf_dataset import DatasetLoader\n",
    "from mmproteo.utils.ml import callbacks, evaluation, layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef12d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b697199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hpi/fs00/home/mirko.krause/masterthesis/pride-downloader/notebooks/TF_model_benchmarks/cnn_model_on_pdeep'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e481a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_SOURCE = 'pride'\n",
    "DATA_SOURCE = 'pdeep'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac43a97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "os.chdir(os.path.join('workspace', 'notebooks'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d57e9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cc0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "\n",
    "if DATA_SOURCE == 'pride':\n",
    "    DUMP_PATH = os.path.join(\"/scratch/mirko.krause/dumps/\", PROJECT)\n",
    "    TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "    FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "elif DATA_SOURCE == 'pdeep':\n",
    "    DUMP_PATH = \"/scratch/mirko.krause/pdeep\"\n",
    "    TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "    FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"file_*.parquet\")\n",
    "else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_COUNT = min(int(os.cpu_count()/2), 16)\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")\n",
    "PROCESSING_FILE_PATH = os.path.join(DATASET_DUMP_PATH, \"processing_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1b96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 22:31:53,123 - mmproteo_cnn_lstm: Logging to file '/scratch/mirko.krause/pdeep/mmproteo_cnn_lstm.log' and to stderr\n"
     ]
    }
   ],
   "source": [
    "logger = log.create_logger(\n",
    "    name='mmproteo_cnn_lstm',\n",
    "    verbose=True,\n",
    "    log_dir=DUMP_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c29d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_characters': {'peptide_sequence': '_',\n",
       "  'mz_array': 0.0,\n",
       "  'intensity_array': 0.0},\n",
       " 'padding_lengths': {'mz_array': 89,\n",
       "  'intensity_array': 89,\n",
       "  'peptide_sequence': 30},\n",
       " 'idx_to_char': {'2': 'A',\n",
       "  '3': 'C',\n",
       "  '4': 'D',\n",
       "  '5': 'E',\n",
       "  '6': 'F',\n",
       "  '7': 'G',\n",
       "  '8': 'H',\n",
       "  '9': 'I',\n",
       "  '10': 'K',\n",
       "  '11': 'L',\n",
       "  '12': 'M',\n",
       "  '13': 'N',\n",
       "  '14': 'P',\n",
       "  '15': 'Q',\n",
       "  '16': 'R',\n",
       "  '17': 'S',\n",
       "  '18': 'T',\n",
       "  '19': 'V',\n",
       "  '20': 'W',\n",
       "  '21': 'Y',\n",
       "  '0': '_',\n",
       "  '1': 'EOS'},\n",
       " 'normalization': {'intensity_array': '<function base_peak_normalize at 0x7f01b581c160>'},\n",
       " 'split_value_columns': None,\n",
       " 'training_data_columns': ['mz_array', 'intensity_array'],\n",
       " 'target_data_columns': ['peptide_sequence'],\n",
       " 'element_spec': '((TensorSpec(shape=(89,), dtype=tf.float64, name=None), TensorSpec(shape=(89,), dtype=tf.float64, name=None)), (TensorSpec(shape=(30,), dtype=tf.int8, name=None),))',\n",
       " 'eos_idx': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PROCESSING_FILE_PATH, 'r') as file:\n",
    "    PROCESSING_INFO = json.loads(file.read())\n",
    "PROCESSING_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7097059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_grouping_path_position(cols: Optional[List[str]], prefered_item: str, alternative_index: int = -1) -> int:\n",
    "    res = alternative_index\n",
    "    if cols is not None:\n",
    "        try:\n",
    "            res = cols.index(prefered_item) - len(cols)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a71deb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouping_path_position = find_grouping_path_position(\n",
    "    cols=PROCESSING_INFO['split_value_columns'],\n",
    "    prefered_item='species',\n",
    "    alternative_index=-1,\n",
    ")\n",
    "grouping_path_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = {int(idx): char for idx, char in PROCESSING_INFO[\"idx_to_char\"].items()}\n",
    "char_to_idx = {char: idx for idx, char in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CACHE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TYPE = 'Train'\n",
    "TEST_TYPE = 'Test'\n",
    "EVAL_TYPE = 'Eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d696d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assigned dataset files:\n",
      "#Train = 22\n",
      "e.g.: /scratch/mirko.krause/pdeep/training_columns/tf_datasets/file_12.parquet\n",
      "#Test = 3\n",
      "e.g.: /scratch/mirko.krause/pdeep/training_columns/tf_datasets/file_10.parquet\n",
      "#Eval = 3\n",
      "e.g.: /scratch/mirko.krause/pdeep/training_columns/tf_datasets/file_6.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 22:31:53,146 - mmproteo_cnn_lstm: found file paths dump '/scratch/mirko.krause/pdeep/training_columns/tf_datasets/dataset_file_paths.json'\n"
     ]
    }
   ],
   "source": [
    "dataset_file_paths = paths.assign_wildcard_paths_to_splits_grouped_by_path_position_value(\n",
    "    wildcard_path = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        *(['*' for _ in PROCESSING_INFO['split_value_columns'] or []])\n",
    "    ),\n",
    "    path_position = grouping_path_position,\n",
    "    splits = {\n",
    "            TRAIN_TYPE: 0.8,\n",
    "            TEST_TYPE: 0.9,\n",
    "            EVAL_TYPE: 1.0\n",
    "        },\n",
    "    paths_dump_file = os.path.join(\n",
    "            DATASET_DUMP_PATH,\n",
    "            \"dataset_file_paths.json\"\n",
    "        ),\n",
    "    skip_existing = KEEP_CACHE,\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"assigned dataset files:\")\n",
    "visualization.print_list_length_in_dict(dataset_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611252",
   "metadata": {},
   "source": [
    "### Loading corresponding TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb7ccf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(89,), dtype=tf.float64, name=None),\n",
       "  TensorSpec(shape=(89,), dtype=tf.float64, name=None)),\n",
       " (TensorSpec(shape=(30,), dtype=tf.int8, name=None),))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = eval(PROCESSING_INFO['element_spec'], {}, {'TensorSpec':tf.TensorSpec, 'tf':tf})\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1422075b",
   "metadata": {},
   "source": [
    "element_spec = (\n",
    "    tuple(tf.TensorSpec(shape=(PROCESSING_INFO['padding_lengths'][col], ), dtype=tf.float32, name=col)\n",
    "     for col in PROCESSING_INFO['training_data_columns']),\n",
    "    tuple(tf.TensorSpec(shape=(PROCESSING_INFO['padding_lengths'][col], ), dtype=tf.int8, name=col)\n",
    "     for col in PROCESSING_INFO['target_data_columns'])\n",
    ")\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b34d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7a80d",
   "metadata": {},
   "source": [
    "In the following step, Tensorflow starts allocating GPUs and GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Tensorflow (might take several minutes (~5 minutes per GPU with 40GB VRAM each))\n",
    "logger.debug(\"started initializing tensorflow by creating a first dataset\")\n",
    "tf.data.Dataset.range(5)\n",
    "logger.info(\"finished initializing tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2609b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_options = tf.data.Options()\n",
    "ds_options.experimental_threading.private_threadpool_size = THREAD_COUNT\n",
    "ds_options.experimental_threading.max_intra_op_parallelism = THREAD_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetLoader(\n",
    "    element_spec=element_spec,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=100_000,\n",
    "    thread_count=min(int(os.cpu_count()/4), 4),\n",
    "    keep_cache=KEEP_CACHE,\n",
    "    logger=logger,\n",
    "    run_benchmarks=False,\n",
    "    options=ds_options,\n",
    ").load_datasets_by_type(dataset_file_paths)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa49c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.metrics import MeanMetricWrapper\n",
    "from tensorflow.python.ops import array_ops, math_ops\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bb2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(elem):\n",
    "    i_true, i_pred = elem[0], elem[1]\n",
    "    unique_true, _ = tf.unique(i_true)\n",
    "    unique_pred, _ = tf.unique(i_pred)\n",
    "    n_unique_true = tf.shape(unique_true)[0]\n",
    "    n_unique_pred = tf.shape(unique_pred)[0]\n",
    "    unique_all, _ = tf.unique(tf.concat(values=[unique_true, unique_pred], axis=-1))\n",
    "    n_unique_all = tf.shape(unique_all)[0]\n",
    "    n_overlap = n_unique_true + n_unique_pred - n_unique_all\n",
    "    return n_overlap / n_unique_all\n",
    "\n",
    "def jaccard_batch_distance(y_true, y_pred):\n",
    "    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
    "    y_true = ops.convert_to_tensor_v2_with_dispatch(y_true)\n",
    "    y_pred_rank = y_pred.shape.ndims\n",
    "    y_true_rank = y_true.shape.ndims\n",
    "    # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "    if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n",
    "            backend.int_shape(y_true)) == len(backend.int_shape(y_pred))):\n",
    "        y_true = array_ops.squeeze(y_true, [-1])\n",
    "    y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if backend.dtype(y_pred) != backend.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, backend.dtype(y_true))\n",
    "    \n",
    "    # 0th dimension is the batch\n",
    "    jaccard = tf.map_fn(fn=jaccard_distance, elems=(y_true, y_pred), fn_output_signature=tf.float64)\n",
    "    return math_ops.cast(jaccard, backend.floatx())\n",
    "    \n",
    "\n",
    "class JaccardBatchDistance(MeanMetricWrapper):\n",
    "    def __init__(self, name='jaccard_batch_distance', dtype=None):\n",
    "        super(JaccardBatchDistance, self).__init__(\n",
    "            jaccard_batch_distance, name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7927b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leuvenshtein_sparse_tensor_batch_distance(y_true: tf.sparse.SparseTensor, y_pred: tf.sparse.SparseTensor) -> tf.Tensor:\n",
    "    return tf.edit_distance(\n",
    "        hypothesis=y_pred,\n",
    "        truth=y_true,\n",
    "        normalize=False,\n",
    "    )\n",
    "\n",
    "def leuvenshtein_batch_distance(y_true: tf.Tensor, y_pred: tf.Tensor, sparse_pred: bool = True) -> tf.Tensor:\n",
    "    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
    "    y_true = ops.convert_to_tensor_v2_with_dispatch(y_true)\n",
    "    \n",
    "    if sparse_pred:\n",
    "        y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    y_pred = tf.sparse.from_dense(y_pred)\n",
    "    y_true = tf.sparse.from_dense(y_true)\n",
    "\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if backend.dtype(y_pred) != backend.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, backend.dtype(y_true))\n",
    "    \n",
    "    # 0th dimension is the batch\n",
    "    leuvenshtein = leuvenshtein_sparse_tensor_batch_distance(y_true, y_pred)\n",
    "    return math_ops.cast(leuvenshtein, backend.floatx())\n",
    "    \n",
    "\n",
    "class LeuvenshteinBatchDistance(MeanMetricWrapper):\n",
    "    def __init__(self, name='leuvenshtein_batch_distance', dtype=None):\n",
    "        super(LeuvenshteinBatchDistance, self).__init__(\n",
    "            leuvenshtein_batch_distance, name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers_list, masked_input_layers_list = layers.create_masked_input_layers(\n",
    "    [\n",
    "        layers.InputLayerConfiguration(\n",
    "            name=col,\n",
    "            shape=PROCESSING_INFO['padding_lengths'][col],\n",
    "            mask_value=PROCESSING_INFO['padding_characters'][col]\n",
    "        )\n",
    "        for col in PROCESSING_INFO['training_data_columns']\n",
    "    ]\n",
    ")\n",
    "print(input_layers_list)\n",
    "print(masked_input_layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4632318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_with_pooling_and_position_model(\n",
    "    model_name: str = \"mmproteo_lstm_with_pooling_and_position\"\n",
    ") -> tf.keras.Model:\n",
    "    input_layers_list, masked_input_layers_list = layers.create_masked_input_layers(\n",
    "        [\n",
    "            layers.InputLayerConfiguration(\n",
    "                name=col,\n",
    "                shape=PROCESSING_INFO['padding_lengths'][col],\n",
    "                mask_value=PROCESSING_INFO['padding_characters'][col]\n",
    "            )\n",
    "            for col in PROCESSING_INFO['training_data_columns']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    x = tf.stack(\n",
    "        values=input_layers_list, \n",
    "        axis=-1,\n",
    "    )\n",
    "    \n",
    "    position_embedding = RelativePositionEmbedding(\n",
    "        hidden_size=12,\n",
    "        name='relative_position_embedding'\n",
    "    )(x)\n",
    "    position_embedding = tf.expand_dims(position_embedding, 0)\n",
    "    position_embedding = tf.broadcast_to(\n",
    "        input=position_embedding, \n",
    "        shape=(tf.shape(x)[0], *tf.shape(position_embedding)[1:])\n",
    "    )\n",
    "    \n",
    "    y_layers=[position_embedding]\n",
    "    \n",
    "    dense_y = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=32,\n",
    "            activation='relu',\n",
    "            name='y_time_distributed_dense',\n",
    "        )\n",
    "    )(x)\n",
    "    y_layers.append(dense_y)\n",
    "    \n",
    "\n",
    "    for i in range(4):\n",
    "        filter_count = 16 * (i+1)\n",
    "        kernel_size = 4 ** min(i, 3)\n",
    "        cnn_y = tf.keras.layers.Conv1D(\n",
    "            filters=filter_count,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name=f\"y_conv_{kernel_size}_{filter_count}\",\n",
    "        )(x)\n",
    "        y_layers.append(cnn_y)\n",
    "    \n",
    "    x = tf.concat(\n",
    "        values=y_layers,\n",
    "        axis=-1\n",
    "    )\n",
    "    \n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        layer=tf.keras.layers.LSTM(\n",
    "            units=256,\n",
    "            return_sequences=True,\n",
    "            name='lstm'\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalMaxPooling1D(\n",
    "        name='global_max_pooling_over_time',\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=1024,\n",
    "        activation='relu',\n",
    "        name=f\"upscaling_dense_{i}\",\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=PROCESSING_INFO['padding_lengths'][SEQ] * 8,\n",
    "        activation='relu',\n",
    "        name=\"final_dense_layer_to_redefine_lengths\",\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.reshape(x, (-1, PROCESSING_INFO['padding_lengths'][SEQ], 8))\n",
    "    \n",
    "    x=tf.keras.layers.LSTM(\n",
    "        units=len(idx_to_char),\n",
    "        return_sequences=True,\n",
    "        name='lstm'\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.activations.softmax(x)\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=input_layers_list, \n",
    "        outputs=x, \n",
    "        name=f\"{model_name}_{DATA_SOURCE}_{utils.get_current_time_str()}\"\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=10**-4\n",
    "        ),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "            JaccardBatchDistance(),\n",
    "            LeuvenshteinBatchDistance(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_with_pooling_and_position_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(DUMP_PATH, \"models\", model.name)\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5481a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ensure_dir_exists(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model,\n",
    "    to_file=os.path.join(MODEL_PATH, \"model.png\"),\n",
    "    show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd410eda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"summary.txt\"), 'w') as file:\n",
    "    def write_lines(line: str) -> None:\n",
    "        file.write(line)\n",
    "        file.write(\"\\n\")\n",
    "    model.summary(print_fn=write_lines)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.json\"), 'w') as file:\n",
    "    file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.yaml\"), 'w') as file:\n",
    "    file.write(model.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b190f673",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7604ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = callbacks.create_callbacks(\n",
    "            tensorboard=True,\n",
    "            progressbar=False,\n",
    "            reduce_lr=False,\n",
    "            early_stopping=False,\n",
    "            checkpoints=False,\n",
    "            csv=True,\n",
    "            base_path=MODEL_PATH,\n",
    ")\n",
    "callback_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40247376",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = datasets[TRAIN_TYPE].repeat()\n",
    "validation_dataset = datasets[TEST_TYPE].repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commons",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=training_dataset,\n",
    "    validation_data=validation_dataset, \n",
    "    validation_steps=STEPS_PER_EPOCH // 5,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=callback_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_idx: Callable[[np.ndarray], np.ndarray] = np.vectorize(idx_to_char.get)\n",
    "\n",
    "eval_evaluator = evaluation.SequenceEvaluator(\n",
    "    dataset=datasets[EVAL_TYPE],\n",
    "    decode_func=decode_idx,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    separator=\" \",\n",
    "    padding_character=PROCESSING_INFO['padding_characters'][SEQ],\n",
    ")\n",
    "\n",
    "train_evaluator = evaluation.SequenceEvaluator(\n",
    "    dataset=datasets[TRAIN_TYPE],\n",
    "    decode_func=decode_idx,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    separator=\" \",\n",
    "    padding_character=PROCESSING_INFO['padding_characters'][SEQ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluator.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df, (x_eval, y_eval, y_pred) = train_evaluator.evaluate_model_visually(\n",
    "    model=model,\n",
    "    sample_size=20,\n",
    "    keep_separator=True,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_evaluator.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e11225",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df, (x_eval, y_eval, y_pred) = eval_evaluator.evaluate_model_visually(\n",
    "    model=model,\n",
    "    sample_size=20,\n",
    "    keep_separator=True,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

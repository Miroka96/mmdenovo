{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Training an ML Model on Tensorflow Datasets\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from typing import Iterable, Callable, Dict, Any, Tuple, Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python import keras as K\n",
    "\n",
    "from mmproteo.utils import log, paths, utils, visualization\n",
    "from mmproteo.utils.formats.mz import FilteringProcessor, MzmlidFileStatsCreator\n",
    "from mmproteo.utils.formats.tf_dataset import Parquet2DatasetFileProcessor\n",
    "from mmproteo.utils.processing import ItemProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Printing to Stdout\n"
     ]
    }
   ],
   "source": [
    "logger = log.DummyLogger(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/workspace/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")\n",
    "PROCESSING_FILE_PATH = os.path.join(DATASET_DUMP_PATH, \"processing_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'\n",
    "MZ = 'mz_array'\n",
    "INT = 'intensity_array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_COLUMNS = [MZ, INT]\n",
    "TARGET_DATA_COLUMNS = [SEQ]\n",
    "SPLIT_VALUE_COLUMNS = ['species', 'istrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c29d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_characters': {'peptide_sequence': '_',\n",
       "  'mz_array': 0.0,\n",
       "  'intensity_array': 0.0},\n",
       " 'padding_lengths': {'mz_array': 2354,\n",
       "  'intensity_array': 2354,\n",
       "  'peptide_sequence': 50},\n",
       " 'idx_to_char': {'0': 'A',\n",
       "  '1': 'C',\n",
       "  '2': 'D',\n",
       "  '3': 'E',\n",
       "  '4': 'F',\n",
       "  '5': 'G',\n",
       "  '6': 'H',\n",
       "  '7': 'I',\n",
       "  '8': 'K',\n",
       "  '9': 'L',\n",
       "  '10': 'M',\n",
       "  '11': 'M(Oxidation)',\n",
       "  '12': 'N',\n",
       "  '13': 'P',\n",
       "  '14': 'Q',\n",
       "  '15': 'R',\n",
       "  '16': 'S',\n",
       "  '17': 'T',\n",
       "  '18': 'V',\n",
       "  '19': 'W',\n",
       "  '20': 'Y',\n",
       "  '21': '_'},\n",
       " 'normalization': {'intensity_array': '<function base_peak_normalize at 0x7fa6046d5158>'},\n",
       " 'split_value_columns': ['species', 'istrain'],\n",
       " 'training_data_columns': ['mz_array', 'intensity_array'],\n",
       " 'target_data_columns': ['peptide_sequence']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PROCESSING_FILE_PATH, 'r') as file:\n",
    "    PROCESSING_INFO = json.loads(file.read())\n",
    "PROCESSING_INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094281b",
   "metadata": {},
   "source": [
    "### ... by species annotation with train-test-eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CACHE = True  # currently, there is no cache; the flag only disables benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c2525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: found file paths dump '../dumps/PXD010000/training_columns/tf_datasets/dataset_file_paths.json'\n",
      "\n",
      "assigned dataset files:\n",
      "#Train = 89\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_C_indologenes_LIB_aerobic_02_03May16_Samwise_16-03-32_mzmlid.parquet/Chryseobacterium_indologenes/Train\n",
      "#Test = 17\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_A_cryptum_FeTSB_anaerobic_1_01Jun16_Pippin_16-03-39_mzmlid.parquet/Acidiphilium_cryptum_JF-5/Train\n",
      "#Eval = 29\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_B_fragilis_CMcarb_anaerobic_01_01Feb16_Arwen_15-07-13_mzmlid.parquet/Bacteroides_fragilis_638R/Train\n"
     ]
    }
   ],
   "source": [
    "dataset_file_paths = paths.assign_wildcard_paths_to_splits_grouped_by_path_position_value(\n",
    "    wildcard_path = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        '*',  # species\n",
    "        '*'   # istrain\n",
    "    ),\n",
    "    path_position = -2,\n",
    "    splits = {\n",
    "            \"Train\": 0.4,\n",
    "            \"Test\": 0.5,\n",
    "            \"Eval\": 0.6\n",
    "        },\n",
    "    paths_dump_file = os.path.join(\n",
    "            DATASET_DUMP_PATH,\n",
    "            \"dataset_file_paths.json\"\n",
    "        ),\n",
    "    skip_existing = KEEP_CACHE,\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"assigned dataset files:\")\n",
    "visualization.print_list_length_in_dict(dataset_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611252",
   "metadata": {},
   "source": [
    "### Loading corresponding TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_spec = ((tf.TensorSpec(shape=(PADDING_LENGTHS[MZ],), dtype=tf.float32), \n",
    "  tf.TensorSpec(shape=(PADDING_LENGTHS[INT],), dtype=tf.float32)),\n",
    "(tf.TensorSpec(shape=(PADDING_LENGTHS[SEQ],), dtype=tf.int8)))\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_datasets = {\n",
    "    training_data_type: tf.data.Dataset.from_tensor_slices(paths).interleave(\n",
    "        lambda path: \n",
    "            tf.data.experimental.load(\n",
    "                path=path, \n",
    "                element_spec=element_spec, \n",
    "                compression='GZIP'\n",
    "            ),\n",
    "         num_parallel_calls=os.cpu_count(),\n",
    "         deterministic=False\n",
    "    )\n",
    "    for training_data_type, paths in dataset_file_paths.items()\n",
    "}\n",
    "\n",
    "merged_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-difficulty",
   "metadata": {},
   "source": [
    "## Configuring Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# although not all data fits into a 100k buffer, the interleaving should make it sufficiently random\n",
    "SHUFFLE_BUFFER_SIZE = 2*10**5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b141e",
   "metadata": {},
   "source": [
    "### Caching (currently abandoned because of too high RAM usage)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4a97453",
   "metadata": {},
   "source": [
    "CACHED_DATASET_DUMP_PATH = os.path.join(DATASET_DUMP_PATH, \"cache\")\n",
    "\n",
    "try:\n",
    "    if not KEEP_CACHE:\n",
    "        shutil.rmtree(CACHED_DATASET_DUMP_PATH)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "utils.ensure_dir_exists(CACHED_DATASET_DUMP_PATH)\n",
    "print(CACHED_DATASET_DUMP_PATH)\n",
    "\n",
    "merged_datasets = {\n",
    "    training_data_type: dataset.cache(os.path.join(CACHED_DATASET_DUMP_PATH, training_data_type))\n",
    "    for training_data_type, dataset in merged_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b491f44",
   "metadata": {},
   "source": [
    "### Preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c881d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cache(dataset, name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Use a benchmark to once process the whole dataset.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        print(f\"{name}:\")\n",
    "    display(tfds.benchmark(dataset))\n",
    "    gc.collect()\n",
    "    logger.info(\"filled a cache - waiting 10 seconds\")\n",
    "    print()\n",
    "    time.sleep(10)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KEEP_CACHE:\n",
    "    merged_datasets = {\n",
    "        training_data_type: fill_cache(dataset, name=training_data_type)\n",
    "        for training_data_type, dataset in merged_datasets.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ee6a5",
   "metadata": {},
   "source": [
    "### Shuffling, Batching, Prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc0466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_datasets = {\n",
    "    training_data_type: dataset\n",
    "        .shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    for training_data_type, dataset in merged_datasets.items()\n",
    "}\n",
    "merged_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TYPE = 'Train'\n",
    "TEST_TYPE = 'Test'\n",
    "EVAL_TYPE = 'Eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_input_layers = {col: tf.keras.layers.Input(shape=(PADDING_LENGTHS[col],), name=col) for col in TRAINING_DATA_COLUMNS}\n",
    "named_input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_input_layers_list = [ named_input_layers[col] for col in TRAINING_DATA_COLUMNS ]\n",
    "named_input_layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input_layers = {\n",
    "    col: tf.keras.layers.Masking(mask_value=PADDING_CHARACTERS[col], name=f\"masked_{col}\")(input_layer)\n",
    "    for col, input_layer in named_input_layers.items()\n",
    "}\n",
    "masked_input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input_layers_list = [ masked_input_layers[col] for col in TRAINING_DATA_COLUMNS ]\n",
    "masked_input_layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(K.losses.LossFunctionWrapper):\n",
    "    def __init__(self, loss_function, masking_value, name='masked_loss', reduction=tf.keras.losses.Reduction.NONE):\n",
    "        def _masked_loss(y_true, y_pred):\n",
    "            y_true = tf.squeeze(y_true, name=\"masked_loss__squeezed_y_true\")\n",
    "            y_pred = tf.squeeze(y_pred, name=\"masked_loss__squeezed_y_pred\")\n",
    "            #print(y_true)\n",
    "            #print(y_pred)\n",
    "            length_mask = tf.equal(y_true, masking_value, name=\"masked_loss__is_masking_value\")\n",
    "            #print(length_mask)\n",
    "            length_mask = tf.cast(length_mask, tf.float32, name=\"masked_loss__is_masking_value_float\")\n",
    "            length_mask = tf.math.subtract(\n",
    "                tf.constant(\n",
    "                    value=1, \n",
    "                    dtype=tf.float32\n",
    "                ), length_mask, name=\"masked_loss__is_masking_value_inverted\")\n",
    "            lengths = tf.math.reduce_sum(length_mask, axis=-1, name=\"masked_loss__sum_to_get_lengths\")\n",
    "            #print(lengths)\n",
    "            lengths = tf.math.add(lengths, 1, name=\"masked_loss__sum_to_include_first_padding\") # to also include the first padding character\n",
    "            #print(lengths)\n",
    "            mask = tf.sequence_mask(\n",
    "                lengths=lengths,\n",
    "                maxlen=y_pred.shape[-2],  # pre-last dimension = padding length; last dimension = one-hot-encoded alphabet\n",
    "                dtype=tf.float32,\n",
    "                name=\"masked_loss__create_sequence_mask\"\n",
    "            )\n",
    "            #print(mask)\n",
    "            losses = loss_function(y_true, y_pred)\n",
    "            #print(losses)\n",
    "            losses = tf.math.multiply(losses, mask, name=\"masked_loss__apply_sequence_mask\")\n",
    "            #print(losses)\n",
    "            summed_losses = tf.math.reduce_sum(losses, axis=-1, name=\"masked_loss__sum_losses\")\n",
    "            #print(summed_losses)\n",
    "            average_losses = tf.math.divide_no_nan(summed_losses, lengths, name=\"masked_loss__average_losses\")\n",
    "            #print(average_losses)\n",
    "            return average_losses\n",
    "            \n",
    "        super(MaskedLoss, self).__init__(_masked_loss, name=name, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_loss = MaskedLoss(\n",
    "    loss_function=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    masking_value=tf.constant(\n",
    "        value=char_to_idx[PADDING_CHARACTERS[SEQ]],\n",
    "        dtype=tf.int8\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b445c7a",
   "metadata": {},
   "source": [
    "masked_loss(y_eval[:2], y_pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-geneva",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = masked_input_layers_list[0]\n",
    "for input_layer in masked_input_layers_list[1:]:\n",
    "    x = x + input_layer\n",
    "\n",
    "x = tf.keras.layers.Flatten(name=\"flattened_masked_inputs\")(x)\n",
    "\n",
    "for _ in range(4):\n",
    "    x = tf.keras.layers.Dense(2**11)(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(PADDING_LENGTHS[SEQ]*len(ALPHABET))(x)\n",
    "\n",
    "x = tf.reshape(x,(-1, PADDING_LENGTHS[SEQ], len(ALPHABET)))\n",
    "\n",
    "x = tf.keras.activations.softmax(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=named_input_layers_list, outputs=x, name='mmproteo')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=masked_loss,\n",
    "              metrics=[\n",
    "                  tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "                  tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "              ]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05152e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_DIR = os.path.join(DUMP_PATH, \"tensorboard\")\n",
    "TENSORBOARD_LOG_DIR = os.path.join(TENSORBOARD_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "try:\n",
    "    shutil.rmtree(TENSORBOARD_DIR)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ea212",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(TENSORBOARD_LOG_DIR, \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "    histogram_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x=merged_datasets[TRAINING_TYPE].repeat(),\n",
    "          validation_data=merged_datasets[TEST_TYPE].repeat(), \n",
    "          validation_steps=500,\n",
    "          epochs=30,\n",
    "          steps_per_epoch=10_000,\n",
    "          callbacks=[tensorboard_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(merged_datasets[EVAL_TYPE].repeat(), steps=int(40000/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(tuple_list: Iterable[Tuple[Any, Any]]) -> Tuple[Iterable[Any], Iterable[Any]]:\n",
    "    return tuple(zip(*tuple_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPARATOR = \" \"\n",
    "PREDICTED = \"predicted\"\n",
    "TRUE = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_onehot(array: np.ndarray) -> np.ndarray:\n",
    "    return np.argmax(array, axis=-1)\n",
    "\n",
    "decode_idx: Callable[[np.ndarray], np.ndarray] = np.vectorize(idx_to_char.get)\n",
    "\n",
    "def concat_letter_rows(array: np.ndarray) -> np.ndarray:\n",
    "    return np.apply_along_axis(lambda row: SEPARATOR.join(row), axis=-1, arr=array)\n",
    "\n",
    "def decode(array: np.ndarray, onehot: bool = True):\n",
    "    if onehot:\n",
    "        array = decode_onehot(array)\n",
    "    array = decode_idx(array)\n",
    "    array = concat_letter_rows(array)\n",
    "    if not onehot:\n",
    "        array = np.apply_along_axis(lambda row: row[0], axis=-1, arr=array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24454d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = merged_datasets[EVAL_TYPE].unbatch().batch(1).take(20)\n",
    "\n",
    "x_eval, y_eval = unzip(eval_ds.as_numpy_iterator())\n",
    "y_pred = model.predict(eval_ds)\n",
    "\n",
    "# although the strings look like they have different lengths, they all have the same length\n",
    "eval_df = pd.DataFrame(data=zip(decode(y_pred), decode(y_eval, onehot=False)), columns=[PREDICTED, TRUE])\n",
    "\n",
    "eval_df[PREDICTED] = eval_df[PREDICTED].combine(\n",
    "    other=eval_df[TRUE].str.rstrip(PADDING_CHARACTERS[SEQ] + SEPARATOR).str.split(SEPARATOR).str.len() + 1,\n",
    "    func=lambda seq, length: SEPARATOR.join(seq.split(SEPARATOR)[:length])\n",
    ")\n",
    "\n",
    "#eval_df = eval_df.applymap(lambda s: s.replace(SEPARATOR, \"\"))\n",
    "\n",
    "eval_df[TRUE] = eval_df[TRUE].str.rstrip(PADDING_CHARACTERS[SEQ] + SEPARATOR)\n",
    "\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.predicted.map(print)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval[:1], y_pred[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a555d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

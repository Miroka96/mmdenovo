{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Hyper Tuning a CNN-LSTM Model on Tensorflow Datasets (run on a server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101df8d",
   "metadata": {},
   "source": [
    "## Preparation on a server\n",
    "\n",
    "* create a conda environment with Tensorflow GPU support:\n",
    "`conda create -n mmproteo tensorflow-gpu python=3.8 jupyter`\n",
    "    * pay attention that all additional tensorflow packages have (roughly) the same version\n",
    "* `conda activate mmproteo`\n",
    "* `pip install -r requirements.txt` (the major `requirements.txt` file from the pride-downloader package, not just the mmproteo one)\n",
    "* start jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc811a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core file size          (blocks, -c) 0\r\n",
      "data seg size           (kbytes, -d) unlimited\r\n",
      "scheduling priority             (-e) 0\r\n",
      "file size               (blocks, -f) unlimited\r\n",
      "pending signals                 (-i) 4127177\r\n",
      "max locked memory       (kbytes, -l) unlimited\r\n",
      "max memory size         (kbytes, -m) unlimited\r\n",
      "open files                      (-n) 500000\r\n",
      "pipe size            (512 bytes, -p) 8\r\n",
      "POSIX message queues     (bytes, -q) 819200\r\n",
      "real-time priority              (-r) 0\r\n",
      "stack size              (kbytes, -s) 8192\r\n",
      "cpu time               (seconds, -t) unlimited\r\n",
      "max user processes              (-u) 4127177\r\n",
      "virtual memory          (kbytes, -v) unlimited\r\n",
      "file locks                      (-x) unlimited\r\n"
     ]
    }
   ],
   "source": [
    "!ulimit -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d28cbc",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from official.nlp.modeling.layers.position_embedding import RelativePositionEmbedding\n",
    "import keras_tuner as kt\n",
    "from mmproteo.utils import log, paths, utils, visualization\n",
    "from mmproteo.utils.formats.tf_dataset import DatasetLoader\n",
    "from mmproteo.utils.ml import callbacks, evaluation, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86274548",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d056062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "GPUs = tf.config.list_physical_devices('GPU')\n",
    "GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d683300c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_gpu = GPUs[0]\n",
    "current_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hpi/fs00/home/mirko.krause/masterthesis/pride-downloader/notebooks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac43a97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "os.chdir(os.path.join('workspace', 'notebooks'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d57e9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a01f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3b58e74",
   "metadata": {},
   "source": [
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d12ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_PATH = \"/scratch/mirko.krause/dumps/\" + PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_COUNT = min(int(os.cpu_count()/2), 16)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")\n",
    "PROCESSING_FILE_PATH = os.path.join(DATASET_DUMP_PATH, \"processing_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c266740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-12 19:58:31,963 - mmproteo_hypertuning_cnn_lstm: Logging to file '/scratch/mirko.krause/dumps/PXD010000/mmproteo_hypertuning_cnn_lstm.log' and to stderr\n"
     ]
    }
   ],
   "source": [
    "logger = log.create_logger(\n",
    "    name='mmproteo_hypertuning_cnn_lstm',\n",
    "    verbose=True,\n",
    "    log_dir=DUMP_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c29d41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_characters': {'peptide_sequence': '_',\n",
       "  'mz_array': 0.0,\n",
       "  'intensity_array': 0.0},\n",
       " 'padding_lengths': {'mz_array': 2354,\n",
       "  'intensity_array': 2354,\n",
       "  'peptide_sequence': 50},\n",
       " 'idx_to_char': {'0': 'A',\n",
       "  '1': 'C',\n",
       "  '2': 'D',\n",
       "  '3': 'E',\n",
       "  '4': 'F',\n",
       "  '5': 'G',\n",
       "  '6': 'H',\n",
       "  '7': 'I',\n",
       "  '8': 'K',\n",
       "  '9': 'L',\n",
       "  '10': 'M',\n",
       "  '11': 'M(Oxidation)',\n",
       "  '12': 'N',\n",
       "  '13': 'P',\n",
       "  '14': 'Q',\n",
       "  '15': 'R',\n",
       "  '16': 'S',\n",
       "  '17': 'T',\n",
       "  '18': 'V',\n",
       "  '19': 'W',\n",
       "  '20': 'Y',\n",
       "  '21': '_'},\n",
       " 'normalization': {'intensity_array': '<function base_peak_normalize at 0x7f8e5c0cf550>'},\n",
       " 'split_value_columns': ['species', 'istrain'],\n",
       " 'training_data_columns': ['mz_array', 'intensity_array'],\n",
       " 'target_data_columns': ['peptide_sequence']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PROCESSING_FILE_PATH, 'r') as file:\n",
    "    PROCESSING_INFO = json.loads(file.read())\n",
    "PROCESSING_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = {int(idx): char for idx, char in PROCESSING_INFO[\"idx_to_char\"].items()}\n",
    "char_to_idx = {char: idx for idx, char in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CACHE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b34d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TYPE = 'Train'\n",
    "TEST_TYPE = 'Test'\n",
    "EVAL_TYPE = 'Eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d696d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assigned dataset files:\n",
      "#Train = 89\n",
      "e.g.: /scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/Biodiversity_P_polymyxa_TBS_aerobic_1_17July16_Samwise_16-04-10_mzmlid.parquet/Paenibacillus_polymyxa_ATCC842/Train\n",
      "#Test = 27\n",
      "e.g.: /scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/Cj_media_MH_R3_23Feb15_Arwen_14-12-03_mzmlid.parquet/Campylobacter_jejuni/Train\n",
      "#Eval = 38\n",
      "e.g.: /scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/M_alcali_copp_CH4_B3_T1_11_QE_23Mar18_Oak_18-01-07_mzmlid.parquet/Methylomicrobium_alcaliphilum/Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-12 19:58:31,981 - mmproteo_hypertuning_cnn_lstm: found file paths dump '/scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/dataset_file_paths.json'\n"
     ]
    }
   ],
   "source": [
    "dataset_file_paths = paths.assign_wildcard_paths_to_splits_grouped_by_path_position_value(\n",
    "    wildcard_path = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        '*',  # species\n",
    "        '*'   # istrain\n",
    "    ),\n",
    "    path_position = -2,\n",
    "    splits = {\n",
    "            TRAINING_TYPE: 0.4,\n",
    "            TEST_TYPE: 0.5,\n",
    "            EVAL_TYPE: 0.6\n",
    "        },\n",
    "    paths_dump_file = os.path.join(\n",
    "            DATASET_DUMP_PATH,\n",
    "            \"dataset_file_paths.json\"\n",
    "        ),\n",
    "    skip_existing = KEEP_CACHE,\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"assigned dataset files:\")\n",
    "visualization.print_list_length_in_dict(dataset_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611252",
   "metadata": {},
   "source": [
    "### Loading corresponding TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d829d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(2354,), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2354,), dtype=tf.float32, name=None)),\n",
       " (TensorSpec(shape=(50,), dtype=tf.int8, name=None),))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = (\n",
    "    tuple(tf.TensorSpec(shape=(PROCESSING_INFO['padding_lengths'][col], ), dtype=tf.float32)\n",
    "     for col in PROCESSING_INFO['training_data_columns']),\n",
    "    tuple(tf.TensorSpec(shape=(PROCESSING_INFO['padding_lengths'][col], ), dtype=tf.int8)\n",
    "     for col in PROCESSING_INFO['target_data_columns'])\n",
    ")\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7245d0a",
   "metadata": {},
   "source": [
    "**In the following step, Tensorflow starts allocating a GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c45ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RangeDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize Tensorflow (might take several minutes (~5))\n",
    "tf.data.Dataset.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c5890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_options = tf.data.Options()\n",
    "ds_options.experimental_threading.private_threadpool_size = THREAD_COUNT\n",
    "ds_options.experimental_threading.max_intra_op_parallelism = THREAD_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbef3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-12 20:13:34,421 - mmproteo_hypertuning_cnn_lstm: DEBUG: preparing dataset 'Train' with 89 paths\n",
      "2021-07-12 20:13:34,423 - mmproteo_hypertuning_cnn_lstm: DEBUG: applied options to dataset 'Train'\n",
      "2021-07-12 20:13:34,434 - mmproteo_hypertuning_cnn_lstm: DEBUG: loaded dataset 'Train' interleaved\n",
      "2021-07-12 20:13:34,434 - mmproteo_hypertuning_cnn_lstm: DEBUG: shuffled dataset 'Train'\n",
      "2021-07-12 20:13:34,435 - mmproteo_hypertuning_cnn_lstm: DEBUG: batched dataset 'Train'\n",
      "2021-07-12 20:13:34,435 - mmproteo_hypertuning_cnn_lstm: DEBUG: skipped caching dataset 'Train'\n",
      "2021-07-12 20:13:34,435 - mmproteo_hypertuning_cnn_lstm: DEBUG: configured prefetching for dataset 'Train'\n",
      "2021-07-12 20:13:34,435 - mmproteo_hypertuning_cnn_lstm: DEBUG: skipped benchmarking dataset 'Train'\n",
      "2021-07-12 20:13:34,435 - mmproteo_hypertuning_cnn_lstm: prepared dataset 'Train'\n",
      "2021-07-12 20:13:34,436 - mmproteo_hypertuning_cnn_lstm: DEBUG: preparing dataset 'Test' with 27 paths\n",
      "2021-07-12 20:13:34,436 - mmproteo_hypertuning_cnn_lstm: DEBUG: applied options to dataset 'Test'\n",
      "2021-07-12 20:13:34,445 - mmproteo_hypertuning_cnn_lstm: DEBUG: loaded dataset 'Test' interleaved\n",
      "2021-07-12 20:13:34,445 - mmproteo_hypertuning_cnn_lstm: DEBUG: shuffled dataset 'Test'\n",
      "2021-07-12 20:13:34,445 - mmproteo_hypertuning_cnn_lstm: DEBUG: batched dataset 'Test'\n",
      "2021-07-12 20:13:34,446 - mmproteo_hypertuning_cnn_lstm: DEBUG: skipped caching dataset 'Test'\n",
      "2021-07-12 20:13:34,446 - mmproteo_hypertuning_cnn_lstm: DEBUG: configured prefetching for dataset 'Test'\n",
      "2021-07-12 20:13:34,446 - mmproteo_hypertuning_cnn_lstm: DEBUG: skipped benchmarking dataset 'Test'\n",
      "2021-07-12 20:13:34,446 - mmproteo_hypertuning_cnn_lstm: prepared dataset 'Test'\n",
      "2021-07-12 20:13:34,447 - mmproteo_hypertuning_cnn_lstm: DEBUG: preparing dataset 'Eval' with 38 paths\n",
      "2021-07-12 20:13:34,447 - mmproteo_hypertuning_cnn_lstm: DEBUG: applied options to dataset 'Eval'\n",
      "2021-07-12 20:13:34,455 - mmproteo_hypertuning_cnn_lstm: DEBUG: loaded dataset 'Eval' interleaved\n",
      "2021-07-12 20:13:34,456 - mmproteo_hypertuning_cnn_lstm: DEBUG: shuffled dataset 'Eval'\n",
      "2021-07-12 20:13:34,456 - mmproteo_hypertuning_cnn_lstm: DEBUG: batched dataset 'Eval'\n",
      "2021-07-12 20:13:34,457 - mmproteo_hypertuning_cnn_lstm: DEBUG: skipped caching dataset 'Eval'\n",
      "2021-07-12 20:13:34,457 - mmproteo_hypertuning_cnn_lstm: DEBUG: configured prefetching for dataset 'Eval'\n",
      "2021-07-12 20:13:34,457 - mmproteo_hypertuning_cnn_lstm: DEBUG: skipped benchmarking dataset 'Eval'\n",
      "2021-07-12 20:13:34,457 - mmproteo_hypertuning_cnn_lstm: prepared dataset 'Eval'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Train': <PrefetchDataset shapes: (((32, 2354), (32, 2354)), ((32, 50),)), types: ((tf.float32, tf.float32), (tf.int8,))>,\n",
       " 'Test': <PrefetchDataset shapes: (((32, 2354), (32, 2354)), ((32, 50),)), types: ((tf.float32, tf.float32), (tf.int8,))>,\n",
       " 'Eval': <PrefetchDataset shapes: (((32, 2354), (32, 2354)), ((32, 50),)), types: ((tf.float32, tf.float32), (tf.int8,))>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = DatasetLoader(\n",
    "    element_spec=element_spec,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=100_000,\n",
    "    thread_count=min(int(os.cpu_count()/4), 4),\n",
    "    keep_cache=KEEP_CACHE,\n",
    "    logger=logger,\n",
    "    run_benchmarks=False,\n",
    "    options=ds_options,\n",
    ").load_datasets_by_type(dataset_file_paths)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Defining the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8f44fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_with_pooling_and_position_model(\n",
    "    hp: kt.HyperParameters,\n",
    "    model_name: str = \"mmproteo_lstm_with_pooling_and_position\"\n",
    ") -> tf.keras.Model:\n",
    "    input_layers_list, masked_input_layers_list = layers.create_masked_input_layers(\n",
    "        [\n",
    "            layers.InputLayerConfiguration(\n",
    "                name=col,\n",
    "                shape=PROCESSING_INFO['padding_lengths'][col],\n",
    "                mask_value=PROCESSING_INFO['padding_characters'][col]\n",
    "            )\n",
    "            for col in PROCESSING_INFO['training_data_columns']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    x = tf.stack(\n",
    "        values=masked_input_layers_list, \n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    # position_embedding_size = hp.Int('position_embedding_size', min_value=4, max_value=20, step=4)\n",
    "    position_embedding_size = hp.Fixed('position_embedding_size', value=12)\n",
    "    \n",
    "    position_embedding = RelativePositionEmbedding(\n",
    "        hidden_size=position_embedding_size,\n",
    "        name='relative_position_embedding'\n",
    "    )(x)\n",
    "    position_embedding = tf.expand_dims(position_embedding, 0)\n",
    "    position_embedding = tf.broadcast_to(\n",
    "        input=position_embedding, \n",
    "        shape=(tf.shape(x)[0], *tf.shape(position_embedding)[1:])\n",
    "    )\n",
    "    \n",
    "    y_layers=[position_embedding]\n",
    "    \n",
    "    hp_y_time_distributed_dense_units_exponent = hp.Int('y_time_distributed_dense_units_exponent', min_value=4, max_value=6, step=1)\n",
    "    dense_y = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=2**hp_y_time_distributed_dense_units_exponent,\n",
    "            activation='relu',\n",
    "            name='y_time_distributed_dense',\n",
    "        )\n",
    "    )(x)\n",
    "    y_layers.append(dense_y)\n",
    "    \n",
    "    base_conv_filter_count = hp.Int('y_base_conv_filter_count', min_value=8, max_value=16, step=8)\n",
    "    base_conv_kernel_size = hp.Int('y_base_conv_kernel_size', min_value=4, max_value=8, step=4)\n",
    "\n",
    "    for i in range(hp.Int('y_number_of_convolutions', min_value=2, max_value=4, step=2)):\n",
    "        filter_count = base_conv_filter_count * (i+1)\n",
    "        kernel_size = base_conv_kernel_size ** min(i, 3)\n",
    "        cnn_y = tf.keras.layers.Conv1D(\n",
    "            filters=filter_count,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name=f\"y_conv_{kernel_size}_{filter_count}\",\n",
    "        )(x)\n",
    "        y_layers.append(cnn_y)\n",
    "    \n",
    "    x = tf.concat(\n",
    "        values=y_layers,\n",
    "        axis=-1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    y = x\n",
    "    for i in range(hp.Int('number_of_time_distributed_dense_layers', min_value=0, max_value=2, step=1)):\n",
    "        y = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(\n",
    "                units=x.shape[1],\n",
    "                activation='relu',\n",
    "                name=f'pre_lstm_time_distributed_dense_{i}',\n",
    "            )\n",
    "        )(y)\n",
    "    \n",
    "    x = tf.concat(\n",
    "        values=[x, y],\n",
    "        axis=-1,\n",
    "    )\n",
    "    \n",
    "    bidirectional_lstm_units_exponent = hp.Int('bidirectional_lstm_units_exponent', min_value=5, max_value=9, step=1)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        layer=tf.keras.layers.LSTM(\n",
    "            units=2**bidirectional_lstm_units_exponent,\n",
    "            return_sequences=True,\n",
    "            name='lstm'\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalMaxPooling1D(\n",
    "        name='global_max_pooling_over_time',\n",
    "    )(x)\n",
    "    \n",
    "    upscaling_dense_max = hp.Int('upscaling_dense_layer_generator_max', min_value=1, max_value=2, step=1)\n",
    "    upscaling_dense_offset = hp.Int('upscaling_dense_layer_generator_offset', min_value=0, max_value=1, step=1)\n",
    "    for i in range(upscaling_dense_max):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            units=2**(8 + i + upscaling_dense_offset),\n",
    "            activation='relu',\n",
    "            name=f\"upscaling_dense_{i}\",\n",
    "        )(x)\n",
    "    \n",
    "    final_dense_feature_units = hp.Int('final_dense_layer_over_length_feature_units', min_value=4, max_value=8, step=4)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=PROCESSING_INFO['padding_lengths'][SEQ] * final_dense_feature_units,\n",
    "        activation='relu',\n",
    "        name=\"final_dense_layer_to_redefine_lengths\",\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.reshape(x, (-1, PROCESSING_INFO['padding_lengths'][SEQ], final_dense_feature_units))\n",
    "    \n",
    "    x = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=len(idx_to_char),\n",
    "            activation=None,\n",
    "            name='final_time_distributed_dense'\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.activations.softmax(x)\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=input_layers_list, \n",
    "        outputs=x, \n",
    "        name=f\"{model_name}_{utils.get_current_time_str()}\")\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "            tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52e0d115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mirko.krause/dumps/PXD010000/models/tuner'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TUNER_PATH = os.path.join(DUMP_PATH, 'models', 'tuner')\n",
    "TUNER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdf3a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ensure_dir_exists(TUNER_PATH)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35a0be1d",
   "metadata": {},
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    build_lstm_with_pooling_and_position_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,\n",
    "    num_initial_points=3,\n",
    "    directory=TUNER_PATH,\n",
    "    project_name='mmproteo-cnn-lstm',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d74021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project /scratch/mirko.krause/dumps/PXD010000/models/tuner/mmproteo-cnn-lstm/oracle.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_lstm_with_pooling_and_position_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory=TUNER_PATH,\n",
    "    project_name='mmproteo-cnn-lstm',\n",
    "    overwrite=False,\n",
    "    executions_per_trial=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50f9fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 11\n",
      "position_embedding_size (Fixed)\n",
      "{'conditions': [], 'value': 12}\n",
      "y_time_distributed_dense_units_exponent (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 6, 'step': 1, 'sampling': None}\n",
      "y_base_conv_filter_count (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 8, 'max_value': 16, 'step': 8, 'sampling': None}\n",
      "y_base_conv_kernel_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 8, 'step': 4, 'sampling': None}\n",
      "y_number_of_convolutions (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 2, 'sampling': None}\n",
      "number_of_time_distributed_dense_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 0, 'max_value': 2, 'step': 1, 'sampling': None}\n",
      "bidirectional_lstm_units_exponent (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 5, 'max_value': 9, 'step': 1, 'sampling': None}\n",
      "upscaling_dense_layer_generator_max (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': None}\n",
      "upscaling_dense_layer_generator_offset (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 0, 'max_value': 1, 'step': 1, 'sampling': None}\n",
      "final_dense_layer_over_length_feature_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 8, 'step': 4, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "630d443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05152e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mirko.krause/dumps/PXD010000/models/tuner/tensorboard'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSORBOARD_LOG_DIR = os.path.join(TUNER_PATH, \"tensorboard\")\n",
    "os.path.realpath(TENSORBOARD_LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe45638f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 1760026), started 5:48:18 ago. (Use '!kill 1760026' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9472817fba38ee27\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9472817fba38ee27\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b37af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "position_embedd...|12                |?                 \n",
      "y_time_distribu...|4                 |?                 \n",
      "y_base_conv_fil...|8                 |?                 \n",
      "y_base_conv_ker...|8                 |?                 \n",
      "y_number_of_con...|4                 |?                 \n",
      "number_of_time_...|1                 |?                 \n",
      "bidirectional_l...|9                 |?                 \n",
      "upscaling_dense...|1                 |?                 \n",
      "upscaling_dense...|1                 |?                 \n",
      "final_dense_lay...|4                 |?                 \n",
      "learning_rate     |0.01              |?                 \n",
      "tuner/epochs      |2                 |?                 \n",
      "tuner/initial_e...|0                 |?                 \n",
      "tuner/bracket     |2                 |?                 \n",
      "tuner/round       |0                 |?                 \n",
      "\n",
      "Epoch 1/2\n",
      "312/312 [==============================] - 661s 2s/step - loss: 1.5713 - sparse_categorical_accuracy: 0.5869 - sparse_categorical_crossentropy: 1.5713 - val_loss: 1.2699 - val_sparse_categorical_accuracy: 0.6635 - val_sparse_categorical_crossentropy: 1.2699\n",
      "Epoch 2/2\n",
      "298/312 [===========================>..] - ETA: 18s - loss: 1.3290 - sparse_categorical_accuracy: 0.6386 - sparse_categorical_crossentropy: 1.3290"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    x=datasets[TRAINING_TYPE].repeat(),\n",
    "    validation_data=datasets[TEST_TYPE].repeat(), \n",
    "    validation_steps=500,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(10_000/BATCH_SIZE),\n",
    "    callbacks=callbacks.create_callbacks(\n",
    "        tensorboard=True,\n",
    "        progressbar=False,\n",
    "        reduce_lr=True,\n",
    "        early_stopping=True,\n",
    "        checkpoints=False,\n",
    "        csv=False,\n",
    "        base_path=TUNER_PATH,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0968576",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters(num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50488d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(DUMP_PATH, \"models\", model.name)\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5481a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ensure_dir_exists(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model,\n",
    "    to_file=os.path.join(MODEL_PATH, \"model.png\"),\n",
    "    show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd410eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"summary.txt\"), 'w') as file:\n",
    "    def write_lines(line: str) -> None:\n",
    "        file.write(line)\n",
    "        file.write(\"\\n\")\n",
    "    model.summary(print_fn=write_lines)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.json\"), 'w') as file:\n",
    "    file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.yaml\"), 'w') as file:\n",
    "    file.write(model.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=datasets[TRAINING_TYPE].repeat(),\n",
    "          validation_data=datasets[TEST_TYPE].repeat(), \n",
    "          validation_steps=500,\n",
    "          epochs=1,\n",
    "          steps_per_epoch=1_000,\n",
    "          callbacks=callbacks.create_callbacks(\n",
    "              tensorboard=True,\n",
    "              progressbar=False,\n",
    "              reduce_lr=True,\n",
    "              early_stopping=True,\n",
    "              checkpoints=True,\n",
    "              csv=True,\n",
    "              base_path=MODEL_PATH,\n",
    "          )\n",
    "         )\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_idx: Callable[[np.ndarray], np.ndarray] = np.vectorize(idx_to_char.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation.SequenceEvaluator(\n",
    "    dataset=datasets[EVAL_TYPE],\n",
    "    decode_func=decode_idx,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    separator=\" \",\n",
    "    padding_character=PROCESSING_INFO['padding_characters'][SEQ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917eb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df, (x_eval, y_eval, y_pred) = evaluator.evaluate_model_visually(\n",
    "    model=model,\n",
    "    sample_size=20,\n",
    "    keep_separator=True,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.predicted.map(print)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d87b01",
   "metadata": {},
   "source": [
    "broken loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caff47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(datasets[EVAL_TYPE].take(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052447fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Hyper Tuning a CNN-LSTM Model on Tensorflow Datasets (run on a server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101df8d",
   "metadata": {},
   "source": [
    "## Preparation on a server\n",
    "\n",
    "* create a conda environment with Tensorflow GPU support:\n",
    "`conda create -n mmproteo tensorflow-gpu python=3.8 jupyter`\n",
    "    * pay attention that all additional tensorflow packages have (roughly) the same version\n",
    "* `conda activate mmproteo`\n",
    "* `pip install -r requirements.txt` (the major `requirements.txt` file from the pride-downloader package, not just the mmproteo one)\n",
    "* start jupyter server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d28cbc",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from official.nlp.modeling.layers.position_embedding import RelativePositionEmbedding\n",
    "import keras_tuner as kt\n",
    "from mmproteo.utils import log, paths, utils, visualization\n",
    "from mmproteo.utils.formats.tf_dataset import DatasetLoader\n",
    "from mmproteo.utils.ml import callbacks, evaluation, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Printing to Stdout\n"
     ]
    }
   ],
   "source": [
    "logger = log.DummyLogger(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86274548",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d056062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "GPUs = tf.config.list_physical_devices('GPU')\n",
    "GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d683300c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_gpu = GPUs[0]\n",
    "current_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hpi/fs00/home/mirko.krause/masterthesis/pride-downloader/notebooks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac43a97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "os.chdir(os.path.join('workspace', 'notebooks'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d57e9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a01f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3b58e74",
   "metadata": {},
   "source": [
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)\n",
    "THREAD_COUNT=os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d12ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_PATH = \"/scratch/mirko.krause/dumps/\" + PROJECT\n",
    "THREAD_COUNT=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")\n",
    "PROCESSING_FILE_PATH = os.path.join(DATASET_DUMP_PATH, \"processing_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c29d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_characters': {'peptide_sequence': '_',\n",
       "  'mz_array': 0.0,\n",
       "  'intensity_array': 0.0},\n",
       " 'padding_lengths': {'mz_array': 2354,\n",
       "  'intensity_array': 2354,\n",
       "  'peptide_sequence': 50},\n",
       " 'idx_to_char': {'0': 'A',\n",
       "  '1': 'C',\n",
       "  '2': 'D',\n",
       "  '3': 'E',\n",
       "  '4': 'F',\n",
       "  '5': 'G',\n",
       "  '6': 'H',\n",
       "  '7': 'I',\n",
       "  '8': 'K',\n",
       "  '9': 'L',\n",
       "  '10': 'M',\n",
       "  '11': 'M(Oxidation)',\n",
       "  '12': 'N',\n",
       "  '13': 'P',\n",
       "  '14': 'Q',\n",
       "  '15': 'R',\n",
       "  '16': 'S',\n",
       "  '17': 'T',\n",
       "  '18': 'V',\n",
       "  '19': 'W',\n",
       "  '20': 'Y',\n",
       "  '21': '_'},\n",
       " 'normalization': {'intensity_array': '<function base_peak_normalize at 0x7f8e5c0cf550>'},\n",
       " 'split_value_columns': ['species', 'istrain'],\n",
       " 'training_data_columns': ['mz_array', 'intensity_array'],\n",
       " 'target_data_columns': ['peptide_sequence']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PROCESSING_FILE_PATH, 'r') as file:\n",
    "    PROCESSING_INFO = json.loads(file.read())\n",
    "PROCESSING_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = {int(idx): char for idx, char in PROCESSING_INFO[\"idx_to_char\"].items()}\n",
    "char_to_idx = {char: idx for idx, char in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CACHE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TYPE = 'Train'\n",
    "TEST_TYPE = 'Test'\n",
    "EVAL_TYPE = 'Eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d696d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: found file paths dump '/scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/dataset_file_paths.json'\n",
      "\n",
      "assigned dataset files:\n",
      "#Train = 89\n",
      "e.g.: /scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/Biodiversity_P_polymyxa_TBS_aerobic_1_17July16_Samwise_16-04-10_mzmlid.parquet/Paenibacillus_polymyxa_ATCC842/Train\n",
      "#Test = 27\n",
      "e.g.: /scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/Cj_media_MH_R3_23Feb15_Arwen_14-12-03_mzmlid.parquet/Campylobacter_jejuni/Train\n",
      "#Eval = 38\n",
      "e.g.: /scratch/mirko.krause/dumps/PXD010000/training_columns/tf_datasets/M_alcali_copp_CH4_B3_T1_11_QE_23Mar18_Oak_18-01-07_mzmlid.parquet/Methylomicrobium_alcaliphilum/Train\n"
     ]
    }
   ],
   "source": [
    "dataset_file_paths = paths.assign_wildcard_paths_to_splits_grouped_by_path_position_value(\n",
    "    wildcard_path = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        '*',  # species\n",
    "        '*'   # istrain\n",
    "    ),\n",
    "    path_position = -2,\n",
    "    splits = {\n",
    "            TRAINING_TYPE: 0.4,\n",
    "            TEST_TYPE: 0.5,\n",
    "            EVAL_TYPE: 0.6\n",
    "        },\n",
    "    paths_dump_file = os.path.join(\n",
    "            DATASET_DUMP_PATH,\n",
    "            \"dataset_file_paths.json\"\n",
    "        ),\n",
    "    skip_existing = KEEP_CACHE,\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"assigned dataset files:\")\n",
    "visualization.print_list_length_in_dict(dataset_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611252",
   "metadata": {},
   "source": [
    "### Loading corresponding TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d829d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(2354,), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2354,), dtype=tf.float32, name=None)),\n",
       " (TensorSpec(shape=(50,), dtype=tf.int8, name=None),))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = (\n",
    "    tuple(tf.TensorSpec(shape=(PROCESSING_INFO['padding_lengths'][col], ), dtype=tf.float32)\n",
    "     for col in PROCESSING_INFO['training_data_columns']),\n",
    "    tuple(tf.TensorSpec(shape=(PROCESSING_INFO['padding_lengths'][col], ), dtype=tf.int8)\n",
    "     for col in PROCESSING_INFO['target_data_columns'])\n",
    ")\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7245d0a",
   "metadata": {},
   "source": [
    "**In the following step, Tensorflow starts allocating a GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbef3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: preparing dataset 'Train'\n",
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV4 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: loaded dataset 'Train' interleaved\n",
      "Executing op DummySeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV3 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: shuffled dataset 'Train'\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: batched dataset 'Train'\n",
      "DEBUG: cached dataset 'Train'\n",
      "DEBUG: benchmarked dataset 'Train'\n",
      "INFO: prepared dataset 'Train'\n",
      "DEBUG: preparing dataset 'Test'\n",
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV4 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: loaded dataset 'Test' interleaved\n",
      "Executing op DummySeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV3 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: shuffled dataset 'Test'\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: batched dataset 'Test'\n",
      "DEBUG: cached dataset 'Test'\n",
      "DEBUG: benchmarked dataset 'Test'\n",
      "INFO: prepared dataset 'Test'\n",
      "DEBUG: preparing dataset 'Eval'\n",
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV4 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: loaded dataset 'Eval' interleaved\n",
      "Executing op DummySeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV3 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: shuffled dataset 'Eval'\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "DEBUG: batched dataset 'Eval'\n",
      "DEBUG: cached dataset 'Eval'\n",
      "DEBUG: benchmarked dataset 'Eval'\n",
      "INFO: prepared dataset 'Eval'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Train': <BatchDataset shapes: (((32, 2354), (32, 2354)), ((32, 50),)), types: ((tf.float32, tf.float32), (tf.int8,))>,\n",
       " 'Test': <BatchDataset shapes: (((32, 2354), (32, 2354)), ((32, 50),)), types: ((tf.float32, tf.float32), (tf.int8,))>,\n",
       " 'Eval': <BatchDataset shapes: (((32, 2354), (32, 2354)), ((32, 50),)), types: ((tf.float32, tf.float32), (tf.int8,))>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = DatasetLoader(\n",
    "    element_spec=element_spec,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=1_000_000,\n",
    "    thread_count=THREAD_COUNT,\n",
    "    keep_cache=KEEP_CACHE,\n",
    "    logger=logger\n",
    ").load_datasets_by_type(dataset_file_paths)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Defining the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f44fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_with_pooling_and_position_model(\n",
    "    hp: kt.HyperParameters,\n",
    "    model_name: str = \"mmproteo_lstm_with_pooling_and_position\"\n",
    ") -> tf.keras.Model:\n",
    "    input_layers_list, masked_input_layers_list = layers.create_masked_input_layers(\n",
    "        [\n",
    "            layers.InputLayerConfiguration(\n",
    "                name=col,\n",
    "                shape=PROCESSING_INFO['padding_lengths'][col],\n",
    "                mask_value=PROCESSING_INFO['padding_characters'][col]\n",
    "            )\n",
    "            for col in PROCESSING_INFO['training_data_columns']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    x = tf.stack(\n",
    "        values=masked_input_layers_list, \n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    position_embedding_size = hp.Int('position_embedding_size', min_value=4, max_value=20, step=1)\n",
    "    \n",
    "    position_embedding = RelativePositionEmbedding(\n",
    "        hidden_size=position_embedding_size,\n",
    "        name='relative_position_embedding'\n",
    "    )(x)\n",
    "    position_embedding = tf.expand_dims(position_embedding, 0)\n",
    "    position_embedding = tf.broadcast_to(\n",
    "        input=position_embedding, \n",
    "        shape=(tf.shape(x)[0], *tf.shape(position_embedding)[1:])\n",
    "    )\n",
    "    \n",
    "    y_layers=[position_embedding]\n",
    "    \n",
    "    dense_y = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=hp.Int('y_time_distributed_dense_units', min_value=4, max_value=64, step=4),\n",
    "            activation='relu',\n",
    "            name='y_time_distributed_dense',\n",
    "        )\n",
    "    )(x)\n",
    "    y_layers.append(dense_y)\n",
    "    \n",
    "    for i in range(hp.Int('y_number_of_convolutions', min_value=1, max_value=8, step=1)):\n",
    "        filter_count = hp.Int('y_base_conv_filter_count', min_value=4, max_value=20, step=4) * (i+1)\n",
    "        kernel_size = hp.Int('y_base_conv_kernel_size', min_value=4, max_value=16, step=2)**i\n",
    "        cnn_y = tf.keras.layers.Conv1D(\n",
    "            filters=filter_count,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name=f\"y_conv_{kernel_size}_{filter_count}\",\n",
    "        )(x)\n",
    "        y_layers.append(cnn_y)\n",
    "    \n",
    "    x = tf.concat(\n",
    "        values=y_layers,\n",
    "        axis=-1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    y = x\n",
    "    for i in range(hp.Int('number_of_time_distributed_dense_layers', min_value=0, max_value=4, step=1)):\n",
    "        y = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(\n",
    "                units=x.shape[1],\n",
    "                activation='relu',\n",
    "                name=f'pre_lstm_time_distributed_dense_{i}',\n",
    "            )\n",
    "        )(y)\n",
    "    \n",
    "    x = tf.concat(\n",
    "        values=[x, y],\n",
    "        axis=-1,\n",
    "    )\n",
    "    \n",
    "    bidirectional_lstm_units_exponent = hp.Int('bidirectional_lstm_units_exponent', min_value=5, max_value=9, step=1)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        layer=tf.keras.layers.LSTM(\n",
    "            units=2**bidirectional_lstm_units_exponent,\n",
    "            return_sequences=True,\n",
    "            name='lstm'\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalMaxPooling1D(\n",
    "        name='global_max_pooling_over_time',\n",
    "    )(x)\n",
    "    \n",
    "    upscaling_dense_max = hp.Int('upscaling_dense_layer_generator_max', min_value=1, max_value=4, step=1)\n",
    "    upscaling_dense_min = hp.Int('upscaling_dense_layer_generator_min', min_value=0, max_value=upscaling_dense_max-1, step=1)\n",
    "    for i in range(upscaling_dense_min, upscaling_dense_max):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            units=2**(7 + i),\n",
    "            activation='relu',\n",
    "            name=f\"upscaling_dense_{i}\",\n",
    "        )(x)\n",
    "    \n",
    "    final_dense_feature_units = hp.Int('final_dense_layer_over_length_feature_units', min_value=3, max_value=7, step=1)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=PROCESSING_INFO['padding_lengths'][SEQ] * final_dense_feature_units,\n",
    "        activation='relu',\n",
    "        name=\"final_dense_layer_to_redefine_lengths\",\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.reshape(x, (-1, PROCESSING_INFO['padding_lengths'][SEQ], final_dense_feature_units))\n",
    "    \n",
    "    x = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=len(idx_to_char),\n",
    "            activation=None,\n",
    "            name='final_time_distributed_dense'\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    x = tf.keras.activations.softmax(x)\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=input_layers_list, \n",
    "        outputs=x, \n",
    "        name=f\"{model_name}_{utils.get_current_time_str()}\")\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "            tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e0d115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mirko.krause/dumps/PXD010000/models/tuner'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TUNER_PATH = os.path.join(DUMP_PATH, 'models', 'tuner')\n",
    "TUNER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf3a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ensure_dir_exists(TUNER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16e53786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\", line 127, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-20-c9ae8c0b4372>\", line 24, in build_lstm_with_pooling_and_position_model\n",
      "    position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n",
      "AttributeError: module 'official.nlp.modeling' has no attribute 'layers'\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\", line 127, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-20-c9ae8c0b4372>\", line 24, in build_lstm_with_pooling_and_position_model\n",
      "    position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n",
      "AttributeError: module 'official.nlp.modeling' has no attribute 'layers'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 1/5\n",
      "Invalid model 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\", line 127, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-20-c9ae8c0b4372>\", line 24, in build_lstm_with_pooling_and_position_model\n",
      "    position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n",
      "AttributeError: module 'official.nlp.modeling' has no attribute 'layers'\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\", line 127, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-20-c9ae8c0b4372>\", line 24, in build_lstm_with_pooling_and_position_model\n",
      "    position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n",
      "AttributeError: module 'official.nlp.modeling' has no attribute 'layers'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 3/5\n",
      "Invalid model 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\", line 127, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-20-c9ae8c0b4372>\", line 24, in build_lstm_with_pooling_and_position_model\n",
      "    position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n",
      "AttributeError: module 'official.nlp.modeling' has no attribute 'layers'\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\", line 127, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-20-c9ae8c0b4372>\", line 24, in build_lstm_with_pooling_and_position_model\n",
      "    position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n",
      "AttributeError: module 'official.nlp.modeling' has no attribute 'layers'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 5/5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\u001B[0m in \u001B[0;36mbuild\u001B[0;34m(self, hp)\u001B[0m\n\u001B[1;32m    126\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mmaybe_distribute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribution_strategy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m                     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhypermodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m             \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-20-c9ae8c0b4372>\u001B[0m in \u001B[0;36mbuild_lstm_with_pooling_and_position_model\u001B[0;34m(hp, model_name)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m     position_embedding = tfnlp.layers.position_embedding.RelativePositionEmbedding(\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0mhidden_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mposition_embedding_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'official.nlp.modeling' has no attribute 'layers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-94b430fcd757>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m tuner = kt.Hyperband(build_lstm_with_pooling_and_position_model,\n\u001B[0m\u001B[1;32m      2\u001B[0m                      \u001B[0mobjective\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'val_accuracy'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                      \u001B[0mmax_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                      \u001B[0mfactor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                      \u001B[0mdirectory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mTUNER_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/tuners/hyperband.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001B[0m\n\u001B[1;32m    359\u001B[0m             \u001B[0mallow_new_entries\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mallow_new_entries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m         )\n\u001B[0;32m--> 361\u001B[0;31m         super(Hyperband, self).__init__(\n\u001B[0m\u001B[1;32m    362\u001B[0m             \u001B[0moracle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypermodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhypermodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m         )\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/multi_execution_tuner.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, oracle, hypermodel, executions_per_trial, **kwargs)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moracle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypermodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexecutions_per_trial\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMultiExecutionTuner\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypermodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobjective\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             raise ValueError(\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite)\u001B[0m\n\u001B[1;32m    100\u001B[0m             )\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m         super(Tuner, self).__init__(\n\u001B[0m\u001B[1;32m    103\u001B[0m             \u001B[0moracle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m             \u001B[0mhypermodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhypermodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, oracle, hypermodel, directory, project_name, logger, overwrite)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_display\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtuner_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_populate_initial_space\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tuner_fname\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\u001B[0m in \u001B[0;36m_populate_initial_space\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    116\u001B[0m         \"\"\"\n\u001B[1;32m    117\u001B[0m         \u001B[0mhp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_space\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 118\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhypermodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    119\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_space\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\u001B[0m in \u001B[0;36m_build_wrapper\u001B[0;34m(self, hp, *args, **kwargs)\u001B[0m\n\u001B[1;32m     82\u001B[0m             \u001B[0;31m# to the search space.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m             \u001B[0mhp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/scratch/mirko.krause/anaconda3/envs/mmproteo/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py\u001B[0m in \u001B[0;36mbuild\u001B[0;34m(self, hp)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_max_fail_streak\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 135\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Too many failed attempts to build model.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    136\u001B[0m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Too many failed attempts to build model."
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(build_lstm_with_pooling_and_position_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory=TUNER_PATH,\n",
    "                     project_name='mmproteo-cnn-lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05152e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_LOG_DIR = os.path.join(TUNER_PATH, \"tensorboard\")\n",
    "os.path.realpath(TENSORBOARD_LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45638f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b37af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    x=datasets[TRAINING_TYPE].repeat(),\n",
    "    validation_data=datasets[TEST_TYPE].repeat(), \n",
    "    validation_steps=500,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(10_000/BATCH_SIZE),\n",
    "    callbacks=callbacks.create_callbacks(\n",
    "        tensorboard=True,\n",
    "        progressbar=False,\n",
    "        reduce_lr=True,\n",
    "        early_stopping=True,\n",
    "        checkpoints=True,\n",
    "        csv=True,\n",
    "        base_path=TUNER_PATH,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50488d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(DUMP_PATH, \"models\", model.name)\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5481a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ensure_dir_exists(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model,\n",
    "    to_file=os.path.join(MODEL_PATH, \"model.png\"),\n",
    "    show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd410eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"summary.txt\"), 'w') as file:\n",
    "    def write_lines(line: str) -> None:\n",
    "        file.write(line)\n",
    "        file.write(\"\\n\")\n",
    "    model.summary(print_fn=write_lines)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.json\"), 'w') as file:\n",
    "    file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.yaml\"), 'w') as file:\n",
    "    file.write(model.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=datasets[TRAINING_TYPE].repeat(),\n",
    "          validation_data=datasets[TEST_TYPE].repeat(), \n",
    "          validation_steps=500,\n",
    "          epochs=1,\n",
    "          steps_per_epoch=1_000,\n",
    "          callbacks=callbacks.create_callbacks(\n",
    "              tensorboard=True,\n",
    "              progressbar=False,\n",
    "              reduce_lr=True,\n",
    "              early_stopping=True,\n",
    "              checkpoints=True,\n",
    "              csv=True,\n",
    "              base_path=MODEL_PATH,\n",
    "          )\n",
    "         )\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_idx: Callable[[np.ndarray], np.ndarray] = np.vectorize(idx_to_char.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation.SequenceEvaluator(\n",
    "    dataset=datasets[EVAL_TYPE],\n",
    "    decode_func=decode_idx,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    separator=\" \",\n",
    "    padding_character=PROCESSING_INFO['padding_characters'][SEQ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917eb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df, (x_eval, y_eval, y_pred) = evaluator.evaluate_model_visually(\n",
    "    model=model,\n",
    "    sample_size=20,\n",
    "    keep_separator=True,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.predicted.map(print)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d87b01",
   "metadata": {},
   "source": [
    "broken loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caff47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(datasets[EVAL_TYPE].take(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052447fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Training an ML Model on Tensorflow Datasets\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Callable, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from mmproteo.utils import log, paths, utils, visualization\n",
    "from mmproteo.utils.formats.tf_dataset import DatasetLoader\n",
    "from mmproteo.utils.ml import callbacks, evaluation, layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62380a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "GPUs = tf.config.list_physical_devices('GPU')\n",
    "GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac43a97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "os.chdir(os.path.join('workspace', 'notebooks'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d57e9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afeb6463",
   "metadata": {},
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"/scratch/mirko.krause/dumps/\", PROJECT)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8612dee",
   "metadata": {},
   "source": [
    "DUMP_PATH = \"/scratch/mirko.krause/pdeep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dump path = {DUMP_PATH}\")\n",
    "THREAD_COUNT = min(int(os.cpu_count()/2), 16)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")\n",
    "PROCESSING_FILE_PATH = os.path.join(DATASET_DUMP_PATH, \"processing_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.create_logger(\n",
    "    name='mmproteo_dense_model',\n",
    "    verbose=True,\n",
    "    log_dir=DUMP_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c29d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSING_FILE_PATH, 'r') as file:\n",
    "    PROCESSING_INFO = json.loads(file.read())\n",
    "PROCESSING_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = {int(idx): char for idx, char in PROCESSING_INFO[\"idx_to_char\"].items()}\n",
    "char_to_idx = {char: idx for idx, char in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CACHE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TYPE = 'Train'\n",
    "TEST_TYPE = 'Test'\n",
    "EVAL_TYPE = 'Eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_grouping_path_position(cols: Optional[List[str]], prefered_item: str, alternative_index: int = -1) -> int:\n",
    "    res = alternative_index\n",
    "    if cols is not None:\n",
    "        try:\n",
    "            res = cols.index(prefered_item) - len(cols)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c50eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_path_position = find_grouping_path_position(\n",
    "    cols=PROCESSING_INFO['split_value_columns'],\n",
    "    prefered_item='species',\n",
    "    alternative_index=-1,\n",
    ")\n",
    "grouping_path_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        *(['*' for _ in PROCESSING_INFO['split_value_columns']] or [])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_paths = paths.assign_wildcard_paths_to_splits_grouped_by_path_position_value(\n",
    "    wildcard_path = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        *(['*' for _ in PROCESSING_INFO['split_value_columns']] or [])\n",
    "    ),\n",
    "    path_position = grouping_path_position,\n",
    "    splits = {\n",
    "            TRAIN_TYPE: 0.8,\n",
    "            TEST_TYPE: 0.9,\n",
    "            EVAL_TYPE: 1.0\n",
    "        },\n",
    "    paths_dump_file = os.path.join(\n",
    "            DATASET_DUMP_PATH,\n",
    "            \"dataset_file_paths.json\"\n",
    "        ),\n",
    "    skip_existing = KEEP_CACHE,\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"assigned dataset files:\")\n",
    "visualization.print_list_length_in_dict(dataset_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611252",
   "metadata": {},
   "source": [
    "### Loading corresponding TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d829d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_spec = eval(PROCESSING_INFO['element_spec'], {}, {'TensorSpec':tf.TensorSpec, 'tf':tf})\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec60438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Tensorflow (might take several minutes (~5 minutes per GPU with 40GB VRAM each))\n",
    "logger.debug(\"started initializing tensorflow by creating a first dataset\")\n",
    "tf.data.Dataset.range(5)\n",
    "logger.info(\"finished initializing tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_options = tf.data.Options()\n",
    "ds_options.experimental_threading.private_threadpool_size = THREAD_COUNT\n",
    "ds_options.experimental_threading.max_intra_op_parallelism = THREAD_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetLoader(\n",
    "    element_spec=element_spec,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=100_000,\n",
    "    thread_count=min(int(os.cpu_count()/4), 4),\n",
    "    keep_cache=KEEP_CACHE,\n",
    "    logger=logger,\n",
    "    run_benchmarks=False,\n",
    "    options=ds_options,\n",
    ").load_datasets_by_type(dataset_file_paths)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.metrics import MeanMetricWrapper\n",
    "from tensorflow.python.ops import array_ops, math_ops\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(elem):\n",
    "    i_true, i_pred = elem[0], elem[1]\n",
    "    unique_true, _ = tf.unique(i_true)\n",
    "    unique_pred, _ = tf.unique(i_pred)\n",
    "    n_unique_true = tf.shape(unique_true)[0]\n",
    "    n_unique_pred = tf.shape(unique_pred)[0]\n",
    "    unique_all, _ = tf.unique(tf.concat(values=[unique_true, unique_pred], axis=-1))\n",
    "    n_unique_all = tf.shape(unique_all)[0]\n",
    "    n_overlap = n_unique_true + n_unique_pred - n_unique_all\n",
    "    return n_overlap / n_unique_all\n",
    "\n",
    "def jaccard_batch_distance(y_true, y_pred):\n",
    "    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
    "    y_true = ops.convert_to_tensor_v2_with_dispatch(y_true)\n",
    "    y_pred_rank = y_pred.shape.ndims\n",
    "    y_true_rank = y_true.shape.ndims\n",
    "    # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "    if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n",
    "            backend.int_shape(y_true)) == len(backend.int_shape(y_pred))):\n",
    "        y_true = array_ops.squeeze(y_true, [-1])\n",
    "    y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if backend.dtype(y_pred) != backend.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, backend.dtype(y_true))\n",
    "    \n",
    "    # 0th dimension is the batch\n",
    "    jaccard = tf.map_fn(fn=jaccard_distance, elems=(y_true, y_pred), fn_output_signature=tf.float64)\n",
    "    return math_ops.cast(jaccard, backend.floatx())\n",
    "    \n",
    "\n",
    "class JaccardBatchDistance(MeanMetricWrapper):\n",
    "    def __init__(self, name='jaccard_batch_distance', dtype=None):\n",
    "        super(JaccardBatchDistance, self).__init__(\n",
    "            jaccard_batch_distance, name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ddf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leuvenshtein_sparse_tensor_batch_distance(y_true: tf.sparse.SparseTensor, y_pred: tf.sparse.SparseTensor) -> tf.Tensor:\n",
    "    return tf.edit_distance(\n",
    "        hypothesis=y_pred,\n",
    "        truth=y_true,\n",
    "        normalize=False,\n",
    "    )\n",
    "\n",
    "def leuvenshtein_batch_distance(y_true: tf.Tensor, y_pred: tf.Tensor, sparse_pred: bool = True) -> tf.Tensor:\n",
    "    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
    "    y_true = ops.convert_to_tensor_v2_with_dispatch(y_true)\n",
    "    \n",
    "    if sparse_pred:\n",
    "        y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    y_pred = tf.sparse.from_dense(y_pred)\n",
    "    y_true = tf.sparse.from_dense(y_true)\n",
    "\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if backend.dtype(y_pred) != backend.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, backend.dtype(y_true))\n",
    "    \n",
    "    # 0th dimension is the batch\n",
    "    leuvenshtein = leuvenshtein_sparse_tensor_batch_distance(y_true, y_pred)\n",
    "    return math_ops.cast(leuvenshtein, backend.floatx())\n",
    "    \n",
    "\n",
    "class LeuvenshteinBatchDistance(MeanMetricWrapper):\n",
    "    def __init__(self, name='leuvenshtein_batch_distance', dtype=None):\n",
    "        super(LeuvenshteinBatchDistance, self).__init__(\n",
    "            leuvenshtein_batch_distance, name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers_list, masked_input_layers_list = layers.create_masked_input_layers(\n",
    "    [\n",
    "        layers.InputLayerConfiguration(\n",
    "            name=col,\n",
    "            shape=PROCESSING_INFO['padding_lengths'][col],\n",
    "            mask_value=PROCESSING_INFO['padding_characters'][col]\n",
    "        )\n",
    "        for col in PROCESSING_INFO['training_data_columns']\n",
    "    ]\n",
    ")\n",
    "print(input_layers_list)\n",
    "print(masked_input_layers_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e94877",
   "metadata": {},
   "source": [
    "masked_loss = losses.MaskedLoss(\n",
    "    loss_function=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    masking_value=tf.constant(\n",
    "        value=char_to_idx[PROCESSING_INFO['padding_characters'][SEQ]],\n",
    "        dtype=tf.int8\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-geneva",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tf.stack(\n",
    "    values=masked_input_layers_list, \n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "x = tf.keras.layers.Flatten(name=\"flattened_masked_inputs\")(x)\n",
    "\n",
    "for _ in range(4):\n",
    "    x = tf.keras.layers.Dense(2**11)(x)\n",
    "    #x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(PROCESSING_INFO['padding_lengths'][SEQ]*len(idx_to_char))(x)\n",
    "\n",
    "x = tf.reshape(x,(-1, PROCESSING_INFO['padding_lengths'][SEQ], len(idx_to_char)))\n",
    "\n",
    "x = tf.keras.activations.softmax(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layers_list, outputs=x, name=f\"mmproteo_dense_{utils.get_current_time_str()}\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate = 10**-4 / 2\n",
    "    ),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        JaccardBatchDistance(),\n",
    "        LeuvenshteinBatchDistance(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(DUMP_PATH, \"models\", model.name)\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ensure_dir_exists(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model,\n",
    "    to_file=os.path.join(MODEL_PATH, \"model.png\"),\n",
    "    show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"summary.txt\"), 'w') as file:\n",
    "    def write_lines(line: str) -> None:\n",
    "        file.write(line)\n",
    "        file.write(\"\\n\")\n",
    "    model.summary(print_fn=write_lines)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43df6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.json\"), 'w') as file:\n",
    "    file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_PATH, \"model.yaml\"), 'w') as file:\n",
    "    file.write(model.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c018a1c5",
   "metadata": {},
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b15b5c8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1477ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = callbacks.create_callbacks(\n",
    "            tensorboard=True,\n",
    "            progressbar=False,\n",
    "            reduce_lr=False,\n",
    "            early_stopping=False,\n",
    "            checkpoints=False,\n",
    "            csv=True,\n",
    "            base_path=MODEL_PATH,\n",
    ")\n",
    "callback_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = datasets[TRAIN_TYPE].repeat()\n",
    "validation_dataset = datasets[TEST_TYPE].repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b169d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=training_dataset,\n",
    "    validation_data=validation_dataset, \n",
    "    validation_steps=STEPS_PER_EPOCH // 5,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=callback_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abe345",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_idx: Callable[[np.ndarray], np.ndarray] = np.vectorize(idx_to_char.get)\n",
    "\n",
    "eval_evaluator = evaluation.SequenceEvaluator(\n",
    "    dataset=datasets[EVAL_TYPE],\n",
    "    decode_func=decode_idx,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    separator=\" \",\n",
    "    padding_character=PROCESSING_INFO['padding_characters'][SEQ],\n",
    ")\n",
    "\n",
    "train_evaluator = evaluation.SequenceEvaluator(\n",
    "    dataset=datasets[TRAIN_TYPE],\n",
    "    decode_func=decode_idx,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    separator=\" \",\n",
    "    padding_character=PROCESSING_INFO['padding_characters'][SEQ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluator.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43563c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df, (x_eval, y_eval, y_pred) = train_evaluator.evaluate_model_visually(\n",
    "    model=model,\n",
    "    sample_size=20,\n",
    "    keep_separator=True,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_evaluator.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917eb62e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_df, (x_eval, y_eval, y_pred) = eval_evaluator.evaluate_model_visually(\n",
    "    model=model,\n",
    "    sample_size=20,\n",
    "    keep_separator=True,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573df56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Prototyping an ML Model on Tensorflow Datasets\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from typing import Iterable, Callable, Dict, Any, Tuple, Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python import keras as K\n",
    "\n",
    "from mmproteo.utils import log, utils, visualization\n",
    "from mmproteo.utils.formats.mz import FilteringProcessor\n",
    "from mmproteo.utils.formats.tf_dataset import Parquet2DatasetFileProcessor\n",
    "from mmproteo.utils.processing import ItemProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Printing to Stdout\n"
     ]
    }
   ],
   "source": [
    "logger = log.DummyLogger(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/workspace/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-message",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS = glob.glob(FILES_PATH)\n",
    "len(MZMLID_FILE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twenty-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a485487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide_sequence</th>\n",
       "      <th>mz_array</th>\n",
       "      <th>intensity_array</th>\n",
       "      <th>species</th>\n",
       "      <th>istrain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[C, K, P, T, S, P, G, R]</td>\n",
       "      <td>[102.0558, 115.05197, 116.971794, 119.907036, 129.1024, 136.06175, 152.05682, 157.84837, 159.22517, 171.11295, 175.119, 175.95169, 199.10796, 202.6932, 215.08527, 228.88432, 232.11212, 244.87819, 286.14047, 307.6665, 312.16718, 329.19223, 360.2081, 378.2132, 385.92047, 400.78918, 401.78973, 416.22388, 422.8325, 440.8446, 441.84528, 517.2766, 614.3271, 615.3258]</td>\n",
       "      <td>[723.529, 569.4288, 659.1485, 599.0097, 19982.768, 4909.943, 771.28937, 596.6283, 593.3602, 1262.0436, 868.29816, 581.3835, 721.64886, 752.1542, 2492.1565, 3854.2283, 1364.17, 615.11633, 746.43365, 1512.8475, 1474.3188, 1069.4283, 762.6549, 744.29315, 925.18164, 7245.0005, 2374.2295, 3248.2861, 4047.135, 21597.44, 5534.1826, 4359.906, 13269.387, 2903.926]</td>\n",
       "      <td>Citrobacter_freundii</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[K, H, I, T, A, G, A, K]</td>\n",
       "      <td>[101.1075, 110.07151, 111.04457, 111.619194, 112.050735, 116.972084, 118.967834, 122.29705, 129.05539, 129.10248, 129.11131, 129.92657, 130.08653, 136.06192, 136.07182, 136.07652, 137.06726, 139.98817, 147.11304, 152.05687, 171.00543, 173.09312, 189.01633, 197.12833, 200.14093, 212.10458, 218.14975, 223.15533, 230.11382, 231.12407, 232.88867, 239.08455, 249.13492, 251.15112, 275.1718, 283.13745, 299.95496, 301.1428, 302.81696, 309.96753, 313.8611, 315.81067, 316.8158, 318.8151, 334.8159, 335.81232, 336.81036, 337.8101, 340.79953, 343.80972, 344.8009, 346.20868, 349.20425, 354.82224, 355.82004, 360.81204, 361.81488, 362.81155, 363.81027, 370.8382, 372.79752, 389.83908, 394.83862, 407.8483, 408.7495, 408.8483, 412.79782, 412.8495, 413.26614, 413.85025, 414.26913, 419.80457, 430.79797, 431.7977, 447.25662, 448.2613, 465.94623, 560.34186, 561.3432, 697.4013, 787.23486]</td>\n",
       "      <td>[1244.104, 18248.63, 747.18225, 672.4936, 3284.768, 5824.9575, 1207.1666, 563.56824, 1090.989, 18666.379, 1132.0656, 547.7189, 8010.2773, 9944.686, 717.7685, 909.038, 927.90424, 1259.5803, 9798.942, 12360.792, 777.4666, 711.51215, 1365.1267, 669.6005, 718.6803, 724.85455, 1516.5447, 6849.315, 1172.2983, 11597.979, 882.7782, 954.53986, 1087.7533, 5462.294, 3395.8171, 717.6081, 663.439, 7134.955, 748.11066, 1207.3207, 3609.01, 838.3727, 1179.3096, 1473.9382, 2907.0327, 3263.6355, 4049.7156, 4270.7646, 793.22906, 1597.9222, 4802.7974, 4149.7407, 6089.5537, 7634.4062, 5610.0933, 1050.6061, 957.7547, 6195.684, 1396.489, 866.404, 846.26697, 1433.4541, 1076.0883, 5400.0293, 1063.56, 1220.6185, 1581.0791, 21550.523, 21930.604, 7990.5386, 3053.2961, 754.4101, 2840.6213, 1415.5275, 9367.521, 1103.6198, 957.9087, 6343.26, 795.6793, 2594.3503, 750.4295]</td>\n",
       "      <td>Citrobacter_freundii</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            peptide_sequence  \\\n",
       "21  [C, K, P, T, S, P, G, R]   \n",
       "70  [K, H, I, T, A, G, A, K]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         mz_array  \\\n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [102.0558, 115.05197, 116.971794, 119.907036, 129.1024, 136.06175, 152.05682, 157.84837, 159.22517, 171.11295, 175.119, 175.95169, 199.10796, 202.6932, 215.08527, 228.88432, 232.11212, 244.87819, 286.14047, 307.6665, 312.16718, 329.19223, 360.2081, 378.2132, 385.92047, 400.78918, 401.78973, 416.22388, 422.8325, 440.8446, 441.84528, 517.2766, 614.3271, 615.3258]   \n",
       "70  [101.1075, 110.07151, 111.04457, 111.619194, 112.050735, 116.972084, 118.967834, 122.29705, 129.05539, 129.10248, 129.11131, 129.92657, 130.08653, 136.06192, 136.07182, 136.07652, 137.06726, 139.98817, 147.11304, 152.05687, 171.00543, 173.09312, 189.01633, 197.12833, 200.14093, 212.10458, 218.14975, 223.15533, 230.11382, 231.12407, 232.88867, 239.08455, 249.13492, 251.15112, 275.1718, 283.13745, 299.95496, 301.1428, 302.81696, 309.96753, 313.8611, 315.81067, 316.8158, 318.8151, 334.8159, 335.81232, 336.81036, 337.8101, 340.79953, 343.80972, 344.8009, 346.20868, 349.20425, 354.82224, 355.82004, 360.81204, 361.81488, 362.81155, 363.81027, 370.8382, 372.79752, 389.83908, 394.83862, 407.8483, 408.7495, 408.8483, 412.79782, 412.8495, 413.26614, 413.85025, 414.26913, 419.80457, 430.79797, 431.7977, 447.25662, 448.2613, 465.94623, 560.34186, 561.3432, 697.4013, 787.23486]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          intensity_array  \\\n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [723.529, 569.4288, 659.1485, 599.0097, 19982.768, 4909.943, 771.28937, 596.6283, 593.3602, 1262.0436, 868.29816, 581.3835, 721.64886, 752.1542, 2492.1565, 3854.2283, 1364.17, 615.11633, 746.43365, 1512.8475, 1474.3188, 1069.4283, 762.6549, 744.29315, 925.18164, 7245.0005, 2374.2295, 3248.2861, 4047.135, 21597.44, 5534.1826, 4359.906, 13269.387, 2903.926]   \n",
       "70  [1244.104, 18248.63, 747.18225, 672.4936, 3284.768, 5824.9575, 1207.1666, 563.56824, 1090.989, 18666.379, 1132.0656, 547.7189, 8010.2773, 9944.686, 717.7685, 909.038, 927.90424, 1259.5803, 9798.942, 12360.792, 777.4666, 711.51215, 1365.1267, 669.6005, 718.6803, 724.85455, 1516.5447, 6849.315, 1172.2983, 11597.979, 882.7782, 954.53986, 1087.7533, 5462.294, 3395.8171, 717.6081, 663.439, 7134.955, 748.11066, 1207.3207, 3609.01, 838.3727, 1179.3096, 1473.9382, 2907.0327, 3263.6355, 4049.7156, 4270.7646, 793.22906, 1597.9222, 4802.7974, 4149.7407, 6089.5537, 7634.4062, 5610.0933, 1050.6061, 957.7547, 6195.684, 1396.489, 866.404, 846.26697, 1433.4541, 1076.0883, 5400.0293, 1063.56, 1220.6185, 1581.0791, 21550.523, 21930.604, 7990.5386, 3053.2961, 754.4101, 2840.6213, 1415.5275, 9367.521, 1103.6198, 957.9087, 6343.26, 795.6793, 2594.3503, 750.4295]   \n",
       "\n",
       "                 species istrain  \n",
       "21  Citrobacter_freundii   Train  \n",
       "70  Citrobacter_freundii   Train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(MZMLID_FILE_PATHS[1])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a453e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'\n",
    "MZ = 'mz_array'\n",
    "INT = 'intensity_array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_COLUMNS = [MZ, INT]\n",
    "TARGET_DATA_COLUMNS = [SEQ]\n",
    "SPLIT_VALUE_COLUMNS = ['species', 'istrain']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-headset",
   "metadata": {},
   "source": [
    "## Calculating Statistics over all MZMLID Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vertical-tiger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded previous statistics file '../dumps/PXD010000/training_columns/statistics.parquet'\n"
     ]
    }
   ],
   "source": [
    "file_path_count = len(MZMLID_FILE_PATHS)\n",
    "\n",
    "def get_mzmlid_file_stats(item: Tuple[int, str]) -> Dict[str, Any]:\n",
    "    idx, path = item\n",
    "    info_text = f\"Processing item {idx + 1}/{file_path_count} '{path}'\"\n",
    "    if idx % 10 == 0:\n",
    "        logger.info(info_text)\n",
    "    else:\n",
    "        logger.debug(info_text)\n",
    "    df = pd.read_parquet(path)\n",
    "    max_sequence_length = df[SEQ].str.len().max()\n",
    "    max_array_length = df[INT].str.len().max()\n",
    "    alphabet = set.union(*df[SEQ].apply(set))\n",
    "    item_count = len(df)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        \"file_path\": path,\n",
    "        \"max_sequence_length\": max_sequence_length,\n",
    "        \"max_array_length\": max_array_length,\n",
    "        \"alphabet\": alphabet,\n",
    "        \"item_count\": item_count\n",
    "    }\n",
    "\n",
    "if os.path.exists(STATISTICS_FILE_PATH):\n",
    "    file_stats = pd.read_parquet(STATISTICS_FILE_PATH)\n",
    "    file_stats.alphabet = file_stats.alphabet.apply(set)\n",
    "    print(f\"loaded previous statistics file '{STATISTICS_FILE_PATH}'\")\n",
    "else:\n",
    "    file_stats = pd.DataFrame(\n",
    "        ItemProcessor(\n",
    "            items=enumerate(MZMLID_FILE_PATHS),\n",
    "            item_processor=get_mzmlid_file_stats,\n",
    "            action_name=\"analyse\",\n",
    "            subject_name=\"mzmlid file\",\n",
    "            thread_count=0,\n",
    "            logger=logger\n",
    "        ).process()\n",
    "    )\n",
    "    \n",
    "    file_stats_writable = file_stats.copy()\n",
    "    file_stats_writable.alphabet = file_stats_writable.alphabet.apply(list) # cannot store sets\n",
    "    file_stats_writable.to_parquet(STATISTICS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "leading-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>max_sequence_length</th>\n",
       "      <th>max_array_length</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>item_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet</td>\n",
       "      <td>50</td>\n",
       "      <td>1845</td>\n",
       "      <td>{Y, H, R, L, M(Oxidation), W, C, G, E, I, F, S, K, Q, P, N, D, M, A, V, T}</td>\n",
       "      <td>26943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_Cibrobacter_freundii_LB_aerobic_01_01Feb16_Arwen_15-07-13_mzmlid.parquet</td>\n",
       "      <td>50</td>\n",
       "      <td>1697</td>\n",
       "      <td>{Y, H, R, L, M(Oxidation), W, C, G, E, I, F, S, K, Q, P, N, D, M, A, V, T}</td>\n",
       "      <td>27516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   file_path  \\\n",
       "0                       ../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet   \n",
       "1  ../dumps/PXD010000/training_columns/Biodiversity_Cibrobacter_freundii_LB_aerobic_01_01Feb16_Arwen_15-07-13_mzmlid.parquet   \n",
       "\n",
       "   max_sequence_length  max_array_length  \\\n",
       "0                   50              1845   \n",
       "1                   50              1697   \n",
       "\n",
       "                                                                     alphabet  \\\n",
       "0  {Y, H, R, L, M(Oxidation), W, C, G, E, I, F, S, K, Q, P, N, D, M, A, V, T}   \n",
       "1  {Y, H, R, L, M(Oxidation), W, C, G, E, I, F, S, K, Q, P, N, D, M, A, V, T}   \n",
       "\n",
       "   item_count  \n",
       "0       26943  \n",
       "1       27516  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "following-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_LENGTHS = {\n",
    "    MZ: file_stats.max_array_length.max(),\n",
    "    INT: file_stats.max_array_length.max(),\n",
    "    SEQ: file_stats.max_sequence_length.max()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "early-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding lengths = {'mz_array': 2354, 'intensity_array': 2354, 'peptide_sequence': 50}\n",
      "TOTAL_ITEM_COUNT = 5513185\n",
      "ALPHABET = A, C, D, E, F, G, H, I, K, L, M, M(Oxidation), N, P, Q, R, S, T, V, W, Y\n"
     ]
    }
   ],
   "source": [
    "print(\"padding lengths =\", PADDING_LENGTHS)\n",
    "\n",
    "TOTAL_ITEM_COUNT = file_stats.item_count.sum()\n",
    "print(f\"TOTAL_ITEM_COUNT = {TOTAL_ITEM_COUNT}\")\n",
    "\n",
    "ALPHABET = set.union(*file_stats.alphabet)\n",
    "print(f\"ALPHABET = {', '.join(sorted(ALPHABET))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-recipient",
   "metadata": {},
   "source": [
    "## Data Normalization, Padding, and Conversion to Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "headed-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return tf.keras.utils.normalize(x=values, order=2)\n",
    "\n",
    "def base_peak_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return values / values.max(initial=0)\n",
    "\n",
    "# by Tom, probably\n",
    "# don't know, what it's based on\n",
    "def ion_current_normalize(intensities: np.ndarray) -> np.ndarray:\n",
    "    total_sum = np.sum(intensities**2)\n",
    "    normalized = intensities/total_sum\n",
    "    return normalized\n",
    "\n",
    "NORMALIZATION = {\n",
    "    INT: base_peak_normalize\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "waiting-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_CHARACTERS = {\n",
    "    SEQ: '_',\n",
    "    MZ: 0.0,\n",
    "    INT: 0.0,\n",
    "}\n",
    "\n",
    "ALPHABET.add(PADDING_CHARACTERS[SEQ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "understood-necklace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'M(Oxidation)': 11,\n",
       " 'N': 12,\n",
       " 'P': 13,\n",
       " 'Q': 14,\n",
       " 'R': 15,\n",
       " 'S': 16,\n",
       " 'T': 17,\n",
       " 'V': 18,\n",
       " 'W': 19,\n",
       " 'Y': 20,\n",
       " '_': 21}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(ALPHABET))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "INDEX_ALPHABET = idx_to_char.keys()\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equivalent-portrait",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Processing item 1/235: '../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet'\n",
      "INFO: Processing item 11/235: '../dumps/PXD010000/training_columns/Biodiversity_P_polymyxa_TBS_aerobic_3_17July16_Samwise_16-04-10_mzmlid.parquet'\n",
      "INFO: Processing item 21/235: '../dumps/PXD010000/training_columns/M_alcali_copp_CH4_B2_T1_09_QE_23Mar18_Oak_18-01-07_mzmlid.parquet'\n",
      "INFO: Processing item 31/235: '../dumps/PXD010000/training_columns/Cj_media_MH_R4_23Feb15_Arwen_14-12-03_mzmlid.parquet'\n",
      "INFO: Processing item 41/235: '../dumps/PXD010000/training_columns/Biodiversity_C_Baltica_T240_R2_C_27Jan16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 51/235: '../dumps/PXD010000/training_columns/Biodiversity_M_xanthus_DZ2_plates_1_03May16_Samwise_16-03-32_mzmlid.parquet'\n",
      "INFO: Processing item 61/235: '../dumps/PXD010000/training_columns/Biodiversity_B_thet_CMgluc_anaerobic_02_01Feb16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 71/235: '../dumps/PXD010000/training_columns/Biodiversity_R_palustris_PMnitro_anaerobic_2_01Jun16_Pippin_16-03-39_mzmlid.parquet'\n",
      "INFO: Processing item 81/235: '../dumps/PXD010000/training_columns/Biodiversity_S_elongatus_BG11_aerobic_1_14July16_Pippin_16-05-01_mzmlid.parquet'\n",
      "INFO: Processing item 91/235: '../dumps/PXD010000/training_columns/Biodiversity_F_novicida_TSB_aerobic_03_01Feb16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 101/235: '../dumps/PXD010000/training_columns/M_alcali_copp_CH4_B3_T1_11_QE_23Mar18_Oak_18-01-07_mzmlid.parquet'\n",
      "INFO: Processing item 111/235: '../dumps/PXD010000/training_columns/Biodiversity_M_xanthus_DZ2_48h_plates_2_13Jun16_Pippin_16-03-39_mzmlid.parquet'\n",
      "INFO: Processing item 121/235: '../dumps/PXD010000/training_columns/Biodiversity_F_prausnitzii_Glc_01_28Oct15_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 131/235: '../dumps/PXD010000/training_columns/Biodiversity_P_ruminicola_MDM_anaerobic_2_09Jun16_Pippin_16-03-39_mzmlid.parquet'\n",
      "INFO: Processing item 141/235: '../dumps/PXD010000/training_columns/M_alcali_copp_MeOH_B2_T1_03_QE_23Mar18_Oak_18-01-07_mzmlid.parquet'\n",
      "INFO: Processing item 151/235: '../dumps/PXD010000/training_columns/Biodiversity_A_cryptum_FeTSB_anaerobic_2_01Jun16_Pippin_16-03-39_mzmlid.parquet'\n",
      "INFO: Processing item 161/235: '../dumps/PXD010000/training_columns/Biodiversity_B_thet_LIB_anaerobic_03_01Feb16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 171/235: '../dumps/PXD010000/training_columns/Biodiversity_C_Baltica_T240_R2_Inf_27Jan16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 181/235: '../dumps/PXD010000/training_columns/Biodiversity_B_thet_CMcarb_anaerobic_02_01Feb16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 191/235: '../dumps/PXD010000/training_columns/Biodiversity_S_elongatus_BG11NaCl_aerobic_1_05Oct16_Pippin_16-05-06_mzmlid.parquet'\n",
      "INFO: Processing item 201/235: '../dumps/PXD010000/training_columns/Biodiversity_B_thet_CMcarb_anaerobic_03_01Feb16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: Processing item 211/235: '../dumps/PXD010000/training_columns/Biodiversity_B_subtilis_pellet_set2_2_13Jun16_Pippin_16-03-39_mzmlid.parquet'\n",
      "INFO: Processing item 221/235: '../dumps/PXD010000/training_columns/Biodiversity_HL49_HLHYE_aerobic_3_05Oct16_Pippin_16-05-06_mzmlid.parquet'\n",
      "INFO: Processing item 231/235: '../dumps/PXD010000/training_columns/Biodiversity_B_subtilis_NCIB3610_48h_plates_3_13Jun16_Pippin_16-03-39_mzmlid.parquet'\n",
      "INFO: No mzmlid parquet files were parquet2tf_dataset-processed\n",
      "INFO: Encountered 0 exceptions during processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parquet2DatasetFileProcessor(\n",
    "    training_data_columns=TRAINING_DATA_COLUMNS,\n",
    "    target_data_columns=TARGET_DATA_COLUMNS,\n",
    "    padding_lengths=PADDING_LENGTHS,\n",
    "    padding_characters=PADDING_CHARACTERS,\n",
    "    column_normalizations=NORMALIZATION,\n",
    "    dataset_dump_path_prefix=DATASET_DUMP_PATH,\n",
    "    char_to_idx_mapping_functions={\n",
    "        SEQ: char_to_idx.get\n",
    "    },\n",
    "    item_count=len(MZMLID_FILE_PATHS),\n",
    "    skip_existing=True,\n",
    "    split_on_column_values_of=SPLIT_VALUE_COLUMNS,\n",
    "    logger=logger\n",
    ").process(parquet_file_paths=MZMLID_FILE_PATHS,\n",
    "          thread_count=3)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5719d9a",
   "metadata": {},
   "source": [
    "### ... by type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8581bc",
   "metadata": {},
   "source": [
    "#### ... by training type annotation (abandoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "323c4964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA_TYPES = {path.split(os.path.sep)[-1] for path in glob.glob(\n",
    "    os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        '*',  # species\n",
    "        '*'   # istrain\n",
    "    ))}\n",
    "TRAINING_DATA_TYPES"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae23af50",
   "metadata": {},
   "source": [
    "# only when using the annotated training data types\n",
    "dataset_file_paths = {training_data_type: glob.glob(os.path.join(DATASET_DUMP_PATH, '*', '*', training_data_type))\n",
    "for training_data_type in TRAINING_DATA_TYPES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094281b",
   "metadata": {},
   "source": [
    "#### ... by species annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f5789b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acidiphilium_cryptum_JF-5',\n",
       " 'Agrobacterium_tumefaciens_IAM_12048',\n",
       " 'Alcaligenes_faecalis',\n",
       " 'Algoriphagus_marincola_HL-49',\n",
       " 'Anaerococcus_hydrogenalis_DSM_7454',\n",
       " 'Bacillus_cereus_ATCC14579',\n",
       " 'Bacillus_subtilis_168',\n",
       " 'Bacillus_subtilis_NCIB3610',\n",
       " 'Bacteroides_fragilis_638R',\n",
       " 'Bacteroides_thetaiotaomicron_VPI-5482',\n",
       " 'Bifidobacterium_bifidum_ATCC29521',\n",
       " 'Bifidobacterium_longum_infantis_ATCC15697',\n",
       " 'Campylobacter_jejuni',\n",
       " 'Cellulomonas_gilvus_ATCC13127',\n",
       " 'Cellulophaga_baltica_18',\n",
       " 'Chryseobacterium_indologenes',\n",
       " 'Citrobacter_freundii',\n",
       " 'Clostridium_ljungdahlii_DMS_13528',\n",
       " 'Coprococcus_comes_ATCC27758',\n",
       " 'Cupriavidus_necator_N-1',\n",
       " 'Cyanobacterium_stanieri',\n",
       " 'Delftia_acidovorans_SPH1',\n",
       " 'Dorea_longicatena_DSM13814',\n",
       " 'Erythrobacter_HL-111',\n",
       " 'Faecalibacterium_prausnitzii',\n",
       " 'Fibrobacter_succinogenes_S85',\n",
       " 'Francisella_novicida_U112',\n",
       " 'Halomonas_HL-48',\n",
       " 'Halomonas_HL-93',\n",
       " 'Lactobacillales_casei',\n",
       " 'Legionella_pneumophila',\n",
       " 'Listeria_monocytogenes_10403S',\n",
       " 'Methylomicrobium_alcaliphilum',\n",
       " 'Micrococcus_luteus',\n",
       " 'Mycobacterium_smegmatis',\n",
       " 'Myxococcus_xanthus_DZ2',\n",
       " 'Paenibacillus_polymyxa_ATCC842',\n",
       " 'Paracoccus_denitrificans',\n",
       " 'Prevotella_ruminicola_23_ATCC_19189',\n",
       " 'Pseudomonas_putida_KT2440',\n",
       " 'Rhodobacteraceae_bacterium_HL-91',\n",
       " 'Rhodococcus_jostii_RHA1',\n",
       " 'Rhodopseudomonas_palustris',\n",
       " 'Ruminococcus_gnavus',\n",
       " 'Shewanella_oneidensis_MR-1',\n",
       " 'Stigmatella_aurantiaca_DW431',\n",
       " 'Streptococcus_agalactiae',\n",
       " 'Streptomyces_griseorubens',\n",
       " 'Streptomyces_venezuelae',\n",
       " 'Sulfobacillus_thermosulfidooxidans',\n",
       " 'Synechococcus_elongatus_PCC7942'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIES = {path.split(os.path.sep)[-2] for path in glob.glob(\n",
    "    os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        '*',  # species\n",
    "        '*'   # istrain\n",
    "    ))}\n",
    "SPECIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6e0937b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SPECIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622256e",
   "metadata": {},
   "source": [
    "### ... with train-test-eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee8aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES_SPLITS = {\n",
    "            \"Train\": 0.4,\n",
    "            \"Test\": 0.5,\n",
    "            \"Eval\": 0.6\n",
    "        }\n",
    "KEEP_CACHE = True  # currently, there is no cache; the flag only disables benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55221ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found dataset file paths dump '../dumps/PXD010000/training_columns/tf_datasets/dataset_file_paths.json'\n",
      "\n",
      "assigned dataset files:\n",
      "#Train = 89\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_C_indologenes_LIB_aerobic_02_03May16_Samwise_16-03-32_mzmlid.parquet/Chryseobacterium_indologenes/Train\n",
      "#Test = 17\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_A_cryptum_FeTSB_anaerobic_1_01Jun16_Pippin_16-03-39_mzmlid.parquet/Acidiphilium_cryptum_JF-5/Train\n",
      "#Eval = 29\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_B_fragilis_CMcarb_anaerobic_01_01Feb16_Arwen_15-07-13_mzmlid.parquet/Bacteroides_fragilis_638R/Train\n"
     ]
    }
   ],
   "source": [
    "def assign_species_randomly(species: List[str], splits: Dict[str, float] = None) -> Dict[str, List[str]]:\n",
    "    if splits is None:\n",
    "        splits = {\n",
    "            \"Train\": 0.8,\n",
    "            \"Test\": 0.94,\n",
    "            \"Eval\": 1.0\n",
    "        }\n",
    "    \n",
    "    splits[None] = 0\n",
    "    \n",
    "    sorted_splits = sorted(splits.items(), key=lambda tupl: tupl[1])\n",
    "    \n",
    "    shuffled_species = list(species)\n",
    "    random.shuffle(shuffled_species)\n",
    "\n",
    "    assigned_species = {\n",
    "        training_data_type: shuffled_species[\n",
    "            int(sorted_splits[i][1] * len(shuffled_species)):\n",
    "            int(split * len(shuffled_species))\n",
    "        ]\n",
    "        for i, (training_data_type, split) in enumerate(sorted_splits[1:])\n",
    "    }\n",
    "    return assigned_species\n",
    "\n",
    "def flatten(lists: List[List[Any]]) -> List[Any]:\n",
    "    res = []\n",
    "    for item in lists:\n",
    "        res += item\n",
    "    return res\n",
    "\n",
    "def find_files_for_assigned_species(\n",
    "    assigned_species: Dict[str, List[str]],\n",
    "    file_pattern: str = os.path.join(DATASET_DUMP_PATH, '*', \"{specie}\", '*')\n",
    ") -> Dict[str, List[str]]:\n",
    "    return {\n",
    "        training_type: flatten(\n",
    "            [\n",
    "                glob.glob(file_pattern.format(specie=specie)) for specie in species\n",
    "            ]\n",
    "        ) for training_type, species in assigned_species.items()\n",
    "    }\n",
    "\n",
    "def store_dataset_file_paths(\n",
    "    dataset_file_paths: str, \n",
    "    output_file) -> str:\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(visualization.pretty_print_json(dataset_file_paths))\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def load_json(file_path: str) -> Dict[str, Any]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.loads(file.read())\n",
    "\n",
    "def print_list_length_in_dict(dic: Dict[str, List[Any]]) -> None:\n",
    "    for key, list_value in dic.items():\n",
    "        print(f\"#{key} = {len(list_value)}\")\n",
    "        if len(list_value) > 0:\n",
    "            print(f\"e.g.: {list_value[0]}\")\n",
    "\n",
    "dataset_file_path_dump_file = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        \"dataset_file_paths.json\"\n",
    "    )\n",
    "\n",
    "if KEEP_CACHE and os.path.exists(dataset_file_path_dump_file):\n",
    "    dataset_file_paths = load_json(dataset_file_path_dump_file)\n",
    "    print(f\"found dataset file paths dump '{dataset_file_path_dump_file}'\")\n",
    "else:\n",
    "    assigned_species = assign_species_randomly(SPECIES, splits=SPECIES_SPLITS)\n",
    "    print(\"assigned species:\")\n",
    "    print_list_length_in_dict(assigned_species)\n",
    "\n",
    "    dataset_file_paths = find_files_for_assigned_species(assigned_species)\n",
    "    store_dataset_file_paths(dataset_file_paths, dataset_file_path_dump_file)\n",
    "    print(f\"dumped dataset file paths into '{dataset_file_path_dump_file}'\")\n",
    "\n",
    "print()\n",
    "print(\"assigned dataset files:\")\n",
    "print_list_length_in_dict(dataset_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611252",
   "metadata": {},
   "source": [
    "### Loading corresponding TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stupid-malaysia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(2354,), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2354,), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(50,), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ((tf.TensorSpec(shape=(PADDING_LENGTHS[MZ],), dtype=tf.float32), \n",
    "  tf.TensorSpec(shape=(PADDING_LENGTHS[INT],), dtype=tf.float32)),\n",
    "(tf.TensorSpec(shape=(PADDING_LENGTHS[SEQ],), dtype=tf.int8)))\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aebc3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': <ParallelInterleaveDataset shapes: (((2354,), (2354,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " 'Test': <ParallelInterleaveDataset shapes: (((2354,), (2354,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " 'Eval': <ParallelInterleaveDataset shapes: (((2354,), (2354,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datasets = {\n",
    "    training_data_type: tf.data.Dataset.from_tensor_slices(paths).interleave(lambda path: \n",
    "        tf.data.experimental.load(\n",
    "            path=path, \n",
    "            element_spec=element_spec, \n",
    "            compression='GZIP'\n",
    "        ),\n",
    "                                                                             num_parallel_calls=os.cpu_count(),\n",
    "                                                                             deterministic=False\n",
    "                                                                            )\n",
    "    for training_data_type, paths in dataset_file_paths.items()\n",
    "}\n",
    "\n",
    "merged_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-difficulty",
   "metadata": {},
   "source": [
    "## Configuring Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba9ffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# although not all data fits into a 100k buffer, the interleaving should make it sufficiently random\n",
    "SHUFFLE_BUFFER_SIZE = 2*10**5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b141e",
   "metadata": {},
   "source": [
    "### Caching (currently abandoned because of too high RAM usage)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4a97453",
   "metadata": {},
   "source": [
    "CACHED_DATASET_DUMP_PATH = os.path.join(DATASET_DUMP_PATH, \"cache\")\n",
    "\n",
    "try:\n",
    "    if not KEEP_CACHE:\n",
    "        shutil.rmtree(CACHED_DATASET_DUMP_PATH)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "utils.ensure_dir_exists(CACHED_DATASET_DUMP_PATH)\n",
    "print(CACHED_DATASET_DUMP_PATH)\n",
    "\n",
    "merged_datasets = {\n",
    "    training_data_type: dataset.cache(os.path.join(CACHED_DATASET_DUMP_PATH, training_data_type))\n",
    "    for training_data_type, dataset in merged_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b491f44",
   "metadata": {},
   "source": [
    "### Preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c881d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cache(dataset, name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Use a benchmark to once process the whole dataset.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        print(f\"{name}:\")\n",
    "    display(tfds.benchmark(dataset))\n",
    "    gc.collect()\n",
    "    logger.info(\"filled a cache - waiting 10 seconds\")\n",
    "    print()\n",
    "    time.sleep(10)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a7a9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KEEP_CACHE:\n",
    "    merged_datasets = {\n",
    "        training_data_type: fill_cache(dataset, name=training_data_type)\n",
    "        for training_data_type, dataset in merged_datasets.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ee6a5",
   "metadata": {},
   "source": [
    "### Shuffling, Batching, Prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cbc0466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': <PrefetchDataset shapes: (((32, 2354), (32, 2354)), (32, 50)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " 'Test': <PrefetchDataset shapes: (((32, 2354), (32, 2354)), (32, 50)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " 'Eval': <PrefetchDataset shapes: (((32, 2354), (32, 2354)), (32, 50)), types: ((tf.float32, tf.float32), tf.int8)>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datasets = {\n",
    "    training_data_type: dataset\n",
    "        .shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    for training_data_type, dataset in merged_datasets.items()\n",
    "}\n",
    "merged_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TYPE = 'Train'\n",
    "TEST_TYPE = 'Test'\n",
    "EVAL_TYPE = 'Eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dress-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mz_array': <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'mz_array')>,\n",
       " 'intensity_array': <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'intensity_array')>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_input_layers = {col: tf.keras.layers.Input(shape=(PADDING_LENGTHS[col],), name=col) for col in TRAINING_DATA_COLUMNS}\n",
    "named_input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d489e61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'mz_array')>,\n",
       " <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'intensity_array')>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_input_layers_list = [ named_input_layers[col] for col in TRAINING_DATA_COLUMNS ]\n",
    "named_input_layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387e127d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mz_array': <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'masked_mz_array')>,\n",
       " 'intensity_array': <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'masked_intensity_array')>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input_layers = {\n",
    "    col: tf.keras.layers.Masking(mask_value=PADDING_CHARACTERS[col], name=f\"masked_{col}\")(input_layer)\n",
    "    for col, input_layer in named_input_layers.items()\n",
    "}\n",
    "masked_input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14d2218b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'masked_mz_array')>,\n",
       " <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'masked_intensity_array')>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input_layers_list = [ masked_input_layers[col] for col in TRAINING_DATA_COLUMNS ]\n",
    "masked_input_layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40b0b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(K.losses.LossFunctionWrapper):\n",
    "    def __init__(self, loss_function, masking_value, name='masked_loss', reduction=tf.keras.losses.Reduction.NONE):\n",
    "        def _masked_loss(y_true, y_pred):\n",
    "            y_true = tf.squeeze(y_true, name=\"masked_loss__squeezed_y_true\")\n",
    "            y_pred = tf.squeeze(y_pred, name=\"masked_loss__squeezed_y_pred\")\n",
    "            #print(y_true)\n",
    "            #print(y_pred)\n",
    "            length_mask = tf.equal(y_true, masking_value, name=\"masked_loss__is_masking_value\")\n",
    "            #print(length_mask)\n",
    "            length_mask = tf.cast(length_mask, tf.float32, name=\"masked_loss__is_masking_value_float\")\n",
    "            length_mask = tf.math.subtract(\n",
    "                tf.constant(\n",
    "                    value=1, \n",
    "                    dtype=tf.float32\n",
    "                ), length_mask, name=\"masked_loss__is_masking_value_inverted\")\n",
    "            lengths = tf.math.reduce_sum(length_mask, axis=-1, name=\"masked_loss__sum_to_get_lengths\")\n",
    "            #print(lengths)\n",
    "            lengths = tf.math.add(lengths, 1, name=\"masked_loss__sum_to_include_first_padding\") # to also include the first padding character\n",
    "            #print(lengths)\n",
    "            mask = tf.sequence_mask(\n",
    "                lengths=lengths,\n",
    "                maxlen=y_pred.shape[-2],  # pre-last dimension = padding length; last dimension = one-hot-encoded alphabet\n",
    "                dtype=tf.float32,\n",
    "                name=\"masked_loss__create_sequence_mask\"\n",
    "            )\n",
    "            #print(mask)\n",
    "            losses = loss_function(y_true, y_pred)\n",
    "            #print(losses)\n",
    "            losses = tf.math.multiply(losses, mask, name=\"masked_loss__apply_sequence_mask\")\n",
    "            #print(losses)\n",
    "            summed_losses = tf.math.reduce_sum(losses, axis=-1, name=\"masked_loss__sum_losses\")\n",
    "            #print(summed_losses)\n",
    "            average_losses = tf.math.divide_no_nan(summed_losses, lengths, name=\"masked_loss__average_losses\")\n",
    "            #print(average_losses)\n",
    "            return average_losses\n",
    "            \n",
    "        super(MaskedLoss, self).__init__(_masked_loss, name=name, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03ed2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_loss = MaskedLoss(\n",
    "    loss_function=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    masking_value=tf.constant(\n",
    "        value=char_to_idx[PADDING_CHARACTERS[SEQ]],\n",
    "        dtype=tf.int8\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "masked_loss(y_eval[:2], y_pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "developmental-geneva",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mmproteo\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mz_array (InputLayer)           [(None, 2354)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "intensity_array (InputLayer)    [(None, 2354)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_mz_array (Masking)       (None, 2354)         0           mz_array[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masked_intensity_array (Masking (None, 2354)         0           intensity_array[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 2354)         0           masked_mz_array[0][0]            \n",
      "                                                                 masked_intensity_array[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flattened_masked_inputs (Flatte (None, 2354)         0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         4823040     flattened_masked_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2048)         4196352     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         4196352     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1100)         2253900     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (None, 50, 22)       0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.softmax (TFOpLa (None, 50, 22)       0           tf.reshape[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 19,665,996\n",
      "Trainable params: 19,665,996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = masked_input_layers_list[0]\n",
    "for input_layer in masked_input_layers_list[1:]:\n",
    "    x = x + input_layer\n",
    "\n",
    "x = tf.keras.layers.Flatten(name=\"flattened_masked_inputs\")(x)\n",
    "\n",
    "for _ in range(4):\n",
    "    x = tf.keras.layers.Dense(2**11)(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(PADDING_LENGTHS[SEQ]*len(ALPHABET))(x)\n",
    "\n",
    "x = tf.reshape(x,(-1, PADDING_LENGTHS[SEQ], len(ALPHABET)))\n",
    "\n",
    "x = tf.keras.activations.softmax(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=named_input_layers_list, outputs=x, name='mmproteo')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=masked_loss,\n",
    "              metrics=[\n",
    "                  tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "                  tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "              ]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "630d443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05152e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_DIR = os.path.join(DUMP_PATH, \"tensorboard\")\n",
    "TENSORBOARD_LOG_DIR = os.path.join(TENSORBOARD_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ba1e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "try:\n",
    "    shutil.rmtree(TENSORBOARD_DIR)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(TENSORBOARD_LOG_DIR, \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "    histogram_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe45638f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d3a8d0210219036d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d3a8d0210219036d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10000/10000 [==============================] - 75s 7ms/step - loss: 15.2252 - sparse_categorical_accuracy: 0.0646 - sparse_categorical_crossentropy: 86443032.0000 - val_loss: 15.4069 - val_sparse_categorical_accuracy: 0.0505 - val_sparse_categorical_crossentropy: 475588832.0000\n",
      "Epoch 2/30\n",
      "10000/10000 [==============================] - 70s 7ms/step - loss: 15.2696 - sparse_categorical_accuracy: 0.0395 - sparse_categorical_crossentropy: 2831698176.0000 - val_loss: 15.3401 - val_sparse_categorical_accuracy: 0.0179 - val_sparse_categorical_crossentropy: 3602289408.0000\n",
      "Epoch 3/30\n",
      "10000/10000 [==============================] - 69s 7ms/step - loss: 15.2901 - sparse_categorical_accuracy: 0.0339 - sparse_categorical_crossentropy: 3749110784.0000 - val_loss: 15.3365 - val_sparse_categorical_accuracy: 0.0179 - val_sparse_categorical_crossentropy: 3602315520.0000\n",
      "Epoch 4/30\n",
      "10000/10000 [==============================] - 69s 7ms/step - loss: 15.3004 - sparse_categorical_accuracy: 0.0326 - sparse_categorical_crossentropy: 2831981568.0000 - val_loss: 15.3351 - val_sparse_categorical_accuracy: 0.0180 - val_sparse_categorical_crossentropy: 3601509632.0000\n",
      "Epoch 5/30\n",
      "10000/10000 [==============================] - 69s 7ms/step - loss: 15.2799 - sparse_categorical_accuracy: 0.0338 - sparse_categorical_crossentropy: 3122717696.0000 - val_loss: 15.3444 - val_sparse_categorical_accuracy: 0.0178 - val_sparse_categorical_crossentropy: 3618463232.0000\n",
      "Epoch 6/30\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 15.2712 - sparse_categorical_accuracy: 0.0335 - sparse_categorical_crossentropy: 4034436096.0000 - val_loss: 15.3124 - val_sparse_categorical_accuracy: 0.0177 - val_sparse_categorical_crossentropy: 7885466624.0000\n",
      "Epoch 7/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2789 - sparse_categorical_accuracy: 0.0236 - sparse_categorical_crossentropy: 7888868352.0000 - val_loss: 15.3195 - val_sparse_categorical_accuracy: 0.0175 - val_sparse_categorical_crossentropy: 7870693888.0000\n",
      "Epoch 8/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2297 - sparse_categorical_accuracy: 0.0243 - sparse_categorical_crossentropy: 6992697344.0000 - val_loss: 15.3075 - val_sparse_categorical_accuracy: 0.0179 - val_sparse_categorical_crossentropy: 7858414592.0000\n",
      "Epoch 9/30\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 15.2484 - sparse_categorical_accuracy: 0.0243 - sparse_categorical_crossentropy: 7955424768.0000 - val_loss: 15.3145 - val_sparse_categorical_accuracy: 0.0177 - val_sparse_categorical_crossentropy: 7899781632.0000\n",
      "Epoch 10/30\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 15.2689 - sparse_categorical_accuracy: 0.0240 - sparse_categorical_crossentropy: 7926568960.0000 - val_loss: 15.3179 - val_sparse_categorical_accuracy: 0.0177 - val_sparse_categorical_crossentropy: 7959045632.0000\n",
      "Epoch 11/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2770 - sparse_categorical_accuracy: 0.0228 - sparse_categorical_crossentropy: 6311202304.0000 - val_loss: 15.3104 - val_sparse_categorical_accuracy: 0.0177 - val_sparse_categorical_crossentropy: 7843829760.0000\n",
      "Epoch 12/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2688 - sparse_categorical_accuracy: 0.0240 - sparse_categorical_crossentropy: 6945048576.0000 - val_loss: 15.3214 - val_sparse_categorical_accuracy: 0.0175 - val_sparse_categorical_crossentropy: 7852924416.0000\n",
      "Epoch 13/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2649 - sparse_categorical_accuracy: 0.0243 - sparse_categorical_crossentropy: 7879050752.0000 - val_loss: 15.3058 - val_sparse_categorical_accuracy: 0.0180 - val_sparse_categorical_crossentropy: 7893008384.0000\n",
      "Epoch 14/30\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 15.2781 - sparse_categorical_accuracy: 0.0236 - sparse_categorical_crossentropy: 7848450560.0000 - val_loss: 15.3212 - val_sparse_categorical_accuracy: 0.0175 - val_sparse_categorical_crossentropy: 7915380224.0000\n",
      "Epoch 15/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2314 - sparse_categorical_accuracy: 0.0242 - sparse_categorical_crossentropy: 7022982656.0000 - val_loss: 15.3202 - val_sparse_categorical_accuracy: 0.0176 - val_sparse_categorical_crossentropy: 7883021312.0000\n",
      "Epoch 16/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2475 - sparse_categorical_accuracy: 0.0244 - sparse_categorical_crossentropy: 7968519680.0000 - val_loss: 15.3086 - val_sparse_categorical_accuracy: 0.0179 - val_sparse_categorical_crossentropy: 7868727808.0000\n",
      "Epoch 17/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2696 - sparse_categorical_accuracy: 0.0240 - sparse_categorical_crossentropy: 7897966080.0000 - val_loss: 15.3139 - val_sparse_categorical_accuracy: 0.0177 - val_sparse_categorical_crossentropy: 7862647808.0000\n",
      "Epoch 18/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2780 - sparse_categorical_accuracy: 0.0228 - sparse_categorical_crossentropy: 6299907584.0000 - val_loss: 15.3049 - val_sparse_categorical_accuracy: 0.0180 - val_sparse_categorical_crossentropy: 7884880896.0000\n",
      "Epoch 19/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2683 - sparse_categorical_accuracy: 0.0240 - sparse_categorical_crossentropy: 6972183552.0000 - val_loss: 15.3204 - val_sparse_categorical_accuracy: 0.0176 - val_sparse_categorical_crossentropy: 7895867392.0000\n",
      "Epoch 20/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2657 - sparse_categorical_accuracy: 0.0242 - sparse_categorical_crossentropy: 7881046528.0000 - val_loss: 15.3132 - val_sparse_categorical_accuracy: 0.0178 - val_sparse_categorical_crossentropy: 7946964480.0000\n",
      "Epoch 21/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2751 - sparse_categorical_accuracy: 0.0237 - sparse_categorical_crossentropy: 7811070976.0000 - val_loss: 15.3192 - val_sparse_categorical_accuracy: 0.0177 - val_sparse_categorical_crossentropy: 7851696640.0000\n",
      "Epoch 22/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2299 - sparse_categorical_accuracy: 0.0243 - sparse_categorical_crossentropy: 7056605184.0000 - val_loss: 15.3244 - val_sparse_categorical_accuracy: 0.0175 - val_sparse_categorical_crossentropy: 7944389120.0000\n",
      "Epoch 23/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2497 - sparse_categorical_accuracy: 0.0250 - sparse_categorical_crossentropy: 8659680256.0000 - val_loss: 15.3083 - val_sparse_categorical_accuracy: 0.0325 - val_sparse_categorical_crossentropy: 18044282880.0000\n",
      "Epoch 24/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2384 - sparse_categorical_accuracy: 0.0347 - sparse_categorical_crossentropy: 18127712256.0000 - val_loss: 15.3297 - val_sparse_categorical_accuracy: 0.0320 - val_sparse_categorical_crossentropy: 18241036288.0000\n",
      "Epoch 25/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2421 - sparse_categorical_accuracy: 0.0339 - sparse_categorical_crossentropy: 14676643840.0000 - val_loss: 15.3260 - val_sparse_categorical_accuracy: 0.0320 - val_sparse_categorical_crossentropy: 18002577408.0000\n",
      "Epoch 26/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2191 - sparse_categorical_accuracy: 0.0348 - sparse_categorical_crossentropy: 16312442880.0000 - val_loss: 15.3198 - val_sparse_categorical_accuracy: 0.0323 - val_sparse_categorical_crossentropy: 18178922496.0000\n",
      "Epoch 27/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2194 - sparse_categorical_accuracy: 0.0351 - sparse_categorical_crossentropy: 18180646912.0000 - val_loss: 15.3106 - val_sparse_categorical_accuracy: 0.0323 - val_sparse_categorical_crossentropy: 18003474432.0000\n",
      "Epoch 28/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.2228 - sparse_categorical_accuracy: 0.0353 - sparse_categorical_crossentropy: 17836967936.0000 - val_loss: 15.3160 - val_sparse_categorical_accuracy: 0.0323 - val_sparse_categorical_crossentropy: 18023008256.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.1879 - sparse_categorical_accuracy: 0.0355 - sparse_categorical_crossentropy: 16483227648.0000 - val_loss: 15.3248 - val_sparse_categorical_accuracy: 0.0321 - val_sparse_categorical_crossentropy: 18192654336.0000\n",
      "Epoch 30/30\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 15.1934 - sparse_categorical_accuracy: 0.0311 - sparse_categorical_crossentropy: 18179549184.0000 - val_loss: 15.1681 - val_sparse_categorical_accuracy: 0.0205 - val_sparse_categorical_crossentropy: 17769506816.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7864070a90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=merged_datasets[TRAINING_TYPE].repeat(),\n",
    "          validation_data=merged_datasets[TEST_TYPE].repeat(), \n",
    "          validation_steps=500,\n",
    "          epochs=30,\n",
    "          steps_per_epoch=10_000,\n",
    "          callbacks=[tensorboard_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "multiple-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 7s 2ms/step - loss: 15.1505 - sparse_categorical_accuracy: 0.0213 - sparse_categorical_crossentropy: 17267476480.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.1504545211792, 0.021316999569535255, 17267476480.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(merged_datasets[EVAL_TYPE].repeat(), steps=int(40000/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd6b9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(tuple_list: Iterable[Tuple[Any, Any]]) -> Tuple[Iterable[Any], Iterable[Any]]:\n",
    "    return tuple(zip(*tuple_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afeb6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPARATOR = \" \"\n",
    "PREDICTED = \"predicted\"\n",
    "TRUE = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a30b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_onehot(array: np.ndarray) -> np.ndarray:\n",
    "    return np.argmax(array, axis=-1)\n",
    "\n",
    "decode_idx: Callable[[np.ndarray], np.ndarray] = np.vectorize(idx_to_char.get)\n",
    "\n",
    "def concat_letter_rows(array: np.ndarray) -> np.ndarray:\n",
    "    return np.apply_along_axis(lambda row: SEPARATOR.join(row), axis=-1, arr=array)\n",
    "\n",
    "def decode(array: np.ndarray, onehot: bool = True):\n",
    "    if onehot:\n",
    "        array = decode_onehot(array)\n",
    "    array = decode_idx(array)\n",
    "    array = concat_letter_rows(array)\n",
    "    if not onehot:\n",
    "        array = np.apply_along_axis(lambda row: row[0], axis=-1, arr=array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24454d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V</td>\n",
       "      <td>V A I V D F S T E K P I I Y P N N G W K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N</td>\n",
       "      <td>Y I S S Y I P H N E E A Q M V S I S K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L A D V G G Y S A</td>\n",
       "      <td>Q V L D I V T K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A</td>\n",
       "      <td>S P G Y T R E E L F K E L A D L I V E I K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A</td>\n",
       "      <td>I E T G V I H V G D E I E I L G L G E D K K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L A D V G G Y _ A I A F T</td>\n",
       "      <td>A K E D F L A D V A K R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W</td>\n",
       "      <td>R G F S N E I I E N I H N A Y R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L A D V G G Y _ A I A</td>\n",
       "      <td>A D L E K E V A L R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N</td>\n",
       "      <td>G Q T A F V S S N T N F V M(Oxidation) L N G Q R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A</td>\n",
       "      <td>I H Q A V E Q M V E S L D M A A G S T F S F D L Y K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A</td>\n",
       "      <td>I V N E P T A A A L A Y G L D K A H K D M K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L A D V G G Y S A I A F</td>\n",
       "      <td>V M I T F G G G S V K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A I M(Oxidation) V D</td>\n",
       "      <td>L K E E Y G I E P A A A A V A V A A G P A A G A A A A E E K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L A D V G G Y _ A</td>\n",
       "      <td>K Y N P I L K R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N</td>\n",
       "      <td>N L A P S G V T F V I V K D D A V G K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L A D V G G Y _ A I A F T M(Oxidation)</td>\n",
       "      <td>A F T A V E L M D G L H K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A I</td>\n",
       "      <td>N M V T G A A Q M D G A I I V V A A T D G P M P Q T R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L A D V G G Y S A I A F</td>\n",
       "      <td>T Y H A A V V D E I R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A I M(Oxidation)</td>\n",
       "      <td>T L L G A D D K A G I A E I V S A V V Y L Q E H P E I K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A</td>\n",
       "      <td>I I N Q E E V A R E M N D L P E S L K G K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         predicted  \\\n",
       "0                                  L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V   \n",
       "1                                    L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N   \n",
       "2                                                                                L A D V G G Y S A   \n",
       "3                                L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A   \n",
       "4                              L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A   \n",
       "5                                                                        L A D V G G Y _ A I A F T   \n",
       "6                                                     L A D V G G Y S A I A F T M(Oxidation) G F W   \n",
       "7                                                                            L A D V G G Y _ A I A   \n",
       "8                                    L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N   \n",
       "9                      L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A   \n",
       "10                             L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A   \n",
       "11                                                                         L A D V G G Y S A I A F   \n",
       "12  L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A I M(Oxidation) V D   \n",
       "13                                                                               L A D V G G Y _ A   \n",
       "14                                   L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N   \n",
       "15                                                          L A D V G G Y _ A I A F T M(Oxidation)   \n",
       "16                   L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A I   \n",
       "17                                                                         L A D V G G Y S A I A F   \n",
       "18      L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A I M(Oxidation)   \n",
       "19                               L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A   \n",
       "\n",
       "                                                           true  \n",
       "0                       V A I V D F S T E K P I I Y P N N G W K  \n",
       "1                         Y I S S Y I P H N E E A Q M V S I S K  \n",
       "2                                               Q V L D I V T K  \n",
       "3                     S P G Y T R E E L F K E L A D L I V E I K  \n",
       "4                   I E T G V I H V G D E I E I L G L G E D K K  \n",
       "5                                       A K E D F L A D V A K R  \n",
       "6                               R G F S N E I I E N I H N A Y R  \n",
       "7                                           A D L E K E V A L R  \n",
       "8              G Q T A F V S S N T N F V M(Oxidation) L N G Q R  \n",
       "9           I H Q A V E Q M V E S L D M A A G S T F S F D L Y K  \n",
       "10                  I V N E P T A A A L A Y G L D K A H K D M K  \n",
       "11                                        V M I T F G G G S V K  \n",
       "12  L K E E Y G I E P A A A A V A V A A G P A A G A A A A E E K  \n",
       "13                                              K Y N P I L K R  \n",
       "14                        N L A P S G V T F V I V K D D A V G K  \n",
       "15                                    A F T A V E L M D G L H K  \n",
       "16        N M V T G A A Q M D G A I I V V A A T D G P M P Q T R  \n",
       "17                                        T Y H A A V V D E I R  \n",
       "18      T L L G A D D K A G I A E I V S A V V Y L Q E H P E I K  \n",
       "19                    I I N Q E E V A R E M N D L P E S L K G K  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds = merged_datasets[EVAL_TYPE].unbatch().batch(1).take(20)\n",
    "\n",
    "x_eval, y_eval = unzip(eval_ds.as_numpy_iterator())\n",
    "y_pred = model.predict(eval_ds)\n",
    "\n",
    "# although the strings look like they have different lengths, they all have the same length\n",
    "eval_df = pd.DataFrame(data=zip(decode(y_pred), decode(y_eval, onehot=False)), columns=[PREDICTED, TRUE])\n",
    "\n",
    "eval_df[PREDICTED] = eval_df[PREDICTED].combine(\n",
    "    other=eval_df[TRUE].str.rstrip(PADDING_CHARACTERS[SEQ] + SEPARATOR).str.split(SEPARATOR).str.len() + 1,\n",
    "    func=lambda seq, length: SEPARATOR.join(seq.split(SEPARATOR)[:length])\n",
    ")\n",
    "\n",
    "#eval_df = eval_df.applymap(lambda s: s.replace(SEPARATOR, \"\"))\n",
    "\n",
    "eval_df[TRUE] = eval_df[TRUE].str.rstrip(PADDING_CHARACTERS[SEQ] + SEPARATOR)\n",
    "\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44bf7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L A D V G G Y _ A I A F T M(Oxidation) G\n",
      "L A D V G G Y S A I A F T M(Oxidation) G F W\n",
      "L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A\n",
      "L A D V G G Y S A I A F T\n",
      "L A D V G G Y _ A I\n",
      "L A D V G G Y S\n",
      "L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N\n",
      "L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E\n",
      "L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A\n",
      "L A D V G G Y S A I\n",
      "L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation)\n",
      "L A D V G G Y _ A I A\n",
      "L A D V G G Y S A I A F T\n",
      "L A D V G G Y S A I\n",
      "L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A\n",
      "L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A\n",
      "L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V\n",
      "L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation) V N V\n",
      "L A D V G G Y _ A I A F T M(Oxidation) G F W M(Oxidation) V N V A A W E E A\n",
      "L A D V G G Y S A I A F T M(Oxidation) G F W M(Oxidation)\n"
     ]
    }
   ],
   "source": [
    "eval_df.predicted.map(print)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d4b261d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ 2,  0,  7, 14,  8,  2, 12, 15, 13,  3, 13,  0,  8, 15, 21, 21,\n",
       "          21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "          21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "          21, 21]], dtype=int8),),\n",
       " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval[:1], y_pred[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

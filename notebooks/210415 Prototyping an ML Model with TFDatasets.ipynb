{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Prototyping an ML Model on Tensorflow Datasets\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from typing import Iterable, Callable, Dict, Any, Tuple, Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from mmproteo.utils import log, utils\n",
    "from mmproteo.utils.formats.mz import FilteringProcessor\n",
    "from mmproteo.utils.formats.tf_dataset import Parquet2DatasetFileProcessor\n",
    "from mmproteo.utils.processing import ItemProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Printing to Stdout\n"
     ]
    }
   ],
   "source": [
    "logger = log.DummyLogger(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/workspace/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-message",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS = glob.glob(FILES_PATH)\n",
    "len(MZMLID_FILE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twenty-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a485487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide_sequence</th>\n",
       "      <th>mz_array</th>\n",
       "      <th>intensity_array</th>\n",
       "      <th>species</th>\n",
       "      <th>istrain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[L, D, N, V, V, Y, R]</td>\n",
       "      <td>[100.03951, 100.07604, 100.08698, 101.0598, 101.07139, 101.107635, 102.05545, 107.04927, 110.07147, 112.050835, 112.07621, 112.07955, 112.08708, 112.11265, 113.07123, 114.05504, 114.10244, 115.0504, 115.08685, 116.07068, 116.97243, 117.10215, 117.8191, 119.049065, 120.08085, 121.08424, 126.054794, 126.06571, 127.0868, 127.095535, 128.07236, 128.08185, 129.06583, 129.1024, 129.1124, 130.0507, 130.08633, 130.09756, 130.10551, 133.06154, 133.09709, 136.07576, 137.07903, 138.06609, 138.0916, 139.08571, 139.69499, 140.0812, 140.14333, 141.06573, 141.1022, 143.08153, 143.11768, 145.06099, 145.09743, 147.11273, 152.07062, 153.06499, 155.08113, 155.1178, 156.10173, 157.09727, 157.10855, 157.13348, 157.14546, 158.08061, 158.09245, 158.13683, 159.07657, 159.09282, 159.11234, 165.1023, 166.06062, 169.08437, 169.09724, 169.13377, 171.07635, 171.11241, 171.14874, 173.0913, 173.12836, 175.11905, 176.12291, 177.10197, 180.06554, 181.09618, 181.13329, 183.11295, 184.09566, 184.11569, 185.05496, 185.12808, 185.16528, 186.1237, 187.07101, 187.10844, 187.12733, 187.1442, 191.11745, 193.09688, ...]</td>\n",
       "      <td>[1472.0198, 1778.061, 982.26117, 849.2956, 7433.908, 1517.598, 10654.481, 1285.867, 22276.096, 1036.7197, 1008.01794, 1077.3555, 17357.316, 1765.9006, 1840.2622, 884.15027, 1030.9141, 1466.3262, 15643.766, 12685.864, 884.1921, 1245.8772, 909.2175, 1600.0315, 22742.06, 1201.5543, 1346.1736, 790.16095, 7063.337, 811.5802, 1109.8296, 6045.0728, 4442.3276, 33509.23, 2367.9402, 1291.7451, 11317.632, 6297.992, 2274.9944, 1097.1204, 1409.3613, 63035.684, 3738.175, 901.6442, 1888.8175, 784.02856, 840.27997, 875.71985, 1409.5555, 2824.3079, 8497.284, 4310.6895, 6666.118, 1519.9, 967.4869, 12989.814, 5915.0684, 942.62933, 4556.132, 1359.317, 5664.3735, 1361.2703, 5972.5083, 20817.252, 1598.2277, 1266.6726, 26993.193, 1077.943, 1951.6704, 796.94104, 1074.771, 1259.3566, 1369.4341, 1098.0599, 14032.469, 3526.5652, 1029.5724, 5769.34, 4063.4348, 1824.1956, 7088.784, 141361.95, 6137.2266, 1245.3956, 1576.3531, 2777.617, 1268.7177, 65213.92, 894.09875, 5262.379, 4305.7144, 1807.8231, 3397.9797, 43168.617, 1053.397, 3030.1729, 3255.7651, 3606.3943, 1305.8389, 4084.5789, ...]</td>\n",
       "      <td>Alcaligenes_faecalis</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[A, G, L, D, N, N, Y, V, K]</td>\n",
       "      <td>[100.03982, 100.07586, 101.071236, 101.107574, 102.05517, 107.04923, 110.0715, 111.05547, 112.0509, 112.08704, 113.071, 115.086555, 116.07075, 116.9723, 119.04964, 120.08085, 127.08648, 128.07094, 128.08191, 129.06592, 129.10237, 130.04881, 130.07718, 130.08636, 130.09673, 130.10614, 136.04158, 136.0756, 137.07896, 139.04985, 141.06577, 141.10187, 142.12172, 143.08133, 143.11786, 144.12074, 147.11266, 148.11627, 152.07063, 155.08133, 155.11841, 157.06078, 157.09642, 157.13316, 158.09187, 159.07617, 159.11238, 166.06183, 169.09738, 169.13321, 171.11256, 171.14896, 173.09091, 173.12814, 173.97696, 174.13239, 175.1189, 181.06166, 181.09688, 183.11264, 185.09157, 186.0881, 186.12384, 187.10779, 187.14383, 195.07558, 195.11305, 197.12822, 201.0985, 201.12306, 202.0831, 204.13431, 211.14339, 212.06625, 212.10194, 215.1022, 215.13902, 216.10397, 223.15463, 226.11867, 227.10811, 228.13367, 228.17004, 229.09404, 229.11703, 230.07692, 230.11682, 231.07915, 233.09244, 235.14424, 242.11343, 242.14897, 244.16553, 246.15945, 246.1812, 247.10516, 247.18224, 250.11873, 254.14992, 255.1447, ...]</td>\n",
       "      <td>[732.6617, 1411.32, 16631.832, 914.66113, 1730.3816, 1458.0236, 11018.118, 814.45605, 824.7896, 4943.581, 1140.477, 3491.9822, 809.35693, 1004.9697, 907.5157, 4783.916, 1493.2881, 1060.6812, 2808.3154, 18674.494, 37496.016, 1481.8646, 1417.5668, 31441.13, 1304.8645, 798.3385, 861.07764, 15419.474, 735.78265, 858.0814, 3415.7173, 1404.7439, 869.0062, 1809.3257, 44420.22, 3551.6487, 40098.707, 1224.2285, 1093.3992, 3180.0164, 834.1555, 840.0687, 1675.8429, 3058.9111, 3239.9705, 3116.7256, 899.7371, 752.41925, 5850.0176, 2989.287, 12923.561, 752.36975, 1145.8423, 3916.3987, 751.41974, 894.45795, 15483.53, 937.4926, 1308.637, 4491.5586, 1013.0089, 781.3709, 1171.0354, 9497.646, 3152.1099, 1440.8368, 904.2847, 4495.3774, 860.7605, 7126.398, 1022.1986, 5772.762, 1022.4706, 3545.5469, 945.3842, 8011.257, 3472.5981, 1743.3401, 728.37146, 1287.6128, 799.1571, 969.7322, 1415.0103, 15196.673, 3107.087, 22553.723, 735.41016, 945.4697, 906.6033, 4455.847, 3310.5352, 4453.3364, 4270.777, 1118.9838, 19797.84, 954.415, 893.77594, 6114.4775, 1068.6388, 729.6997, ...]</td>\n",
       "      <td>Alcaligenes_faecalis</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              peptide_sequence  \\\n",
       "6        [L, D, N, V, V, Y, R]   \n",
       "7  [A, G, L, D, N, N, Y, V, K]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  mz_array  \\\n",
       "6  [100.03951, 100.07604, 100.08698, 101.0598, 101.07139, 101.107635, 102.05545, 107.04927, 110.07147, 112.050835, 112.07621, 112.07955, 112.08708, 112.11265, 113.07123, 114.05504, 114.10244, 115.0504, 115.08685, 116.07068, 116.97243, 117.10215, 117.8191, 119.049065, 120.08085, 121.08424, 126.054794, 126.06571, 127.0868, 127.095535, 128.07236, 128.08185, 129.06583, 129.1024, 129.1124, 130.0507, 130.08633, 130.09756, 130.10551, 133.06154, 133.09709, 136.07576, 137.07903, 138.06609, 138.0916, 139.08571, 139.69499, 140.0812, 140.14333, 141.06573, 141.1022, 143.08153, 143.11768, 145.06099, 145.09743, 147.11273, 152.07062, 153.06499, 155.08113, 155.1178, 156.10173, 157.09727, 157.10855, 157.13348, 157.14546, 158.08061, 158.09245, 158.13683, 159.07657, 159.09282, 159.11234, 165.1023, 166.06062, 169.08437, 169.09724, 169.13377, 171.07635, 171.11241, 171.14874, 173.0913, 173.12836, 175.11905, 176.12291, 177.10197, 180.06554, 181.09618, 181.13329, 183.11295, 184.09566, 184.11569, 185.05496, 185.12808, 185.16528, 186.1237, 187.07101, 187.10844, 187.12733, 187.1442, 191.11745, 193.09688, ...]   \n",
       "7  [100.03982, 100.07586, 101.071236, 101.107574, 102.05517, 107.04923, 110.0715, 111.05547, 112.0509, 112.08704, 113.071, 115.086555, 116.07075, 116.9723, 119.04964, 120.08085, 127.08648, 128.07094, 128.08191, 129.06592, 129.10237, 130.04881, 130.07718, 130.08636, 130.09673, 130.10614, 136.04158, 136.0756, 137.07896, 139.04985, 141.06577, 141.10187, 142.12172, 143.08133, 143.11786, 144.12074, 147.11266, 148.11627, 152.07063, 155.08133, 155.11841, 157.06078, 157.09642, 157.13316, 158.09187, 159.07617, 159.11238, 166.06183, 169.09738, 169.13321, 171.11256, 171.14896, 173.09091, 173.12814, 173.97696, 174.13239, 175.1189, 181.06166, 181.09688, 183.11264, 185.09157, 186.0881, 186.12384, 187.10779, 187.14383, 195.07558, 195.11305, 197.12822, 201.0985, 201.12306, 202.0831, 204.13431, 211.14339, 212.06625, 212.10194, 215.1022, 215.13902, 216.10397, 223.15463, 226.11867, 227.10811, 228.13367, 228.17004, 229.09404, 229.11703, 230.07692, 230.11682, 231.07915, 233.09244, 235.14424, 242.11343, 242.14897, 244.16553, 246.15945, 246.1812, 247.10516, 247.18224, 250.11873, 254.14992, 255.1447, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       intensity_array  \\\n",
       "6  [1472.0198, 1778.061, 982.26117, 849.2956, 7433.908, 1517.598, 10654.481, 1285.867, 22276.096, 1036.7197, 1008.01794, 1077.3555, 17357.316, 1765.9006, 1840.2622, 884.15027, 1030.9141, 1466.3262, 15643.766, 12685.864, 884.1921, 1245.8772, 909.2175, 1600.0315, 22742.06, 1201.5543, 1346.1736, 790.16095, 7063.337, 811.5802, 1109.8296, 6045.0728, 4442.3276, 33509.23, 2367.9402, 1291.7451, 11317.632, 6297.992, 2274.9944, 1097.1204, 1409.3613, 63035.684, 3738.175, 901.6442, 1888.8175, 784.02856, 840.27997, 875.71985, 1409.5555, 2824.3079, 8497.284, 4310.6895, 6666.118, 1519.9, 967.4869, 12989.814, 5915.0684, 942.62933, 4556.132, 1359.317, 5664.3735, 1361.2703, 5972.5083, 20817.252, 1598.2277, 1266.6726, 26993.193, 1077.943, 1951.6704, 796.94104, 1074.771, 1259.3566, 1369.4341, 1098.0599, 14032.469, 3526.5652, 1029.5724, 5769.34, 4063.4348, 1824.1956, 7088.784, 141361.95, 6137.2266, 1245.3956, 1576.3531, 2777.617, 1268.7177, 65213.92, 894.09875, 5262.379, 4305.7144, 1807.8231, 3397.9797, 43168.617, 1053.397, 3030.1729, 3255.7651, 3606.3943, 1305.8389, 4084.5789, ...]   \n",
       "7           [732.6617, 1411.32, 16631.832, 914.66113, 1730.3816, 1458.0236, 11018.118, 814.45605, 824.7896, 4943.581, 1140.477, 3491.9822, 809.35693, 1004.9697, 907.5157, 4783.916, 1493.2881, 1060.6812, 2808.3154, 18674.494, 37496.016, 1481.8646, 1417.5668, 31441.13, 1304.8645, 798.3385, 861.07764, 15419.474, 735.78265, 858.0814, 3415.7173, 1404.7439, 869.0062, 1809.3257, 44420.22, 3551.6487, 40098.707, 1224.2285, 1093.3992, 3180.0164, 834.1555, 840.0687, 1675.8429, 3058.9111, 3239.9705, 3116.7256, 899.7371, 752.41925, 5850.0176, 2989.287, 12923.561, 752.36975, 1145.8423, 3916.3987, 751.41974, 894.45795, 15483.53, 937.4926, 1308.637, 4491.5586, 1013.0089, 781.3709, 1171.0354, 9497.646, 3152.1099, 1440.8368, 904.2847, 4495.3774, 860.7605, 7126.398, 1022.1986, 5772.762, 1022.4706, 3545.5469, 945.3842, 8011.257, 3472.5981, 1743.3401, 728.37146, 1287.6128, 799.1571, 969.7322, 1415.0103, 15196.673, 3107.087, 22553.723, 735.41016, 945.4697, 906.6033, 4455.847, 3310.5352, 4453.3364, 4270.777, 1118.9838, 19797.84, 954.415, 893.77594, 6114.4775, 1068.6388, 729.6997, ...]   \n",
       "\n",
       "                species istrain  \n",
       "6  Alcaligenes_faecalis   Train  \n",
       "7  Alcaligenes_faecalis   Train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(MZMLID_FILE_PATHS[1])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'\n",
    "MZ = 'mz_array'\n",
    "INT = 'intensity_array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_COLUMNS = [MZ, INT]\n",
    "TARGET_DATA_COLUMNS = [SEQ]\n",
    "SPLIT_VALUE_COLUMNS = ['species', 'istrain']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-headset",
   "metadata": {},
   "source": [
    "## Calculating Statistics over all MZMLID Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vertical-tiger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded previous statistics file '../dumps/PXD010000/training_columns/statistics.parquet'\n"
     ]
    }
   ],
   "source": [
    "file_path_count = len(MZMLID_FILE_PATHS)\n",
    "\n",
    "def get_mzmlid_file_stats(item: Tuple[int, str]) -> Dict[str, Any]:\n",
    "    idx, path = item\n",
    "    info_text = f\"Processing item {idx + 1}/{file_path_count} '{path}'\"\n",
    "    if idx % 10 == 0:\n",
    "        logger.info(info_text)\n",
    "    else:\n",
    "        logger.debug(info_text)\n",
    "    df = pd.read_parquet(path)\n",
    "    max_sequence_length = df[SEQ].str.len().max()\n",
    "    max_array_length = df[INT].str.len().max()\n",
    "    alphabet = set.union(*df[SEQ].apply(set))\n",
    "    item_count = len(df)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        \"file_path\": path,\n",
    "        \"max_sequence_length\": max_sequence_length,\n",
    "        \"max_array_length\": max_array_length,\n",
    "        \"alphabet\": alphabet,\n",
    "        \"item_count\": item_count\n",
    "    }\n",
    "\n",
    "if os.path.exists(STATISTICS_FILE_PATH):\n",
    "    file_stats = pd.read_parquet(STATISTICS_FILE_PATH)\n",
    "    file_stats.alphabet = file_stats.alphabet.apply(set)\n",
    "    print(f\"loaded previous statistics file '{STATISTICS_FILE_PATH}'\")\n",
    "else:\n",
    "    file_stats = pd.DataFrame(\n",
    "        ItemProcessor(\n",
    "            items=enumerate(MZMLID_FILE_PATHS),\n",
    "            item_processor=get_mzmlid_file_stats,\n",
    "            action_name=\"analyse\",\n",
    "            subject_name=\"mzmlid file\",\n",
    "            thread_count=0,\n",
    "            logger=logger\n",
    "        ).process()\n",
    "    )\n",
    "    \n",
    "    file_stats_writable = file_stats.copy()\n",
    "    file_stats_writable.alphabet = file_stats_writable.alphabet.apply(list) # cannot store sets\n",
    "    file_stats_writable.to_parquet(STATISTICS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "leading-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>max_sequence_length</th>\n",
       "      <th>max_array_length</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>item_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet</td>\n",
       "      <td>50</td>\n",
       "      <td>1845</td>\n",
       "      <td>{F, S, K, L, N, C, G, T, V, R, Y, A, M(Oxidation), E, W, M, D, I, H, P, Q}</td>\n",
       "      <td>26943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_A_faecalis_LB_aerobic_03_26Feb16_Arwen_16-01-01_mzmlid.parquet</td>\n",
       "      <td>49</td>\n",
       "      <td>1082</td>\n",
       "      <td>{F, S, K, L, N, C, G, T, V, R, Y, A, M(Oxidation), E, W, M, D, I, H, P, Q}</td>\n",
       "      <td>16723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         file_path  \\\n",
       "0             ../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet   \n",
       "1  ../dumps/PXD010000/training_columns/Biodiversity_A_faecalis_LB_aerobic_03_26Feb16_Arwen_16-01-01_mzmlid.parquet   \n",
       "\n",
       "   max_sequence_length  max_array_length  \\\n",
       "0                   50              1845   \n",
       "1                   49              1082   \n",
       "\n",
       "                                                                     alphabet  \\\n",
       "0  {F, S, K, L, N, C, G, T, V, R, Y, A, M(Oxidation), E, W, M, D, I, H, P, Q}   \n",
       "1  {F, S, K, L, N, C, G, T, V, R, Y, A, M(Oxidation), E, W, M, D, I, H, P, Q}   \n",
       "\n",
       "   item_count  \n",
       "0       26943  \n",
       "1       16723  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "following-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_LENGTHS = {\n",
    "    MZ: file_stats.max_array_length.max(),\n",
    "    INT: file_stats.max_array_length.max(),\n",
    "    SEQ: file_stats.max_sequence_length.max()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "early-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding lengths = {'mz_array': 1845, 'intensity_array': 1845, 'peptide_sequence': 50}\n",
      "TOTAL_ITEM_COUNT = 820586\n",
      "ALPHABET = A, C, D, E, F, G, H, I, K, L, M, M(Oxidation), N, P, Q, R, S, T, V, W, Y\n"
     ]
    }
   ],
   "source": [
    "print(\"padding lengths =\", PADDING_LENGTHS)\n",
    "\n",
    "TOTAL_ITEM_COUNT = file_stats.item_count.sum()\n",
    "print(f\"TOTAL_ITEM_COUNT = {TOTAL_ITEM_COUNT}\")\n",
    "\n",
    "ALPHABET = set.union(*file_stats.alphabet)\n",
    "print(f\"ALPHABET = {', '.join(sorted(ALPHABET))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-recipient",
   "metadata": {},
   "source": [
    "## Data Normalization, Padding, and Conversion to Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "headed-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return tf.keras.utils.normalize(x=values, order=2)\n",
    "\n",
    "def base_peak_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return values / values.max(initial=0)\n",
    "\n",
    "# by Tom, probably\n",
    "# don't know, what it's based on\n",
    "def ion_current_normalize(intensities: np.ndarray) -> np.ndarray:\n",
    "    total_sum = np.sum(intensities**2)\n",
    "    normalized = intensities/total_sum\n",
    "    return normalized\n",
    "\n",
    "NORMALIZATION = {\n",
    "    INT: base_peak_normalize\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "waiting-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_CHARACTERS = {\n",
    "    SEQ: '_',\n",
    "    MZ: 0.0,\n",
    "    INT: 0.0,\n",
    "}\n",
    "\n",
    "ALPHABET.add(PADDING_CHARACTERS[SEQ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "understood-necklace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'M(Oxidation)': 11,\n",
       " 'N': 12,\n",
       " 'P': 13,\n",
       " 'Q': 14,\n",
       " 'R': 15,\n",
       " 'S': 16,\n",
       " 'T': 17,\n",
       " 'V': 18,\n",
       " 'W': 19,\n",
       " 'Y': 20,\n",
       " '_': 21}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(ALPHABET))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "INDEX_ALPHABET = idx_to_char.keys()\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "equivalent-portrait",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Processing item 1/40: '../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet'\n",
      "INFO: Processing item 11/40: '../dumps/PXD010000/training_columns/Biodiversity_A_tumefaciens_R2A_aerobic_1_23Nov16_Pippin_16-09-11_mzmlid.parquet'\n",
      "INFO: Processing item 21/40: '../dumps/PXD010000/training_columns/Biodiversity_B_cereus_PN_L_CL_3_09Oct16_Pippin_16-05-06_mzmlid.parquet'\n",
      "INFO: Processing item 31/40: '../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_CMcarb_anaerobic_03_01Feb16_Arwen_15-07-13_mzmlid.parquet'\n",
      "INFO: No mzmlid parquet files were parquet2tf_dataset-processed\n",
      "INFO: Encountered 0 exceptions during processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parquet2DatasetFileProcessor(\n",
    "    training_data_columns=TRAINING_DATA_COLUMNS,\n",
    "    target_data_columns=TARGET_DATA_COLUMNS,\n",
    "    padding_lengths=PADDING_LENGTHS,\n",
    "    padding_characters=PADDING_CHARACTERS,\n",
    "    column_normalizations=NORMALIZATION,\n",
    "    dataset_dump_path_prefix=DATASET_DUMP_PATH,\n",
    "    char_to_idx_mapping_functions={\n",
    "        SEQ: char_to_idx.get\n",
    "    },\n",
    "    item_count=len(MZMLID_FILE_PATHS),\n",
    "    skip_existing=True,\n",
    "    split_on_column_values_of=SPLIT_VALUE_COLUMNS,\n",
    "    logger=logger\n",
    ").process(parquet_file_paths=MZMLID_FILE_PATHS,\n",
    "          thread_count=2)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "323c4964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA_TYPES = {path.split(os.path.sep)[-1] for path in glob.glob(\n",
    "    os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        '*',  # filename\n",
    "        '*',  # species\n",
    "        '*'   # istrain\n",
    "    ))}\n",
    "TRAINING_DATA_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f064edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train = 40\n",
      "e.g.: ../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_B_subtilis_NCIB3610_24h_plates_1_13Jun16_Pippin_16-03-39_mzmlid.parquet/Bacillus_subtilis_NCIB3610/Train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_file_paths = {training_data_type: glob.glob(os.path.join(DATASET_DUMP_PATH, '*', '*', training_data_type))\n",
    "for training_data_type in TRAINING_DATA_TYPES}\n",
    "\n",
    "for training_data_type, paths in dataset_file_paths.items():\n",
    "    print(f\"#{training_data_type} = {len(paths)}\")\n",
    "    print(f\"e.g.: {paths[0]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stupid-malaysia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(1845,), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(1845,), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(50,), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ((tf.TensorSpec(shape=(PADDING_LENGTHS[MZ],), dtype=tf.float32), \n",
    "  tf.TensorSpec(shape=(PADDING_LENGTHS[INT],), dtype=tf.float32)),\n",
    "(tf.TensorSpec(shape=(PADDING_LENGTHS[SEQ],), dtype=tf.int8)))\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aebc3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': [<_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       "  <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typed_datasets = {\n",
    "    training_data_type: [\n",
    "        tf.data.experimental.load(path=path, element_spec=element_spec, compression='GZIP') for path in paths\n",
    "    ] for training_data_type, paths in dataset_file_paths.items()\n",
    "}\n",
    "\n",
    "typed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-difficulty",
   "metadata": {},
   "source": [
    "## Concatenating Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vocal-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for manual splits\n",
    "def split_dataset(dataset, fraction):\n",
    "    split_value = int(len(dataset) * fraction)\n",
    "    a = dataset.take(split_value)\n",
    "    b = dataset.skip(split_value)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9ffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "SHUFFLE_BUFFER_SIZE = 10_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23203e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dumps/PXD010000/training_columns/tf_datasets/cache'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CACHED_DATASET_DUMP_PATH = os.path.join(DATASET_DUMP_PATH, \"cache\")\n",
    "shutil.rmtree(CACHED_DATASET_DUMP_PATH)\n",
    "utils.ensure_dir_exists(CACHED_DATASET_DUMP_PATH)\n",
    "CACHED_DATASET_DUMP_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf04e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': <CacheDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_datasets(datasets: List[tf.data.Dataset]) -> tf.data.Dataset:\n",
    "    dataset = datasets[0]\n",
    "    for ds in datasets[1:]:\n",
    "        dataset = dataset.concatenate(ds)\n",
    "    return dataset\n",
    "\n",
    "merged_datasets = {\n",
    "    training_data_type: concatenate_datasets(datasets)\n",
    "        .cache(os.path.join(CACHED_DATASET_DUMP_PATH, training_data_type))\n",
    "    for training_data_type, datasets in typed_datasets.items()\n",
    "}\n",
    "\n",
    "merged_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c881d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimal_model():\n",
    "    input_layers = [tf.keras.layers.Input(shape=(PADDING_LENGTHS[col],)) for col in TRAINING_DATA_COLUMNS]\n",
    "    \n",
    "    x = input_layers[0]\n",
    "    for input_layer in input_layers[1:]:\n",
    "        x = x + input_layer\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(PADDING_LENGTHS[SEQ]*len(ALPHABET))(x)\n",
    "    x = tf.reshape(x,(-1, PADDING_LENGTHS[SEQ], len(ALPHABET)))\n",
    "    x = tf.keras.activations.softmax(x)\n",
    "    model = tf.keras.Model(input_layers,x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model\n",
    "        \n",
    "def fill_cache(dataset):\n",
    "    model = get_minimal_model()\n",
    "    model.fit(dataset.batch(BATCH_SIZE), epochs=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cbc0466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3206/3206 [==============================] - 20s 6ms/step - loss: 6.0332\n"
     ]
    }
   ],
   "source": [
    "merged_datasets = {\n",
    "    training_data_type: fill_cache(dataset)\n",
    "        .shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    for training_data_type, dataset in merged_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ee8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TYPE = 'Train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dress-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1845) dtype=float32 (created by layer 'input_5')>,\n",
       " <KerasTensor: shape=(None, 1845) dtype=float32 (created by layer 'input_6')>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layers = [tf.keras.layers.Input(shape=(PADDING_LENGTHS[col],)) for col in TRAINING_DATA_COLUMNS]\n",
    "input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "developmental-geneva",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 1845)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1845)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 1845)         0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1845)         0           tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1100)         2030600     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_2 (TFOpLambda)       (None, 50, 22)       0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_2 (TFOpLambd (None, 50, 1)        0           tf.reshape_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_2 (TFOpLambda) (None, 50, 22)       0           tf.reshape_2[0][0]               \n",
      "                                                                 tf.math.reduce_max_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.exp_2 (TFOpLambda)      (None, 50, 22)       0           tf.math.subtract_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_2 (TFOpLambd (None, 50, 1)        0           tf.math.exp_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_2 (TFOpLambda)  (None, 50, 22)       0           tf.math.exp_2[0][0]              \n",
      "                                                                 tf.math.reduce_sum_2[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 2,030,600\n",
      "Trainable params: 2,030,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = input_layers[0]\n",
    "for input_layer in input_layers[1:]:\n",
    "    x = x + input_layer\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(PADDING_LENGTHS[SEQ]*len(ALPHABET))(x)\n",
    "\n",
    "x = tf.reshape(x,(-1, PADDING_LENGTHS[SEQ], len(ALPHABET)))\n",
    "\n",
    "x = tf.keras.activations.softmax(x)\n",
    "\n",
    "model = tf.keras.Model(input_layers,x)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3205/3205 [==============================] - 24s 4ms/step - loss: 6.9082\n",
      "Epoch 2/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8943\n",
      "Epoch 3/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8890\n",
      "Epoch 4/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8916\n",
      "Epoch 5/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8901\n",
      "Epoch 6/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8931\n",
      "Epoch 7/10\n",
      "3205/3205 [==============================] - 20s 4ms/step - loss: 5.8941\n",
      "Epoch 8/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8950\n",
      "Epoch 9/10\n",
      "3205/3205 [==============================] - 20s 4ms/step - loss: 5.8917\n",
      "Epoch 10/10\n",
      "3205/3205 [==============================] - 21s 4ms/step - loss: 5.8885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b2452e7b8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(merged_datasets[TRAINING_TYPE], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-appendix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

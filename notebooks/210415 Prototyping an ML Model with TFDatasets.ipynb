{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Prototyping an ML Model\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from mmproteo.utils.utils import ensure_dir_exists\n",
    "from mmproteo.utils import log\n",
    "from mmproteo.utils.formats.mz import FilteringProcessor, filter_files\n",
    "from mmproteo.utils.processing import ItemProcessor\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Iterable, Callable, Dict, Any, Tuple\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Printing to Stdout\n"
     ]
    }
   ],
   "source": [
    "logger = log.DummyLogger(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/workspace/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-message",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS = glob.glob(FILES_PATH)\n",
    "len(MZMLID_FILE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twenty-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dumps/PXD010000/training_columns/Biodiversity_S_agalactiae_LIB_aerobic_02_26Feb16_Arwen_16-01-01_mzmlid.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = FilteringProcessor.default_peptide_sequence_column_name\n",
    "MZ = FilteringProcessor.default_mz_array_column_name\n",
    "INT = FilteringProcessor.default_intensity_array_column_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-headset",
   "metadata": {},
   "source": [
    "## Calculating Statistics over all MZMLID Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "offensive-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vertical-tiger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded previous statistics\n"
     ]
    }
   ],
   "source": [
    "file_path_count = len(MZMLID_FILE_PATHS)\n",
    "\n",
    "def get_mzmlid_file_stats(item: Tuple[int, str]) -> Dict[str, Any]:\n",
    "    idx, path = item\n",
    "    info_text = f\"Processing item {idx + 1}/{file_path_count} '{path}'\"\n",
    "    if idx % 10 == 0:\n",
    "        logger.info(info_text)\n",
    "    else:\n",
    "        logger.debug(info_text)\n",
    "    df = pd.read_parquet(path)\n",
    "    max_sequence_length = df[SEQ].str.replace(r\"[^A-Z]\",'').str.len().max()\n",
    "    max_array_length = df[INT].str.len().max()\n",
    "    alphabet = set.union(*df[SEQ].apply(set))\n",
    "    item_count = len(df)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        \"file_path\": path,\n",
    "        \"max_sequence_length\": max_sequence_length,\n",
    "        \"max_array_length\": max_array_length,\n",
    "        \"alphabet\": alphabet,\n",
    "        \"item_count\": item_count\n",
    "    }\n",
    "\n",
    "if os.path.exists(STATISTICS_FILE_PATH):\n",
    "    file_stats = pd.read_parquet(STATISTICS_FILE_PATH)\n",
    "    file_stats.alphabet = file_stats.alphabet.apply(set)\n",
    "    print(\"loaded previous statistics\")\n",
    "else:\n",
    "    file_stats = pd.DataFrame(\n",
    "        ItemProcessor(\n",
    "            items=enumerate(MZMLID_FILE_PATHS),\n",
    "            item_processor=get_mzmlid_file_stats,\n",
    "            action_name=\"analyse\",\n",
    "            subject_name=\"mzmlid file\",\n",
    "            thread_count=0,\n",
    "            logger=logger\n",
    "        ).process()\n",
    "    )\n",
    "    \n",
    "    file_stats_writable = file_stats.copy()\n",
    "    file_stats_writable.alphabet = file_stats_writable.alphabet.apply(list) # cannot store sets\n",
    "    file_stats_writable.write_parquet(STATISTICS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "leading-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>max_sequence_length</th>\n",
       "      <th>max_array_length</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>item_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_S_agalactiae_LIB_aerobic_02_26Feb16_Arwen_16-01-01_mzmlid.parquet</td>\n",
       "      <td>50</td>\n",
       "      <td>1267</td>\n",
       "      <td>{A, Q, D, Y, I, T, P, L, M, F, C, N, S, G, V, W, K, R, E, H}</td>\n",
       "      <td>13279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet</td>\n",
       "      <td>50</td>\n",
       "      <td>1845</td>\n",
       "      <td>{A, Q, D, Y, T, P, E, L, M, F, C, N, S, G, V, W, K, R, I, H}</td>\n",
       "      <td>26830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            file_path  \\\n",
       "0  ../dumps/PXD010000/training_columns/Biodiversity_S_agalactiae_LIB_aerobic_02_26Feb16_Arwen_16-01-01_mzmlid.parquet   \n",
       "1                ../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet   \n",
       "\n",
       "   max_sequence_length  max_array_length  \\\n",
       "0                   50              1267   \n",
       "1                   50              1845   \n",
       "\n",
       "                                                       alphabet  item_count  \n",
       "0  {A, Q, D, Y, I, T, P, L, M, F, C, N, S, G, V, W, K, R, E, H}       13279  \n",
       "1  {A, Q, D, Y, T, P, E, L, M, F, C, N, S, G, V, W, K, R, I, H}       26830  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "early-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SEQUENCE_LENGTH = 50\n",
      "MAX_ARRAY_LENGTH = 2354\n",
      "TOTAL_ITEM_COUNT = 5408046\n",
      "ALPHABET = A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = file_stats.max_sequence_length.max()\n",
    "print(f\"MAX_SEQUENCE_LENGTH = {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "MAX_ARRAY_LENGTH = file_stats.max_array_length.max()\n",
    "print(f\"MAX_ARRAY_LENGTH = {MAX_ARRAY_LENGTH}\")\n",
    "\n",
    "TOTAL_ITEM_COUNT = file_stats.item_count.sum()\n",
    "print(f\"TOTAL_ITEM_COUNT = {TOTAL_ITEM_COUNT}\")\n",
    "\n",
    "ALPHABET = set.union(*file_stats.alphabet)\n",
    "print(f\"ALPHABET = {', '.join(sorted(ALPHABET))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-recipient",
   "metadata": {},
   "source": [
    "## Data Normalization, Padding, and Conversion to Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "headed-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return tf.keras.utils.normalize(x=values, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finnish-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_peak_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return values / values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "floating-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by Tom, probably\n",
    "# don't know, what it's based on\n",
    "def ion_current_normalize(intensities):\n",
    "    total_sum = np.sum(intensities**2)\n",
    "    normalized = intensities/total_sum\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "funky-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION=base_peak_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "waiting-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_CHARACTERS = {\n",
    "    SEQ: '_',\n",
    "    MZ: 0.0,\n",
    "    INT: 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sudden-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET.add(PADDING_CHARACTERS[SEQ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "understood-necklace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'N': 11,\n",
       " 'P': 12,\n",
       " 'Q': 13,\n",
       " 'R': 14,\n",
       " 'S': 15,\n",
       " 'T': 16,\n",
       " 'V': 17,\n",
       " 'W': 18,\n",
       " 'Y': 19,\n",
       " '_': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(ALPHABET))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "INDEX_ALPHABET = idx_to_char.keys()\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "listed-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARRAY_COLS = [MZ, INT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "following-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensities(df: pd.DataFrame):\n",
    "    df[INT] = df[INT].apply(NORMALIZATION)\n",
    "\n",
    "def pad_sequence_column(df: pd.DataFrame):\n",
    "    df[SEQ] = df[SEQ].str.pad(\n",
    "        width=MAX_SEQUENCE_LENGTH, \n",
    "        fillchar=PADDING_CHARACTERS[SEQ], \n",
    "        side='right'\n",
    "    )\n",
    "\n",
    "def pad_array_columns(df: pd.DataFrame):\n",
    "    for col in ARRAY_COLS:\n",
    "        if len(df[col]) == 0:\n",
    "            continue\n",
    "        item_dtype = df[col].iloc[0].dtype\n",
    "\n",
    "        df[col] = list(tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences=df[col], \n",
    "            maxlen=MAX_ARRAY_LENGTH, \n",
    "            padding='post', \n",
    "            value=PADDING_CHARACTERS[col],\n",
    "            dtype=item_dtype\n",
    "        ))\n",
    "\n",
    "def _sequence_to_indices(sequence: Iterable[str], \n",
    "                char_to_idx_mapping_fun: Callable[[str], int] = char_to_idx.get) -> np.ndarray:\n",
    "    return np.array([char_to_idx_mapping_fun(char) for char in sequence], dtype=np.int8)\n",
    "\n",
    "def sequence_column_to_indices(df: pd.DataFrame):\n",
    "    df[SEQ] = df[SEQ].apply(list).apply(_sequence_to_indices)\n",
    "\n",
    "def stack_numpy_arrays_in_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.apply(lambda item: [np.stack(item)])\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    normalize_intensities(df)\n",
    "    pad_sequence_column(df)\n",
    "    pad_array_columns(df)\n",
    "    sequence_column_to_indices(df)\n",
    "    return stack_numpy_arrays_in_dataframe(df)\n",
    "\n",
    "def df2dataset(stacked_df: pd.DataFrame) -> tf.data.Dataset:\n",
    "    training_data = tuple(stacked_df[ARRAY_COLS].iloc[0])\n",
    "    target_data = tuple(stacked_df[[SEQ]].iloc[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((training_data, target_data))\n",
    "    return dataset\n",
    "    \n",
    "def parquet_file_to_dataset_file_converter(item: Tuple[int, str]) -> str:\n",
    "    idx, path = item\n",
    "    tf_dataset_path = os.path.join(\n",
    "        DATASET_DUMP_PATH, \n",
    "        path[len(TRAINING_COLUMNS_DUMP_PATH)+len(os.path.sep):])\n",
    "    if os.path.exists(tf_dataset_path):\n",
    "        logger.debug(f\"Skipped '{path}' because '{tf_dataset_path}' already exists\")\n",
    "        return None\n",
    "    \n",
    "    info_text = f\"Processing item {idx + 1}/{len(MZMLID_FILE_PATHS)}: '{path}'\"\n",
    "    if idx % 10 == 0:\n",
    "        logger.info(info_text)\n",
    "    else:\n",
    "        logger.debug(info_text)\n",
    "    df = pd.read_parquet(path)\n",
    "    df = preprocess_dataframe(df)\n",
    "    dataset = df2dataset(df)\n",
    "    logger.debug(dataset.element_spec)\n",
    "    \n",
    "    tf.data.experimental.save(dataset=dataset, path=tf_dataset_path, compression='GZIP')\n",
    "    \n",
    "    del dataset\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return tf_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "equivalent-portrait",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: No mzmlid parquet files were parquet2tf_dataset-processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_paths = list(ItemProcessor(\n",
    "    items=enumerate(MZMLID_FILE_PATHS),\n",
    "    item_processor=parquet_file_to_dataset_file_converter,\n",
    "    action_name=\"parquet2tf_dataset-process\",\n",
    "    subject_name=\"mzmlid parquet file\",\n",
    "    thread_count=2,\n",
    "    logger=logger\n",
    ").process())\n",
    "dataset_file_paths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fundamental-karen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_HL48_HLHxylose_aerobic_2_09Jun16_Pippin_16-03-39_mzmlid.parquet',\n",
       " '../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_P_ruminicola_MDM_anaerobic_1_09Jun16_Pippin_16-03-39_mzmlid.parquet',\n",
       " '../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_C_Baltica_T240_R3_C_27Jan16_Arwen_15-07-13_mzmlid.parquet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_paths = glob.glob(os.path.join(DATASET_DUMP_PATH, '*'))\n",
    "dataset_file_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stupid-malaysia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(2354,), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2354,), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(50,), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ((tf.TensorSpec(shape=(MAX_ARRAY_LENGTH,), dtype=tf.float32), \n",
    "  tf.TensorSpec(shape=(MAX_ARRAY_LENGTH,), dtype=tf.float32)),\n",
    "(tf.TensorSpec(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int8)))\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stretch-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [tf.data.experimental.load(path=path, element_spec=element_spec, compression='GZIP') \n",
    "            for path in dataset_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "surprised-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_LoadDataset shapes: (((2354,), (2354,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " <_LoadDataset shapes: (((2354,), (2354,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " <_LoadDataset shapes: (((2354,), (2354,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-difficulty",
   "metadata": {},
   "source": [
    "## Concatenating Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "billion-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset = datasets[0]\n",
    "for ds in datasets[1:]:\n",
    "    dataset = dataset.concatenate(ds)\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dress-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mz_array': <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'input_1')>,\n",
       " 'intensity_array': <KerasTensor: shape=(None, 2354) dtype=float32 (created by layer 'input_2')>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layers = {col: tf.keras.layers.Input(shape=(MAX_ARRAY_LENGTH,)) for col in ARRAY_COLS}\n",
    "input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "developmental-geneva",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2354)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2354)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 2354)         0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2354)         0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16800)        39564000    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16800)        0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1050)         17641050    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (None, 50, 21)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max (TFOpLambda) (None, 50, 1)        0           tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 50, 21)       0           tf.reshape[0][0]                 \n",
      "                                                                 tf.math.reduce_max[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.exp (TFOpLambda)        (None, 50, 21)       0           tf.math.subtract[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 50, 1)        0           tf.math.exp[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None, 50, 21)       0           tf.math.exp[0][0]                \n",
      "                                                                 tf.math.reduce_sum[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 57,205,050\n",
      "Trainable params: 57,205,050\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = input_layers[MZ] + input_layers[INT]\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "for _ in range(1):\n",
    "    x = tf.keras.layers.Dense(16*MAX_SEQUENCE_LENGTH*len(ALPHABET))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(MAX_SEQUENCE_LENGTH*len(ALPHABET))(x)\n",
    "\n",
    "x = tf.reshape(x,(-1, MAX_SEQUENCE_LENGTH, len(ALPHABET)))\n",
    "\n",
    "x = tf.keras.activations.softmax(x)\n",
    "\n",
    "model = tf.keras.Model([input_layers[MZ],input_layers[INT]],x)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "vocal-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, fraction):\n",
    "    split_value = int(len(dataset) * fraction)\n",
    "    a = dataset.take(split_value)\n",
    "    b = dataset.skip(split_value)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tracked-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=int(10000 / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21126/21126 [==============================] - 877s 41ms/step - loss: 6.0935\n",
      "Epoch 2/10\n",
      "  209/21126 [..............................] - ETA: 15:32 - loss: 5.4155"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f8c5e0c71664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-appendix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

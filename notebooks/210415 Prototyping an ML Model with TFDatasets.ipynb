{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-landing",
   "metadata": {},
   "source": [
    "# Prototyping an ML Model on Tensorflow Datasets\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "from typing import Iterable, Callable, Dict, Any, Tuple, Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from mmproteo.utils import log\n",
    "from mmproteo.utils.formats.mz import FilteringProcessor\n",
    "from mmproteo.utils.processing import ItemProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Printing to Stdout\n"
     ]
    }
   ],
   "source": [
    "logger = log.DummyLogger(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-playlist",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/workspace/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genuine-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PXD010000\"\n",
    "DUMP_PATH = os.path.join(\"..\", \"dumps\", PROJECT)\n",
    "TRAINING_COLUMNS_DUMP_PATH = os.path.join(DUMP_PATH, \"training_columns\")\n",
    "FILES_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"*_mzmlid.parquet\")\n",
    "STATISTICS_FILE_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"statistics.parquet\")\n",
    "DATASET_DUMP_PATH = os.path.join(TRAINING_COLUMNS_DUMP_PATH, \"tf_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-message",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS = glob.glob(FILES_PATH)\n",
    "len(MZMLID_FILE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twenty-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MZMLID_FILE_PATHS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a485487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide_sequence</th>\n",
       "      <th>mz_array</th>\n",
       "      <th>intensity_array</th>\n",
       "      <th>species</th>\n",
       "      <th>istrain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[L, D, N, V, V, Y, R]</td>\n",
       "      <td>[100.03951, 100.07604, 100.08698, 101.0598, 101.07139, 101.107635, 102.05545, 107.04927, 110.07147, 112.050835, 112.07621, 112.07955, 112.08708, 112.11265, 113.07123, 114.05504, 114.10244, 115.0504, 115.08685, 116.07068, 116.97243, 117.10215, 117.8191, 119.049065, 120.08085, 121.08424, 126.054794, 126.06571, 127.0868, 127.095535, 128.07236, 128.08185, 129.06583, 129.1024, 129.1124, 130.0507, 130.08633, 130.09756, 130.10551, 133.06154, 133.09709, 136.07576, 137.07903, 138.06609, 138.0916, 139.08571, 139.69499, 140.0812, 140.14333, 141.06573, 141.1022, 143.08153, 143.11768, 145.06099, 145.09743, 147.11273, 152.07062, 153.06499, 155.08113, 155.1178, 156.10173, 157.09727, 157.10855, 157.13348, 157.14546, 158.08061, 158.09245, 158.13683, 159.07657, 159.09282, 159.11234, 165.1023, 166.06062, 169.08437, 169.09724, 169.13377, 171.07635, 171.11241, 171.14874, 173.0913, 173.12836, 175.11905, 176.12291, 177.10197, 180.06554, 181.09618, 181.13329, 183.11295, 184.09566, 184.11569, 185.05496, 185.12808, 185.16528, 186.1237, 187.07101, 187.10844, 187.12733, 187.1442, 191.11745, 193.09688, ...]</td>\n",
       "      <td>[1472.0198, 1778.061, 982.26117, 849.2956, 7433.908, 1517.598, 10654.481, 1285.867, 22276.096, 1036.7197, 1008.01794, 1077.3555, 17357.316, 1765.9006, 1840.2622, 884.15027, 1030.9141, 1466.3262, 15643.766, 12685.864, 884.1921, 1245.8772, 909.2175, 1600.0315, 22742.06, 1201.5543, 1346.1736, 790.16095, 7063.337, 811.5802, 1109.8296, 6045.0728, 4442.3276, 33509.23, 2367.9402, 1291.7451, 11317.632, 6297.992, 2274.9944, 1097.1204, 1409.3613, 63035.684, 3738.175, 901.6442, 1888.8175, 784.02856, 840.27997, 875.71985, 1409.5555, 2824.3079, 8497.284, 4310.6895, 6666.118, 1519.9, 967.4869, 12989.814, 5915.0684, 942.62933, 4556.132, 1359.317, 5664.3735, 1361.2703, 5972.5083, 20817.252, 1598.2277, 1266.6726, 26993.193, 1077.943, 1951.6704, 796.94104, 1074.771, 1259.3566, 1369.4341, 1098.0599, 14032.469, 3526.5652, 1029.5724, 5769.34, 4063.4348, 1824.1956, 7088.784, 141361.95, 6137.2266, 1245.3956, 1576.3531, 2777.617, 1268.7177, 65213.92, 894.09875, 5262.379, 4305.7144, 1807.8231, 3397.9797, 43168.617, 1053.397, 3030.1729, 3255.7651, 3606.3943, 1305.8389, 4084.5789, ...]</td>\n",
       "      <td>Alcaligenes_faecalis</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[A, G, L, D, N, N, Y, V, K]</td>\n",
       "      <td>[100.03982, 100.07586, 101.071236, 101.107574, 102.05517, 107.04923, 110.0715, 111.05547, 112.0509, 112.08704, 113.071, 115.086555, 116.07075, 116.9723, 119.04964, 120.08085, 127.08648, 128.07094, 128.08191, 129.06592, 129.10237, 130.04881, 130.07718, 130.08636, 130.09673, 130.10614, 136.04158, 136.0756, 137.07896, 139.04985, 141.06577, 141.10187, 142.12172, 143.08133, 143.11786, 144.12074, 147.11266, 148.11627, 152.07063, 155.08133, 155.11841, 157.06078, 157.09642, 157.13316, 158.09187, 159.07617, 159.11238, 166.06183, 169.09738, 169.13321, 171.11256, 171.14896, 173.09091, 173.12814, 173.97696, 174.13239, 175.1189, 181.06166, 181.09688, 183.11264, 185.09157, 186.0881, 186.12384, 187.10779, 187.14383, 195.07558, 195.11305, 197.12822, 201.0985, 201.12306, 202.0831, 204.13431, 211.14339, 212.06625, 212.10194, 215.1022, 215.13902, 216.10397, 223.15463, 226.11867, 227.10811, 228.13367, 228.17004, 229.09404, 229.11703, 230.07692, 230.11682, 231.07915, 233.09244, 235.14424, 242.11343, 242.14897, 244.16553, 246.15945, 246.1812, 247.10516, 247.18224, 250.11873, 254.14992, 255.1447, ...]</td>\n",
       "      <td>[732.6617, 1411.32, 16631.832, 914.66113, 1730.3816, 1458.0236, 11018.118, 814.45605, 824.7896, 4943.581, 1140.477, 3491.9822, 809.35693, 1004.9697, 907.5157, 4783.916, 1493.2881, 1060.6812, 2808.3154, 18674.494, 37496.016, 1481.8646, 1417.5668, 31441.13, 1304.8645, 798.3385, 861.07764, 15419.474, 735.78265, 858.0814, 3415.7173, 1404.7439, 869.0062, 1809.3257, 44420.22, 3551.6487, 40098.707, 1224.2285, 1093.3992, 3180.0164, 834.1555, 840.0687, 1675.8429, 3058.9111, 3239.9705, 3116.7256, 899.7371, 752.41925, 5850.0176, 2989.287, 12923.561, 752.36975, 1145.8423, 3916.3987, 751.41974, 894.45795, 15483.53, 937.4926, 1308.637, 4491.5586, 1013.0089, 781.3709, 1171.0354, 9497.646, 3152.1099, 1440.8368, 904.2847, 4495.3774, 860.7605, 7126.398, 1022.1986, 5772.762, 1022.4706, 3545.5469, 945.3842, 8011.257, 3472.5981, 1743.3401, 728.37146, 1287.6128, 799.1571, 969.7322, 1415.0103, 15196.673, 3107.087, 22553.723, 735.41016, 945.4697, 906.6033, 4455.847, 3310.5352, 4453.3364, 4270.777, 1118.9838, 19797.84, 954.415, 893.77594, 6114.4775, 1068.6388, 729.6997, ...]</td>\n",
       "      <td>Alcaligenes_faecalis</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              peptide_sequence  \\\n",
       "6        [L, D, N, V, V, Y, R]   \n",
       "7  [A, G, L, D, N, N, Y, V, K]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  mz_array  \\\n",
       "6  [100.03951, 100.07604, 100.08698, 101.0598, 101.07139, 101.107635, 102.05545, 107.04927, 110.07147, 112.050835, 112.07621, 112.07955, 112.08708, 112.11265, 113.07123, 114.05504, 114.10244, 115.0504, 115.08685, 116.07068, 116.97243, 117.10215, 117.8191, 119.049065, 120.08085, 121.08424, 126.054794, 126.06571, 127.0868, 127.095535, 128.07236, 128.08185, 129.06583, 129.1024, 129.1124, 130.0507, 130.08633, 130.09756, 130.10551, 133.06154, 133.09709, 136.07576, 137.07903, 138.06609, 138.0916, 139.08571, 139.69499, 140.0812, 140.14333, 141.06573, 141.1022, 143.08153, 143.11768, 145.06099, 145.09743, 147.11273, 152.07062, 153.06499, 155.08113, 155.1178, 156.10173, 157.09727, 157.10855, 157.13348, 157.14546, 158.08061, 158.09245, 158.13683, 159.07657, 159.09282, 159.11234, 165.1023, 166.06062, 169.08437, 169.09724, 169.13377, 171.07635, 171.11241, 171.14874, 173.0913, 173.12836, 175.11905, 176.12291, 177.10197, 180.06554, 181.09618, 181.13329, 183.11295, 184.09566, 184.11569, 185.05496, 185.12808, 185.16528, 186.1237, 187.07101, 187.10844, 187.12733, 187.1442, 191.11745, 193.09688, ...]   \n",
       "7  [100.03982, 100.07586, 101.071236, 101.107574, 102.05517, 107.04923, 110.0715, 111.05547, 112.0509, 112.08704, 113.071, 115.086555, 116.07075, 116.9723, 119.04964, 120.08085, 127.08648, 128.07094, 128.08191, 129.06592, 129.10237, 130.04881, 130.07718, 130.08636, 130.09673, 130.10614, 136.04158, 136.0756, 137.07896, 139.04985, 141.06577, 141.10187, 142.12172, 143.08133, 143.11786, 144.12074, 147.11266, 148.11627, 152.07063, 155.08133, 155.11841, 157.06078, 157.09642, 157.13316, 158.09187, 159.07617, 159.11238, 166.06183, 169.09738, 169.13321, 171.11256, 171.14896, 173.09091, 173.12814, 173.97696, 174.13239, 175.1189, 181.06166, 181.09688, 183.11264, 185.09157, 186.0881, 186.12384, 187.10779, 187.14383, 195.07558, 195.11305, 197.12822, 201.0985, 201.12306, 202.0831, 204.13431, 211.14339, 212.06625, 212.10194, 215.1022, 215.13902, 216.10397, 223.15463, 226.11867, 227.10811, 228.13367, 228.17004, 229.09404, 229.11703, 230.07692, 230.11682, 231.07915, 233.09244, 235.14424, 242.11343, 242.14897, 244.16553, 246.15945, 246.1812, 247.10516, 247.18224, 250.11873, 254.14992, 255.1447, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       intensity_array  \\\n",
       "6  [1472.0198, 1778.061, 982.26117, 849.2956, 7433.908, 1517.598, 10654.481, 1285.867, 22276.096, 1036.7197, 1008.01794, 1077.3555, 17357.316, 1765.9006, 1840.2622, 884.15027, 1030.9141, 1466.3262, 15643.766, 12685.864, 884.1921, 1245.8772, 909.2175, 1600.0315, 22742.06, 1201.5543, 1346.1736, 790.16095, 7063.337, 811.5802, 1109.8296, 6045.0728, 4442.3276, 33509.23, 2367.9402, 1291.7451, 11317.632, 6297.992, 2274.9944, 1097.1204, 1409.3613, 63035.684, 3738.175, 901.6442, 1888.8175, 784.02856, 840.27997, 875.71985, 1409.5555, 2824.3079, 8497.284, 4310.6895, 6666.118, 1519.9, 967.4869, 12989.814, 5915.0684, 942.62933, 4556.132, 1359.317, 5664.3735, 1361.2703, 5972.5083, 20817.252, 1598.2277, 1266.6726, 26993.193, 1077.943, 1951.6704, 796.94104, 1074.771, 1259.3566, 1369.4341, 1098.0599, 14032.469, 3526.5652, 1029.5724, 5769.34, 4063.4348, 1824.1956, 7088.784, 141361.95, 6137.2266, 1245.3956, 1576.3531, 2777.617, 1268.7177, 65213.92, 894.09875, 5262.379, 4305.7144, 1807.8231, 3397.9797, 43168.617, 1053.397, 3030.1729, 3255.7651, 3606.3943, 1305.8389, 4084.5789, ...]   \n",
       "7           [732.6617, 1411.32, 16631.832, 914.66113, 1730.3816, 1458.0236, 11018.118, 814.45605, 824.7896, 4943.581, 1140.477, 3491.9822, 809.35693, 1004.9697, 907.5157, 4783.916, 1493.2881, 1060.6812, 2808.3154, 18674.494, 37496.016, 1481.8646, 1417.5668, 31441.13, 1304.8645, 798.3385, 861.07764, 15419.474, 735.78265, 858.0814, 3415.7173, 1404.7439, 869.0062, 1809.3257, 44420.22, 3551.6487, 40098.707, 1224.2285, 1093.3992, 3180.0164, 834.1555, 840.0687, 1675.8429, 3058.9111, 3239.9705, 3116.7256, 899.7371, 752.41925, 5850.0176, 2989.287, 12923.561, 752.36975, 1145.8423, 3916.3987, 751.41974, 894.45795, 15483.53, 937.4926, 1308.637, 4491.5586, 1013.0089, 781.3709, 1171.0354, 9497.646, 3152.1099, 1440.8368, 904.2847, 4495.3774, 860.7605, 7126.398, 1022.1986, 5772.762, 1022.4706, 3545.5469, 945.3842, 8011.257, 3472.5981, 1743.3401, 728.37146, 1287.6128, 799.1571, 969.7322, 1415.0103, 15196.673, 3107.087, 22553.723, 735.41016, 945.4697, 906.6033, 4455.847, 3310.5352, 4453.3364, 4270.777, 1118.9838, 19797.84, 954.415, 893.77594, 6114.4775, 1068.6388, 729.6997, ...]   \n",
       "\n",
       "                species istrain  \n",
       "6  Alcaligenes_faecalis   Train  \n",
       "7  Alcaligenes_faecalis   Train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(MZMLID_FILE_PATHS[1]).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interesting-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = 'peptide_sequence'\n",
    "MZ = FilteringProcessor.default_mz_array_column_name\n",
    "INT = FilteringProcessor.default_intensity_array_column_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-headset",
   "metadata": {},
   "source": [
    "## Calculating Statistics over all MZMLID Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vertical-tiger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded previous statistics file '../dumps/PXD010000/training_columns/statistics.parquet'\n"
     ]
    }
   ],
   "source": [
    "file_path_count = len(MZMLID_FILE_PATHS)\n",
    "\n",
    "def get_mzmlid_file_stats(item: Tuple[int, str]) -> Dict[str, Any]:\n",
    "    idx, path = item\n",
    "    info_text = f\"Processing item {idx + 1}/{file_path_count} '{path}'\"\n",
    "    if idx % 10 == 0:\n",
    "        logger.info(info_text)\n",
    "    else:\n",
    "        logger.debug(info_text)\n",
    "    df = pd.read_parquet(path)\n",
    "    max_sequence_length = df[SEQ].str.len().max()\n",
    "    max_array_length = df[INT].str.len().max()\n",
    "    alphabet = set.union(*df[SEQ].apply(set))\n",
    "    item_count = len(df)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        \"file_path\": path,\n",
    "        \"max_sequence_length\": max_sequence_length,\n",
    "        \"max_array_length\": max_array_length,\n",
    "        \"alphabet\": alphabet,\n",
    "        \"item_count\": item_count\n",
    "    }\n",
    "\n",
    "if os.path.exists(STATISTICS_FILE_PATH):\n",
    "    file_stats = pd.read_parquet(STATISTICS_FILE_PATH)\n",
    "    file_stats.alphabet = file_stats.alphabet.apply(set)\n",
    "    print(f\"loaded previous statistics file '{STATISTICS_FILE_PATH}'\")\n",
    "else:\n",
    "    file_stats = pd.DataFrame(\n",
    "        ItemProcessor(\n",
    "            items=enumerate(MZMLID_FILE_PATHS),\n",
    "            item_processor=get_mzmlid_file_stats,\n",
    "            action_name=\"analyse\",\n",
    "            subject_name=\"mzmlid file\",\n",
    "            thread_count=0,\n",
    "            logger=logger\n",
    "        ).process()\n",
    "    )\n",
    "    \n",
    "    file_stats_writable = file_stats.copy()\n",
    "    file_stats_writable.alphabet = file_stats_writable.alphabet.apply(list) # cannot store sets\n",
    "    file_stats_writable.to_parquet(STATISTICS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "leading-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>max_sequence_length</th>\n",
       "      <th>max_array_length</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>item_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet</td>\n",
       "      <td>50</td>\n",
       "      <td>1845</td>\n",
       "      <td>{G, I, T, R, M(Oxidation), L, K, N, P, W, Y, M, F, E, V, S, A, D, C, Q, H}</td>\n",
       "      <td>26943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../dumps/PXD010000/training_columns/Biodiversity_A_faecalis_LB_aerobic_03_26Feb16_Arwen_16-01-01_mzmlid.parquet</td>\n",
       "      <td>49</td>\n",
       "      <td>1082</td>\n",
       "      <td>{G, I, T, R, M(Oxidation), L, K, N, P, W, Y, M, F, E, V, S, A, D, C, Q, H}</td>\n",
       "      <td>16723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         file_path  \\\n",
       "0             ../dumps/PXD010000/training_columns/Biodiversity_B_fragilis_01_28Jul15_Arwen_14-12-03_mzmlid.parquet   \n",
       "1  ../dumps/PXD010000/training_columns/Biodiversity_A_faecalis_LB_aerobic_03_26Feb16_Arwen_16-01-01_mzmlid.parquet   \n",
       "\n",
       "   max_sequence_length  max_array_length  \\\n",
       "0                   50              1845   \n",
       "1                   49              1082   \n",
       "\n",
       "                                                                     alphabet  \\\n",
       "0  {G, I, T, R, M(Oxidation), L, K, N, P, W, Y, M, F, E, V, S, A, D, C, Q, H}   \n",
       "1  {G, I, T, R, M(Oxidation), L, K, N, P, W, Y, M, F, E, V, S, A, D, C, Q, H}   \n",
       "\n",
       "   item_count  \n",
       "0       26943  \n",
       "1       16723  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "early-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SEQUENCE_LENGTH = 50\n",
      "MAX_ARRAY_LENGTH = 1845\n",
      "TOTAL_ITEM_COUNT = 820586\n",
      "ALPHABET = A, C, D, E, F, G, H, I, K, L, M, M(Oxidation), N, P, Q, R, S, T, V, W, Y\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = file_stats.max_sequence_length.max()\n",
    "print(f\"MAX_SEQUENCE_LENGTH = {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "MAX_ARRAY_LENGTH = file_stats.max_array_length.max()\n",
    "print(f\"MAX_ARRAY_LENGTH = {MAX_ARRAY_LENGTH}\")\n",
    "\n",
    "TOTAL_ITEM_COUNT = file_stats.item_count.sum()\n",
    "print(f\"TOTAL_ITEM_COUNT = {TOTAL_ITEM_COUNT}\")\n",
    "\n",
    "ALPHABET = set.union(*file_stats.alphabet)\n",
    "print(f\"ALPHABET = {', '.join(sorted(ALPHABET))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-recipient",
   "metadata": {},
   "source": [
    "## Data Normalization, Padding, and Conversion to Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "headed-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return tf.keras.utils.normalize(x=values, order=2)\n",
    "\n",
    "def base_peak_normalize(values: np.ndarray) -> np.ndarray:\n",
    "    return values / values.max(initial=0)\n",
    "\n",
    "# by Tom, probably\n",
    "# don't know, what it's based on\n",
    "def ion_current_normalize(intensities: np.ndarray) -> np.ndarray:\n",
    "    total_sum = np.sum(intensities**2)\n",
    "    normalized = intensities/total_sum\n",
    "    return normalized\n",
    "\n",
    "NORMALIZATION = {\n",
    "    INT: base_peak_normalize\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "waiting-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_CHARACTERS = {\n",
    "    SEQ: '_',\n",
    "    MZ: 0.0,\n",
    "    INT: 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sudden-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET.add(PADDING_CHARACTERS[SEQ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "understood-necklace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'M(Oxidation)': 11,\n",
       " 'N': 12,\n",
       " 'P': 13,\n",
       " 'Q': 14,\n",
       " 'R': 15,\n",
       " 'S': 16,\n",
       " 'T': 17,\n",
       " 'V': 18,\n",
       " 'W': 19,\n",
       " 'Y': 20,\n",
       " '_': 21}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(ALPHABET))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "INDEX_ALPHABET = idx_to_char.keys()\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "listed-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARRAY_COLS = [MZ, INT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "following-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transform to class\n",
    "\n",
    "PADDING_LENGTHS = {\n",
    "    MZ: MAX_ARRAY_LENGTH,\n",
    "    INT: MAX_ARRAY_LENGTH,\n",
    "    SEQ: MAX_SEQUENCE_LENGTH\n",
    "}\n",
    "\n",
    "class Parquet2DatasetFileProcessor:\n",
    "\n",
    "    def __init__(self,\n",
    "                 training_data_columns: List[str],\n",
    "                 target_data_columns: List[str],\n",
    "                 padding_lengths: Dict[str, int],\n",
    "                 padding_characters: Dict[str, Union[str, int, float]],\n",
    "                 column_normalizations: Dict[str, Callable[[Any], Any]],\n",
    "                 char_to_idx_mappers: Optional[Dict[str, Dict[str, int]]] = None,\n",
    "                 char_to_idx_mapping_functions: Optional[Dict[str, Callable[[str], int]]] = None,\n",
    "                 item_count: int = 0,\n",
    "                 dataset_dump_path: str = DATASET_DUMP_PATH,\n",
    "                 training_columns_dump_path: str = TRAINING_COLUMNS_DUMP_PATH,\n",
    "                 logger: log.Logger = logger\n",
    "                 ):\n",
    "        self.training_data_columns = training_data_columns\n",
    "        self.target_data_columns = target_data_columns\n",
    "        self.padding_lengths = padding_lengths\n",
    "        self.padding_characters = padding_characters\n",
    "        self.column_normalizations = column_normalizations\n",
    "        self.char_to_idx_mapping_functions = char_to_idx_mapping_functions\n",
    "        if self.char_to_idx_mapping_functions is None:\n",
    "            assert char_to_idx_mappers is not None, \\\n",
    "                \"either char_to_idx_mappers or char_to_idx_mapping_functions must be given\"\n",
    "            self.char_to_idx_mapping_functions = {\n",
    "                column: mapping.get for column, mapping in char_to_idx_mappers.items()\n",
    "            }\n",
    "        self.item_count = item_count\n",
    "        self.dataset_dump_path = dataset_dump_path\n",
    "        self.training_columns_dump_path = training_columns_dump_path\n",
    "        self.char_idx_dtype = np.int8\n",
    "        self.logger = logger\n",
    "\n",
    "    def normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.column_normalizations is None:\n",
    "            return df\n",
    "        df = df.copy()\n",
    "        for column, normalize_func in self.column_normalizations.items():\n",
    "            df[column] = df[column].apply(normalize_func)\n",
    "        return df\n",
    "\n",
    "    def pad_array_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if len(df) == 0:\n",
    "            return df\n",
    "\n",
    "        df = df.copy()\n",
    "        for column, padding_length in self.padding_lengths.items():\n",
    "            item_dtype = df[column].iloc[0].dtype\n",
    "\n",
    "            df[column] = list(tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                sequences=df[column],\n",
    "                maxlen=padding_length,\n",
    "                padding='post',\n",
    "                value=self.padding_characters[column],\n",
    "                dtype=item_dtype\n",
    "            ))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _sequence_to_indices(sequence: Iterable[str],\n",
    "                             char_to_idx_mapping_func: Callable[[str], int],\n",
    "                             dtype: type) -> np.ndarray:\n",
    "        return np.array([char_to_idx_mapping_func(char) for char in sequence],\n",
    "                        dtype=dtype)\n",
    "\n",
    "    def sequence_column_to_indices(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if len(self.char_to_idx_mapping_functions) == 0:\n",
    "            return df\n",
    "        df = df.copy()\n",
    "        for column, mapping_function in self.char_to_idx_mapping_functions.items():\n",
    "            df[column] = df[column].apply(lambda seq: self._sequence_to_indices(seq,\n",
    "                                                                                mapping_function,\n",
    "                                                                                self.char_idx_dtype))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def stack_numpy_arrays_in_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.apply(lambda item: [np.stack(item)])\n",
    "\n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.normalize_columns(df)\n",
    "        df = self.pad_array_columns(df)\n",
    "        df = self.sequence_column_to_indices(df)\n",
    "        df = self.stack_numpy_arrays_in_dataframe(df)\n",
    "        return df\n",
    "\n",
    "    def stacked_df_to_dataset(self, stacked_df: pd.DataFrame) -> tf.data.Dataset:\n",
    "        assert len(stacked_df) == 1, \"all column values should be stacked at this point\"\n",
    "        training_data = tuple(stacked_df[self.training_data_columns].iloc[0])\n",
    "        target_data = tuple(stacked_df[self.target_data_columns].iloc[0])\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices((training_data, target_data))\n",
    "        return tf_dataset\n",
    "\n",
    "    def convert_df_file_to_dataset_file(self,\n",
    "                                        df_input_file_path: str,\n",
    "                                        tf_dataset_output_file_path: str):\n",
    "        df = pd.read_parquet(df_input_file_path)\n",
    "        df = self.preprocess_dataframe(df)\n",
    "        tf_dataset = self.stacked_df_to_dataset(df)\n",
    "        self.logger.debug(tf_dataset.element_spec)\n",
    "\n",
    "        tf.data.experimental.save(dataset=tf_dataset,\n",
    "                                  path=tf_dataset_output_file_path,\n",
    "                                  compression='GZIP')\n",
    "\n",
    "    def __call__(self, item: Tuple[int, str]) -> Optional[str]:\n",
    "        idx, path = item\n",
    "        tf_dataset_path = os.path.join(\n",
    "            self.dataset_dump_path,\n",
    "            path[len(self.training_columns_dump_path)+len(os.path.sep):])\n",
    "\n",
    "        info_text = f\"Processing item {idx + 1}/{self.item_count}: '{path}'\"\n",
    "        if idx % 10 == 0:\n",
    "            self.logger.info(info_text)\n",
    "        else:\n",
    "            self.logger.debug(info_text)\n",
    "\n",
    "        if os.path.exists(tf_dataset_path):\n",
    "            self.logger.debug(f\"Skipped '{path}' because '{tf_dataset_path}' already exists\")\n",
    "            return None\n",
    "\n",
    "        self.convert_df_file_to_dataset_file(df_input_file_path=path,\n",
    "                                             tf_dataset_output_file_path=tf_dataset_path)\n",
    "        gc.collect()\n",
    "\n",
    "        return tf_dataset_path\n",
    "\n",
    "    def process(self, parquet_file_paths: Iterable[str], **kwargs) -> List[str]:\n",
    "        item_processor = ItemProcessor(\n",
    "            items=enumerate(parquet_file_paths),\n",
    "            item_processor=self.__call__,\n",
    "            action_name=\"parquet2tf_dataset-process\",\n",
    "            subject_name=\"mzmlid parquet file\",\n",
    "            logger=self.logger,\n",
    "            **kwargs\n",
    "        )\n",
    "        results = list(item_processor.process())\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "equivalent-portrait",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: No mzmlid parquet files were parquet2tf_dataset-processed\n",
      "INFO: Encountered 0 exceptions during processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace with function call to process #TODO\n",
    "list(ItemProcessor(\n",
    "    items=enumerate(MZMLID_FILE_PATHS),\n",
    "    item_processor=parquet_file_to_dataset_file_converter,\n",
    "    action_name=\"parquet2tf_dataset-process\",\n",
    "    subject_name=\"mzmlid parquet file\",\n",
    "    thread_count=2,\n",
    "    logger=logger\n",
    ").process())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-proceeding",
   "metadata": {},
   "source": [
    "## Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fundamental-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_B_cereus_PN_L_CL_2_09Oct16_Pippin_16-05-06_mzmlid.parquet',\n",
       " '../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_A_tumefaciens_R2A_aerobic_2_23Nov16_Pippin_16-09-11_mzmlid.parquet',\n",
       " '../dumps/PXD010000/training_columns/tf_datasets/Biodiversity_B_fragilis_Carb_01_28Oct15_Arwen_15-07-13_mzmlid.parquet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_paths = glob.glob(os.path.join(DATASET_DUMP_PATH, '*'))\n",
    "print(len(dataset_file_paths))\n",
    "dataset_file_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stupid-malaysia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(1845,), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(1845,), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(50,), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ((tf.TensorSpec(shape=(MAX_ARRAY_LENGTH,), dtype=tf.float32), \n",
    "  tf.TensorSpec(shape=(MAX_ARRAY_LENGTH,), dtype=tf.float32)),\n",
    "(tf.TensorSpec(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int8)))\n",
    "element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stretch-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [tf.data.experimental.load(path=path, element_spec=element_spec, compression='GZIP') \n",
    "            for path in dataset_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "surprised-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>,\n",
       " <_LoadDataset shapes: (((1845,), (1845,)), (50,)), types: ((tf.float32, tf.float32), tf.int8)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-difficulty",
   "metadata": {},
   "source": [
    "## Concatenating Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba9ffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "billion-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]\n",
    "for ds in datasets[1:]:\n",
    "    dataset = dataset.concatenate(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5cf115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (((None, 1845), (None, 1845)), (None, 50)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e68066a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-30-a871fdc9ebee>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-company",
   "metadata": {},
   "source": [
    "## Building the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers = {col: tf.keras.layers.Input(shape=(MAX_ARRAY_LENGTH,)) for col in ARRAY_COLS}\n",
    "input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-geneva",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = input_layers[MZ] + input_layers[INT]\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "for _ in range(1):\n",
    "    x = tf.keras.layers.Dense(16*MAX_SEQUENCE_LENGTH*len(ALPHABET))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(MAX_SEQUENCE_LENGTH*len(ALPHABET))(x)\n",
    "\n",
    "x = tf.reshape(x,(-1, MAX_SEQUENCE_LENGTH, len(ALPHABET)))\n",
    "\n",
    "x = tf.keras.activations.softmax(x)\n",
    "\n",
    "model = tf.keras.Model([input_layers[MZ],input_layers[INT]],x)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-queue",
   "metadata": {},
   "source": [
    "## Training the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, fraction):\n",
    "    split_value = int(len(dataset) * fraction)\n",
    "    a = dataset.take(split_value)\n",
    "    b = dataset.skip(split_value)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=int(10000 / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-commons",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-laser",
   "metadata": {},
   "source": [
    "## Evaluating the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-appendix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}